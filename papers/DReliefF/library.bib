Automatically generated by Mendeley Desktop 1.16.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@article{XindongWu2014,
author = {{Xindong Wu}, Xindong and {Xingquan Zhu}, Xingquan and {Gong-Qing Wu}, Gong-Qing and {Wei Ding}, Wei},
doi = {10.1109/TKDE.2013.109},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - Data mining with big data(2).pdf:pdf},
issn = {1041-4347},
journal = {IEEE Transactions on Knowledge and Data Engineering},
keywords = {Big Data,Big Data processing model,Big Data revolution,Data handling,Data models,Data privacy,Data storage systems,Distributed databases,HACE theorem,Information management,autonomous sources,complex and evolving associations,data collection capacity,data driven model,data mining,data storage,demand driven aggregation,growing data sets,heterogeneity,information sources,networking,user interest modeling,user modelling},
month = {jan},
number = {1},
pages = {97--107},
publisher = {IEEE},
title = {{Data mining with big data}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6547630},
volume = {26},
year = {2014}
}
@article{Kitchenham2007a,
abstract = {The objective of this report is to propose comprehensive guidelines for systematic literature reviews appropriate for software engineering researchers, including PhD students. A systematic literature review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guidelines presented in this report were derived from three existing guidelines used by medical researchers, two books produced by researchers with social science backgrounds and discussions with researchers from other disciplines who are involved in evidence-based practice. The guidelines have been adapted to reflect the specific problems of software engineering research. The guidelines cover three phases of a systematic literature review: planning the review, conducting the review and reporting the review. They provide a relatively high level description. They do not consider the impact of the research questions on the review procedures, nor do they specify in detail the mechanisms needed to perform meta-analysis.},
author = {Kitchenham, Barbara and Charters, S},
doi = {10.1145/1134285.1134500},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2007 - Guidelines for performing Systematic Literature Reviews in Software Engineering(2).pdf:pdf},
isbn = {1595933751},
issn = {00010782},
journal = {Engineering},
pages = {1051},
title = {{Guidelines for performing Systematic Literature Reviews in Software Engineering}},
volume = {2},
year = {2007}
}
@article{Magdaleno2012,
abstract = {Purpose: The purpose of this paper is to characterize reconciliation among the plan-driven, agile, and free/open source software models of software development. Design/methodology/approach: An automated quasi-systematic review identified 42 papers, which were then analyzed. Findings: The main findings are: there exist distinct - organization, group and process - levels of reconciliation; few studies deal with reconciliation among the three models of development; a significant amount of work addresses reconciliation between plan-driven and agile development; several large organizations (such as Microsoft, Motorola, and Philips) are interested in trying to combine these models; and reconciliation among software development models is still an open issue, since it is an emerging area and research on most proposals is at an early stage. Research limitations: Automated searches may not capture relevant papers in publications that are not indexed. Other data sources not amenable to execution of the protocol were not used. Data extraction was performed by only one researcher, which may increase the risk of threats to internal validity. Implications: This characterization is important for practitioners wanting to be current with the state of research. This review will also assist the scientific community working with software development processes to build a common understanding of the challenges that must be faced, and to identify areas where research is lacking. Finally, the results will be useful to software industry that is calling for solutions in this area. Originality/value: There is no other systematic review on this subject, and reconciliation among software development models is an emerging area. This study helps to identify and consolidate the work done so far and to guide future research. The conclusions are an important step towards expanding the body of knowledge in the field. {\textcopyright} 2011 Elsevier Inc.},
author = {Magdaleno, Andr{\'{e}}a Magalh{\~{a}}es and Werner, Cl{\'{a}}udia Maria Lima and Araujo, Renata Mendes De},
doi = {10.1016/j.jss.2011.08.028},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Reconciling software development models A quasi-systematic review.pdf:pdf},
isbn = {0164-1212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Agile,Free/open source software,Plan-driven,Reconciliation among development models,Software process,Systematic review},
number = {2},
pages = {351--369},
publisher = {Elsevier Inc.},
title = {{Reconciling software development models: A quasi-systematic review}},
url = {http://dx.doi.org/10.1016/j.jss.2011.08.028},
volume = {85},
year = {2012}
}
@article{Kennedy1995,
author = {Kennedy, James; and Eberhart, Russell;},
doi = {10.1007/s11721-007-0002-0},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1995 - Particle swarm optimization.pdf:pdf},
isbn = {0780327683},
issn = {1935-3812},
journal = {Encyclopedia of Machine Learning},
keywords = {particle swarm optimization,particle swarms,pso,real world applications,social networks,swarm,swarm dynamics,theory},
pages = {1942--1948},
pmid = {20371407},
title = {{Particle swarm optimization}},
year = {1995}
}
@phdthesis{Canedo2014,
author = {Bol{\'{o}}n-Canedo, Ver{\'{o}}nica},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - Novel feature selection methods for high dimensional data.pdf:pdf},
number = {March},
title = {{Novel feature selection methods for high dimensional data}},
year = {2014}
}
@article{Bae2010,
abstract = {Data mining is the most commonly used name to solve problems by analyzing data already present in databases. Feature selection is an important problem in the emerging field of data mining which is aimed at finding a small set of rules from the training data set with predetermined targets. Many approaches, methods and goals including Genetic Algorithms (GA) and swarm-based approaches have been tried out for feature selection in order to these goals. Furthermore, a new technique which named Particle Swarm Optimization (PSO) has been proved to be competitive with GA in several tasks, mainly in optimization areas. However, there are some shortcomings in PSO such as premature convergence. To overcome these, we propose a new evolutionary algorithm called Intelligent Dynamic Swarm (IDS) that is a modified Particle Swarm Optimization. Experimental results states competitive performance of IDS. Due to less computing for swarm generation, averagely IDS is over 30{\%} faster than traditional PSO.},
author = {Bae, Changseok and Yeh, Wei-Chang and Chung, Yuk Ying and Liu, Sin-Long},
doi = {10.1016/j.eswa.2010.03.016},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2010 - Feature selection with Intelligent Dynamic Swarm and Rough Set.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Data mining,Feature selection,Intelligent Dynamic Swarm (IDS),Particle Swarm Optimization (PSO)},
month = {oct},
number = {10},
pages = {7026--7032},
title = {{Feature selection with Intelligent Dynamic Swarm and Rough Set}},
url = {http://www.sciencedirect.com/science/article/pii/S0957417410001971},
volume = {37},
year = {2010}
}
@inproceedings{Jondhale2015,
author = {Jondhale, Asmita and Das, Gautami and Sonavane, Samadhan},
booktitle = {2015 International Conference on Pervasive Computing (ICPC)},
doi = {10.1109/PERVASIVE.2015.7087151},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - OCR and RFID enabled vehicle identification and parking allocation system(2).pdf:pdf},
isbn = {978-1-4799-6272-3},
keywords = {Boom Barrier,Databases,Image edge detection,Number Plate,OCR,Optical character recognition software,Protocols,RFID,RFID tags,Reader,Tag,Vehicles,boom barriers,optical character recognition,parking allocation system,parking cameras,parking management systems,radiofrequency identification,road vehicles,vehicle identification,video cameras},
month = {jan},
pages = {1--4},
publisher = {IEEE},
title = {{OCR and RFID enabled vehicle identification and parking allocation system}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7087151},
year = {2015}
}
@article{Drugan2010,
abstract = {When constructing a Bayesian network classifier from data, the more or less redundant features included in a dataset may bias the classifier and as a consequence may result in a relatively poor classification accuracy. In this paper, we study the problem of selecting appropriate subsets of features for such classifiers. To this end, we propose a new definition of the concept of redundancy in noisy data. For comparing alternative classifiers, we use the Minimum Description Length for Feature Selection (MDL-FS) function that we introduced before. Our function differs from the well-known MDL function in that it captures a classifier's conditional log-likelihood. We show that the MDL-FS function serves to identify redundancy at different levels and is able to eliminate redundant features from different types of classifier. We support our theoretical findings by comparing the feature-selection behaviours of the various functions in a practical setting. Our results indicate that the MDL-FS function is more suited to the task of feature selection than MDL as it often yields classifiers of equal or better performance with significantly fewer attributes.},
author = {Drugan, Mădălina M. and Wiering, Marco A.},
doi = {10.1016/j.ijar.2010.02.001},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2010 - Feature selection for Bayesian network classifiers using the MDL-FS score.pdf:pdf},
issn = {0888613X},
journal = {International Journal of Approximate Reasoning},
keywords = {Feature subset selection,Minimum Description Length,Selective Bayesian classifiers,Tree augumented networks},
month = {jul},
number = {6},
pages = {695--717},
title = {{Feature selection for Bayesian network classifiers using the MDL-FS score}},
url = {http://www.sciencedirect.com/science/article/pii/S0888613X10000411},
volume = {51},
year = {2010}
}
@article{Bolon-Canedo2015,
abstract = {Feature selection is often required as a preliminary step for many pattern recognition problems. However, most of the existing algorithms only work in a centralized fashion, i.e. using the whole dataset at once. In this research a new method for distributing the feature selection process is proposed. It distributes the data by features, i.e. according to a vertical distribution, and then performs a merging procedure which updates the feature subset according to improvements in the classification accuracy. The effectiveness of our proposal is tested on microarray data, which has brought a difficult challenge for researchers due to the high number of gene expression contained and the small samples size. The results on eight microarray datasets show that the execution time is considerably shortened whereas the performance is maintained or even improved compared to the standard algorithms applied to the non-partitioned datasets.},
annote = {With a vey fex exceptions the non-distributed versions of the algorithms take less than five minutes to execute, so, the faster distributed versions do not solve a big problem.},
author = {Bol{\'{o}}n-Canedo, V. and S{\'{a}}nchez-Maro{\~{n}}o, N. and Alonso-Betanzos, A.},
doi = {10.1016/j.asoc.2015.01.035},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Distributed feature selection An application to microarray data classification.pdf:pdf},
issn = {15684946},
journal = {Applied Soft Computing},
keywords = {Distributed learning,Feature selection,Microarray data},
month = {may},
pages = {136--150},
title = {{Distributed feature selection: An application to microarray data classification}},
url = {http://www.sciencedirect.com/science/article/pii/S156849461500054X},
volume = {30},
year = {2015}
}
@article{Lopez-Meneses2015,
abstract = {The emergence of massive open online course (MOOCs) has been a turning point for the academic world and, especially, in the design and provision of training courses in Higher Education. Now that the first moments of the information explosion have passed, a rigorous analysis of the effect of the movement in high-impact scientific world is needed in order to assess the state of the art and future lines of research. This study analyzes the impact of the MOOC movement in the form of scientific article during the birth and explosion period (2010-2013) in two of the most relevant databases: Journal Citation Reports (WoS) and Scopus (Scimago). We present, through a descriptive and quantitative methodology, the most significant bibliometric data according to citation index and database impact. Furthermore, with the use of a methodology based on social network analysis (SNA), an analysis of the article's keyword co-occurrence is presented through graphs to determine the fields of study and research. The results show that both the number of articles published and the citations received in both databases present a medium-low signi- ficant impact, and the conceptual network of relationships in the abstracts and keywords does not reflect the current analysis developed in general educational media.},
author = {L{\'{o}}pez-Meneses, E. and V{\'{a}}zquez-Cano, E. and Rom{\'{a}}n, P.},
doi = {10.3916/C44-2015-08},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Analysis and Implications of the Impact of MOOC Movement in the Scientific Community JCR and Scopus ( 2010-13 ).pdf:pdf},
issn = {1134-3478, 1988-3293},
journal = {Media Education Research Journal},
keywords = {actual de,campos de estudio e,citas que reciben presentan,de datos como las,de interrelaciones en los,de los art{\'{i}}culos para,de los art{\'{i}}culos publicados,el n{\'{u}}mero de art{\'{i}}culos,impacto,investigaci{\'{o}}n,la determinaci{\'{o}}n de los,los medios divulgativos generales,los resultados muestran que,no reflejan la cr{\'{i}}tica,publicados en ambas bases,res{\'{u}}menes y palabras clave,tanto,un {\'{i}}ndice medio-bajo de,y la red tem{\'{a}}tica},
number = {22},
pages = {73--80},
title = {{Analysis and Implications of the Impact of MOOC Movement in the Scientific Community : JCR and Scopus ( 2010-13 )}},
volume = {44},
year = {2015}
}
@book{Wu2008,
abstract = {This paper presents the top 10 data mining algorithms identified by the IEEE International Conference on Data Mining (ICDM) in December 2006: C4.5, k-Means, SVM, Apriori, EM, PageRank, AdaBoost, k NN, Naive Bayes, and CART. These top 10 algorithms are among the most influential data mining algorithms in the research community. With each algorithm, we provide a description of the algorithm, discuss the impact of the algorithm, and review current and further research on the algorithm. These 10 algorithms cover classification, clustering, statistical learning, association analysis, and link mining, which are all among the most important topics in data mining research and development. {\textcopyright} Springer-Verlag London Limited 2007. },
author = {Wu, Xindong and Kumar, Vipin and Ross, Quinlan J. and Ghosh, Joydeep and Yang, Qiang and Motoda, Hiroshi and McLachlan, Geoffrey J. and Ng, Angus and Liu, Bing and Yu, Philip S. and Zhou, Zhi Hua and Steinbach, Michael and Hand, David J. and Steinberg, Dan},
booktitle = {Knowledge and Information Systems},
doi = {10.1007/s10115-007-0114-2},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2008 - Top 10 algorithms in data mining.pdf:pdf},
isbn = {1011500701},
issn = {02191377},
number = {1},
pages = {1--37},
title = {{Top 10 algorithms in data mining}},
volume = {14},
year = {2008}
}
@article{Walczak1999,
abstract = {The basic concepts of the rough set theory are introduced and adequately illustrated. An example of the rough set theory application to the QSAR classification problem is presented. Numerous earlier applications of rough set theory to the various scientific domains suggest that it also can be a useful tool for the analysis of inexact, uncertain, or vague chemical data.},
author = {Walczak, B. and Massart, D.L.},
doi = {10.1016/S0169-7439(98)00200-7},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1999 - Rough sets theory(2).pdf:pdf},
issn = {01697439},
journal = {Chemometrics and Intelligent Laboratory Systems},
keywords = {Classification,QSAR,Rough sets,Set theory,Supervised pattern recognition},
month = {apr},
number = {1},
pages = {1--16},
title = {{Rough sets theory}},
url = {http://www.sciencedirect.com/science/article/pii/S0169743998002007},
volume = {47},
year = {1999}
}
@article{Rodriguez2011a,
author = {Rodr{\'{i}}guez, D and Ruiz, R and Riquelme, JC and Harrison, R.},
doi = {http://dx.doi.org/10.1007/978-3-642-23716-4_25},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2011 - Subgroup discovery for defect prediction.pdf:pdf},
isbn = {0302-9743 978-3-642-23715-7},
journal = {Ssbse},
number = {Ssbse},
pages = {2011},
title = {{Subgroup discovery for defect prediction}},
url = {http://www.cc.uah.es/drg/c/RodriguezEtAl{\_}SSBSE11{\_}Poster.pdf},
year = {2011}
}
@article{Vandewalle1999,
abstract = {In this letter we discuss a least squares version for support vector machine (SVM) classifiers. Due to equality type constraints in the formulation, the solution follows from solving a set of linear equations, instead of quadratic programming for classical SVM‘s. The approach is illustrated on a two-spiral benchmark classification problem.},
archivePrefix = {arXiv},
arxivId = {1018628609742},
author = {Vandewalle, J},
doi = {10.1023/A:1018628609742},
eprint = {1018628609742},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1999 - Least Squares Support Vector Machine Classifiers.pdf:pdf},
isbn = {1370-4621},
issn = {13704621},
journal = {Neural Processing Letters},
keywords = {abbreviations,classification,linear least squares,radial basis,radial basis function kernel,rbf,support vector machines,svm,vapnik-chervonenkis,vc},
number = {3},
pages = {293--300},
pmid = {99},
title = {{Least Squares Support Vector Machine Classifiers}},
volume = {9},
year = {1999}
}
@article{Achtert2008,
author = {Achtert, Elke and Kriegel, Hp and Zimek, Arthur},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2008 - ELKI a software system for evaluation of subspace clustering algorithms.pdf:pdf},
issn = {3540694765},
journal = {Scientific and Statistical Database {\ldots}},
number = {Ssdbm},
pages = {580--585},
title = {{ELKI: a software system for evaluation of subspace clustering algorithms}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-69497-7{\_}41},
year = {2008}
}
@article{Yu2004,
author = {Yu, Lei and Liu, Huan},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2004 - Efficient Feature Selection via Analysis of Relevance and Redundancy.pdf:pdf},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
month = {dec},
pages = {1205--1224},
publisher = {JMLR.org},
title = {{Efficient Feature Selection via Analysis of Relevance and Redundancy}},
url = {http://dl.acm.org/citation.cfm?id=1005332.1044700},
volume = {5},
year = {2004}
}
@article{Melanie1996,
abstract = {Science arises from the very human desire to understand and control the world. Over the course of history, we humans have gradually built up a grand edifice of knowledge that enables us to predict, to varying extents, the weather, the motions of the planets, solar and lunar eclipses, the courses of diseases, the rise and fall of economic growth, the stages of language development in children, and a vast panorama of other natural, social, and cultural phenomena. More recently we have even come to understand some fundamental limits to our abilities to predict. Over the eons we have developed increasingly complex means to control many aspects of our lives and our interactions with nature, and we have learned, often the hard way, the extent to which other aspects are uncontrollable. The advent of electronic computers has arguably been the most revolutionary development in the history of science and technology. This ongoing revolution is profoundly increasing our ability to predict and control nature in ways that were barely conceived of even half a century ago. For many, the crowning achievements of this revolution will be the creation—in the form of computer programs—of new species of intelligent beings, and even of new forms of life. The goals of creating artificial intelligence and artificial life can be traced back to the very beginnings of the computer age. The earliest computer scientists—Alan Turing, John von Neumann, Norbert Wiener, and others—were motivated in large part by visions of imbuing computer programs with intelligence, with the life−like ability to self−replicate, and with the adaptive capability to learn and to control their environments. These early pioneers of computer science were as much interested in biology and psychology as in electronics, and they looked to natural systems as guiding metaphors for how to achieve their visions. It should be no surprise, then, that from the earliest days computers were applied not only to calculating missile trajectories and deciphering military codes but also to modeling the brain, mimicking human learning, and simulating biological evolution. These biologically motivated computing activities have waxed and waned over the years, but since the early 1980s they have all undergone a resurgence in the computation research community. The first has grown into the field of neural networks, the second into machine learning, and the third into what is now called "evolutionary computation," of which genetic algorithms are the most prominent example.},
author = {Melanie, Mitchell},
doi = {10.1016/S0898-1221(96)90227-8},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1996 - An introduction to genetic algorithms.pdf:pdf},
isbn = {0-262-13316-4},
issn = {08981221},
journal = {Cambridge, Massachusetts London, England, {\ldots}},
pages = {162},
title = {{An introduction to genetic algorithms}},
year = {1996}
}
@article{Romero2009,
abstract = {This work describes the application of subgroup discovery using evolutionary algorithms to the usage data of the Moodle course management system, a case study of the University of Cordoba, Spain. The objective is to obtain rules which describe relationships between the student's usage of the different activities and modules provided by this e-learning system and the final marks obtained in the courses. We use an evolutionary algorithm for the induction of fuzzy rules in canonical form and disjunctive normal form. The results obtained by different algorithms for subgroup discovery are compared, showing the suitability of the evolutionary subgroup discovery to this problem. ?? 2007 Elsevier Ltd. All rights reserved.},
author = {Romero, C. and Gonz??lez, P. and Ventura, S. and del Jesus, M. J. and Herrera, F.},
doi = {10.1016/j.eswa.2007.11.026},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - Evolutionary algorithms for subgroup discovery in e-learning A practical application using Moodle data.pdf:pdf},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Evolutionary algorithms,Fuzzy rules,Subgroup discovery,Web-based education},
number = {2 PART 1},
pages = {1632--1644},
title = {{Evolutionary algorithms for subgroup discovery in e-learning: A practical application using Moodle data}},
volume = {36},
year = {2009}
}
@article{Deb2002,
abstract = {Multi-objective evolutionary algorithms (MOEAs) that use non-dominated sorting and sharing have been criticized mainly for: (1) their O(MN3) computational complexity (where M is the number of objectives and N is the population size); (2) their non-elitism approach; and (3) the need to specify a sharing parameter. In this paper, we suggest a non-dominated sorting-based MOEA, called NSGA-II (Non-dominated Sorting Genetic Algorithm II), which alleviates all of the above three difficulties. Specifically, a fast non-dominated sorting approach with O(MN2) computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best N solutions (with respect to fitness and spread). Simulation results on difficult test problems show that NSGA-II is able, for most problems, to find a much better spread of solutions and better convergence near the true Pareto-optimal front compared to the Pareto-archived evolution strategy and the strength-Pareto evolutionary algorithm - two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multi-objective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective, seven-constraint nonlinear problem, are compared with another constrained multi-objective optimizer, and the much better performance of NSGA-II is observed},
author = {Deb, Kalyanmoy and Pratap, Amrit and Agarwal, Sameer and Meyarivan, T.},
doi = {10.1109/4235.996017},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2002 - A fast and elitist multiobjective genetic algorithm NSGA-II.pdf:pdf},
isbn = {1089-778X VO - 6},
issn = {1089778X},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Constraint handling,Elitism,Genetic algorithms,Multicriterion decision making,Multiobjective optimization,Pareto-optimal solutions},
number = {2},
pages = {182--197},
title = {{A fast and elitist multiobjective genetic algorithm: NSGA-II}},
volume = {6},
year = {2002}
}
@article{Dean2004a,
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program's execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google's clusters every day.},
archivePrefix = {arXiv},
arxivId = {10.1.1.163.5292},
author = {Dean, Jeffrey and Ghemawat, Sanjay},
doi = {10.1145/1327452.1327492},
eprint = {10.1.1.163.5292},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2004 - MapReduce Simplied Data Processing on Large Clusters.pdf:pdf},
isbn = {9781595936868},
issn = {00010782},
journal = {Proceedings of 6th Symposium on Operating Systems Design and Implementation},
pages = {137--149},
pmid = {11687618},
title = {{MapReduce: Simplied Data Processing on Large Clusters}},
year = {2004}
}
@article{Mierswa2006,
abstract = {KDD is a complex and demanding task. While a large number of methods has been established for numerous problems, many challenges remain to be solved. New tasks emerge requiring the development of new methods or processing schemes. Like in software development, the development of such solutions demands for careful analysis, specification, implementation, and testing. Rapid prototyping is an approach which allows crucial design decisions as early as possible. A rapid prototyping system should support maximal re-use and innovative combinations of existing methods, as well as simple and quick integration of new ones. This paper describes YALE, a free open-source environment for KDD and machine learning. YALE provides a rich variety of methods which allows rapid prototyping for new applications and makes costly re-implementations unnecessary. Additionally, YALE offers extensive functionality for process evaluation and optimization which is a crucial property for any KDD rapid prototyping tool. Following the paradigm of visual programming eases the design of processing schemes. While the graphical user interface supports interactive design, the underlying XML representation enables automated applications after the prototyping phase. After a discussion of the key concepts of YALE, we illustrate the advantages of rapid prototyping for KDD on case studies ranging from data pre-processing to result visualization. These case studies cover tasks like feature engineering, text mining, data stream mining and tracking drifting. Copyright 2006 ACM.},
author = {Mierswa, I and Wurst, M and Klinkenberg, R and Scholz, M and Euler, T},
doi = {10.1145/1150402.1150531},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2006 - YALE Rapid prototyping for complex data mining tasks.pdf:pdf},
isbn = {1595933395 (ISBN); 9781595933393 (ISBN)},
journal = {Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
keywords = {Audio and text mining,Computer software,Data mining,Data pre-processing,Data stream mining,Distributed data mining,Feature construction,KDD system,Learning systems,Multimedia mining,Multimedia systems,Optimization,Problem solving,Rapid prototyping,Result visualization,XML},
pages = {935--940},
title = {{YALE: Rapid prototyping for complex data mining tasks}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-33749558210{\&}partnerID=40{\&}md5=960eb3c57f304062486fc05c89db4b9c},
volume = {2006},
year = {2006}
}
@article{Rossum2014,
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - Sorting HOW TO.pdf:pdf},
pages = {1--6},
title = {{Sorting HOW TO}},
year = {2014}
}
@article{Rossum2003,
author = {Rossum, G V},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2003 - The Python Language Reference Manual.pdf:pdf},
isbn = {0954161785},
title = {{The Python Language Reference Manual}},
year = {2003}
}
@article{Kitchenham2010,
abstract = {Context: In a previous study, we reported on a systematic literature review (SLR), based on a manual search of 13 journals and conferences undertaken in the period 1st January 2004 to 30th June 2007. Objective: The aim of this on-going research is to provide an annotated catalogue of SLRs available to software engineering researchers and practitioners. This study updates our previous study using a broad automated search. Method: We performed a broad automated search to find SLRs published in the time period 1st January 2004 to 30th June 2008. We contrast the number, quality and source of these SLRs with SLRs found in the original study. Results: Our broad search found an additional 35 SLRs corresponding to 33 unique studies. Of these papers, 17 appeared relevant to the undergraduate educational curriculum and 12 appeared of possible interest to practitioners. The number of SLRs being published is increasing. The quality of papers in conferences and workshops has improved as more researchers use SLR guidelines. Conclusion: SLRs appear to have gone past the stage of being used solely by innovators but cannot yet be considered a main stream software engineering research methodology. They are addressing a wide range of topics but still have limitations, such as often failing to assess primary study quality. ?? 2010 Elsevier B.V. All rights reserved.},
author = {Kitchenham, Barbara and Pretorius, Rialette and Budgen, David and Brereton, O. Pearl and Turner, Mark and Niazi, Mahmood and Linkman, Stephen},
doi = {10.1016/j.infsof.2010.03.006},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2010 - Systematic literature reviews in software engineering-A tertiary study.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Mapping study,Software engineering,Systematic literature review,Tertiary study},
number = {8},
pages = {792--805},
publisher = {Elsevier B.V.},
title = {{Systematic literature reviews in software engineering-A tertiary study}},
url = {http://dx.doi.org/10.1016/j.infsof.2010.03.006},
volume = {52},
year = {2010}
}
@article{Ramirez-Gallego2016,
author = {Ram{\'{i}}rez-Gallego, S. and Lastra, I. and Mart{\'{i}}nez-Rego, D. and Bol{\'{o}}n-Canedo, V. and Ben{\'{i}}tez, J. M. and Herrera, F. and Alonso-Betanzos, A.},
doi = {10.1002/int.21833},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2016 - Fast-mRMR Fast Minimum Redundancy Maximum Relevance Algorithm for High-Dimensional Big Data.pdf:pdf},
issn = {08848173},
journal = {International Journal of Intelligent Systems},
month = {jul},
title = {{Fast-mRMR: Fast Minimum Redundancy Maximum Relevance Algorithm for High-Dimensional Big Data}},
url = {http://doi.wiley.com/10.1002/int.21833},
year = {2016}
}
@article{Kira1992,
author = {Kira, Kenji and Rendell, Larry A.},
isbn = {1-5586-247-X},
journal = {Proceedings of the ninth international workshop on Machine learning},
pages = {249--256},
publisher = {Morgan Kaufmann Publishers Inc.},
title = {{A practical approach to feature selection}},
year = {1992}
}
@article{Rodriguez2012,
abstract = {Data mining methods in software engineering are becoming increasingly important as they can support several aspects of the software development life-cycle such as quality. In this work, we present a data mining approach to induce rules extracted from static software metrics characterising fault-prone modules. Due to the special characteristics of the defect prediction data (imbalanced, inconsistency, redundancy) not all classification algorithms are capable of dealing with this task conveniently. To deal with these problems, Subgroup Discovery (SD) algorithms can be used to find groups of statistically different data given a property of interest. We propose EDER-SD (Evolutionary Decision Rules for Subgroup Discovery), a SD algorithm based on evolutionary computation that induces rules describing only fault-prone modules. The rules are a well-known model representation that can be easily understood and applied by project managers and quality engineers. Thus, rules can help them to develop software systems that can be justifiably trusted. Contrary to other approaches in SD, our algorithm has the advantage of working with continuous variables as the conditions of the rules are defined using intervals. We describe the rules obtained by applying our algorithm to seven publicly available datasets from the PROMISE repository showing that they are capable of characterising subgroups of fault-prone modules. We also compare our results with three other well known SD algorithms and the EDER-SD algorithm performs well in most cases.},
author = {Rodr{\'{i}}guez, D. and Ruiz, R. and Riquelme, J.C. and Aguilar–Ruiz, J.S.},
doi = {10.1016/j.ins.2011.01.039},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Searching for rules to detect defective modules A subgroup discovery approach.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Defect prediction,Imbalanced datasets,Rules,Subgroup discovery},
month = {may},
pages = {14--30},
title = {{Searching for rules to detect defective modules: A subgroup discovery approach}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025511000661},
volume = {191},
year = {2012}
}
@book{Jensen2008,
abstract = {The rough and fuzzy set approaches presented here open up many new frontiers for continued research and development Computational Intelligence and Feature Selection provides readers with the background and fundamental ideas behind Feature Selection (FS), with an emphasis on techniques based on rough and fuzzy sets. For readers who are less familiar with the subject, the book begins with an introduction to fuzzy set theory and fuzzy-rough set theory. Building on this foundation, the book provides:  A critical review of FS methods, with particular emphasis on their current limitations   Program files implementing major algorithms, together with the necessary instructions and datasets, available on a related Web site   Coverage of the background and fundamental ideas behind FS   A systematic presentation of the leading methods reviewed in a consistent algorithmic framework   Real-world applications with worked examples that illustrate the power and efficacy of the FS approaches covered   An investigation of the associated areas of FS, including rule induction and clustering methods using hybridizations of fuzzy and rough set theories  Computational Intelligence and Feature Selection is an ideal resource for advanced undergraduates, postgraduates, researchers, and professional engineers. However, its straightforward presentation of the underlying concepts makes the book meaningful to specialists and nonspecialists alike.},
author = {Jensen, Richard and Shen, Qiang},
isbn = {0470377917},
pages = {300},
publisher = {John Wiley {\&} Sons},
title = {{Computational Intelligence and Feature Selection: Rough and Fuzzy Approaches}},
url = {https://books.google.com/books?id=rgDDCeY-COkC{\&}pgis=1},
year = {2008}
}
@inproceedings{Gao2015a,
author = {Gao, Yongbin and Lee, Hyo Jong},
booktitle = {2015 2nd International Conference on Information Science and Security (ICISS)},
doi = {10.1109/ICISSEC.2015.7371039},
file = {:home/raul/07371039.pdf:pdf},
isbn = {978-1-4673-8611-1},
keywords = {Biological neural networks,Computational modeling,Computer architecture,Feature extraction,Neurons,Vehicle detection,Vehicles,binary image,convolution,convolutional neural network,image filtering,image motion analysis,image recognition,intelligent traffic,license plate recognition,neural nets,symmetry filter,traffic engineering computing,vehicle analysis,vehicle make recognition},
month = {dec},
pages = {1--4},
publisher = {IEEE},
title = {{Vehicle Make Recognition Based on Convolutional Neural Network}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7371039},
year = {2015}
}
@article{Guyon2003,
author = {Guyon, Isabelle and Elisseeff, Andr{\'{e}}},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2003 - An introduction to variable and feature selection.pdf:pdf},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
month = {mar},
pages = {1157--1182},
publisher = {JMLR.org},
title = {{An introduction to variable and feature selection}},
url = {http://dl.acm.org/citation.cfm?id=944919.944968},
volume = {3},
year = {2003}
}
@article{Rodriguez2011,
abstract = {This paper presents an Efficient Distributed Genetic Algorithm for classification Rule extraction in data mining (EDGAR), which promotes a new method of data distribution in computer networks. This is done by spatial partitioning of the population into several semi-isolated nodes, each evolving in parallel and possibly exploring different regions of the search space. The presented algorithm shows some advantages when compared with other distributed algorithms proposed in the specific literature. In this way, some results are presented showing significant learning rate speedup without compromising the accuracy.},
author = {Rodr{\'{i}}guez, Miguel and Escalante, Diego M. and Peregr{\'{i}}n, Antonio},
doi = {10.1016/j.asoc.2009.12.035},
issn = {15684946},
journal = {Applied Soft Computing},
keywords = {Classification rules,Coarse-grained implementation,Distributed computing,Parallel genetic algorithms,Rule induction},
month = {jan},
number = {1},
pages = {733--743},
title = {{Efficient Distributed Genetic Algorithm for Rule extraction}},
url = {http://www.sciencedirect.com/science/article/pii/S1568494609002920},
volume = {11},
year = {2011}
}
@article{Felizardo2012,
abstract = {Context: Systematic Literature Reviews (SLRs) are an important component to identify and aggregate research evidence from different empirical studies. Despite its relevance, most of the process is conducted manually, implying additional effort when the Selection Review task is performed and leading to reading all studies under analysis more than once. Objective: We propose an approach based on Visual Text Mining (VTM) techniques to assist the Selection Review task in SLR. It is implemented into a VTM tool (Revis), which is freely available for use. Method: We have selected and implemented appropriate visualization techniques into our approach and validated and demonstrated its usefulness in performing real SLRs. Results: The results have shown that employment of VTM techniques can successfully assist in the Selection Review task, speeding up the entire SLR process in comparison to the conventional approach. Conclusion: VTM techniques are valuable tools to be used in the context of selecting studies in the SLR process, prone to speed up some stages of SLRs. {\textcopyright} 2012 Elsevier B.V. All rights reserved.},
author = {Felizardo, Katia R. and Andery, Gabriel F. and Paulovich, Fernando V. and Minghim, Rosane and Maldonado, Jos{\'{e}} C.},
doi = {10.1016/j.infsof.2012.04.003},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - A visual analysis approach to validate the selection review of primary studies in systematic reviews.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Citation document map,Content document map,Information visualization,Systematic Literature Review (SLR),Visual Text Mining (VTM)},
number = {10},
pages = {1079--1091},
publisher = {Elsevier B.V.},
title = {{A visual analysis approach to validate the selection review of primary studies in systematic reviews}},
url = {http://dx.doi.org/10.1016/j.infsof.2012.04.003},
volume = {54},
year = {2012}
}
@book{Cossio2012,
abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Cossio, Mar{\'{i}}a Laura T and Giesen, Laura F and Araya, Gabriela and P{\'{e}}rez-Cotapos, Mar{\'{i}}a Luisa S and VERGARA, RICARDO L{\'{O}}PEZ and Manca, Maura and Tohme, R. A. and Holmberg, S. D. and Bressmann, Tim and Lirio, Daniel Rodrigues and Rom{\'{a}}n, Jelitza Soto and Sol{\'{i}}s, Rodrigo Ganter and Thakur, Sanjay and Rao, SVD Nageswara and Modelado, E L and La, Artificial D E and Durante, Cabeza and Tradici{\'{o}}n, U N A and En, Maya and Espejo, E L and Fuentes, D E L A S and Yucat{\'{a}}n, Universidad Aut{\'{o}}noma De and Lenin, Cruz Moreno and Cian, Laura Franco and Douglas, M Joanne and Plata, La and H{\'{e}}ritier, Fran{\c{c}}oise},
booktitle = {Uma {\'{e}}tica para quantos?},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {9809069v1},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Probability{\_}{\_}Statistics{\_}{\_}and{\_}Stochastic{\_}Processes.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
keywords = {Adolescence,Adolescencia,Adolescent,Adolescent Behavior,Adolescent Behavior: psychology,Adult,Agresiones al cuerpo,Attachment to the body,Attaque au corps,Autolesiones deliberadas,Automutilation d{\'{e}}lib{\'{e}}r{\'{e}}e,Body Piercing,Body Piercing: psychology,Body Piercing: statistics {\&} numerical data,Body image,CUERPO,Chile,Chile: epidemiology,Cosmetic Techniques,Deliberate self-harm,Epidemiologic Methods,Female,Humans,Image corporelle,Imagen corporal,JUVENTUD,MODIFICACIONES CORPORALES,Male,Motivation,Movement,Risk-Taking,Self Mutilation,Self Mutilation: physiopathology,Self Mutilation: ultrasonography,Sex Distribution,Speech Articulation Tests,Speech Intelligibility,Tattooing,Tattooing: psychology,Tattooing: statistics {\&} numerical data,Tongue,Tongue: injuries,Tongue: physiopathology,Tongue: ultrasonography,aesthetics,and on cor-,as none were found,autoinjury and health,body,complications did not,complications from inserting a,constituci{\'{o}}n del yo,control postural- estabilizaci{\'{o}}n- v{\'{i}}as,corporal modifications,corps,cuerpo,culturas juveniles,cultures juv{\'{e}}niles,epidural,esth{\'{e}}tique,est{\'{e}}tica,find any reports of,high resolution images,if neuraxial anes-,ing with neuraxial anesthesia,jeunesse,juvenile cultures,juventud,mecanismos de anteroalimentaci{\'{o}}n y,modificacio -,needle through a,nes corporales,perforaci{\'{o}}n corporal,piel,pr{\'{a}}ctica autolesiva,psicoan{\'{a}}lisis,research,retroalimentaci{\'{o}}n,risks management,segunda piel,sensitivas y motoras,spinal,sustainable reconstruction,tattoo,tattooing,tattoos,tatuaje,the literature on tattoos,was reviewed to see,youth},
number = {2},
pages = {81--87},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
title = {{Probability{\_}{\_}Statistics{\_}{\_}and{\_}Stochastic{\_}Processes}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15003161$\backslash$nhttp://cid.oxfordjournals.org/lookup/doi/10.1093/cid/cir991$\backslash$nhttp://www.scielo.cl/pdf/udecada/v15n26/art06.pdf$\backslash$nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84861150233{\&}partnerID=tZOtx3y1},
volume = {XXXIII},
year = {2012}
}
@article{Garcia-Cabot2013,
author = {Garcia-Cabot, Antonio},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - A PROPOSAL OF A MULTI-AGENT SYSTEM FOR ADAPTING LEARNING CONTENTS TO USER COMPETENCES , CONTEXT AND MOBILE DEVICE Antonio GARCIA-.pdf:pdf},
keywords = {adaptation,competences,context,mobile device,multi-agent system},
pages = {18--23},
title = {{A PROPOSAL OF A MULTI-AGENT SYSTEM FOR ADAPTING LEARNING CONTENTS TO USER COMPETENCES , CONTEXT AND MOBILE DEVICE Antonio GARCIA-CABOT}},
volume = {21},
year = {2013}
}
@article{Puppe2008,
author = {Puppe, Frank and Atzmueller, Martin and Buscher, Georg and H{\"{u}}ttig, Matthias and Luehrs, Hardi and Buscher, Hans-Peter},
doi = {10.3233/978-1-58603-891-5-683},
isbn = {978-1-58603-891-5},
journal = {ECAI'08/PAIS'08: Proceedings of the 18th European Conference on Artificial Intelligence, including Prestigious Applications of Intelligent Systems},
pages = {683--687},
title = {{Application and Evaluation of a Medical Knowledge System in Sonography ({\{}{\{}{\}}SONOCONSULT{\{}{\}}{\}})}},
year = {2008}
}
@article{Kuhn2008,
abstract = {The caret package, short for classification and regression training, contains numerous tools for developing predictive models using the rich set of models available in R. The package focuses on simplifying model training and tuning across a wide variety of modeling techniques. It also includes methods for pre-processing training data, calculating variable importance, and model visualizations. An example from computational chemistry is used to illustrate the functionality on a real data set and to benchmark the benefits of parallel processing with several types of models.},
author = {Kuhn, Max},
doi = {10.1053/j.sodo.2009.03.002},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2008 - Building Predictive Models in R Using the caret Package.pdf:pdf},
isbn = {1548-7660},
issn = {15487660},
journal = {Journal Of Statistical Software},
keywords = {model building,networkspaces,parallel processing,r,tuning parameters},
number = {5},
pages = {1--26},
title = {{Building Predictive Models in R Using the caret Package}},
url = {http://www.jstatsoft.org/v28/i05/},
volume = {28},
year = {2008}
}
@article{Yusoff2015,
abstract = {This paper reports a systematic review of shared visualisation based on fifteen papers from 2000 to 2013. The findings identified five shared visualisation strategies that represent the ways implemented to process data sharing and knowledge to arrive at the desired level of understanding. Four visualisation techniques were also identified to show how shared cognition is made possible in designing tools for mediating data or knowledge among the users involved. These findings provide research opportunities in integrating rich interactive data visualisation for mobile-based technologies as an effective mean in supporting collaborative work. Finally, social, task and cognitive elements which can be significantly supported by shared visualisation and a guideline for future researchers seeking to design shared visualisation-based systems are presented.},
author = {Yusoff, Nor'ain Mohd and Salim, Siti Salwah},
doi = {10.1016/j.jvlc.2014.12.003},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - A systematic review of shared visualisation to achieve common ground.pdf:pdf},
isbn = {1045926X},
issn = {1045926X},
journal = {Journal of Visual Languages and Computing},
keywords = {Collaborative design,Human-computer interaction,Shared visualisation,Teamwork},
pages = {83--99},
publisher = {Elsevier},
title = {{A systematic review of shared visualisation to achieve common ground}},
url = {http://dx.doi.org/10.1016/j.jvlc.2014.12.003},
volume = {28},
year = {2015}
}
@article{Zhang2008,
author = {Zhang, Yi and Ding, Chris and Li, Tao},
doi = {10.1186/1471-2164-9-S2-S27},
issn = {1471-2164},
journal = {BMC Genomics},
number = {Suppl 2},
pages = {S27},
publisher = {BioMed Central},
title = {{Gene selection algorithm by combining reliefF and mRMR}},
url = {http://bmcgenomics.biomedcentral.com/articles/10.1186/1471-2164-9-S2-S27},
volume = {9},
year = {2008}
}
@article{Neumann2010,
author = {Neumann, Frank and Witt, Carsten},
doi = {10.1007/978-3-642-16544-3},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2010 - Bioinspired Computation in Combinatorial Optimization Algorithms and Their Computational Complexity (Natural Computing Series).pdf:pdf},
isbn = {3642165435},
pages = {214},
title = {{Bioinspired Computation in Combinatorial Optimization: Algorithms and Their Computational Complexity (Natural Computing Series)}},
url = {http://www.amazon.com/Bioinspired-Computation-Combinatorial-Optimization-Computational/dp/3642165435},
year = {2010}
}
@book{Karau,
author = {Karau, Holden and Konwinski, Andy and Wendell, Patrick and Zaharia, Matei},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Holden Karau, Andy Konwinski, Patrick Wendell {\&} Matei Zaharia.pdf:pdf},
isbn = {9781449358624},
title = {{Holden Karau, Andy Konwinski, Patrick Wendell {\{}{\&}{\}} Matei Zaharia}}
}
@article{Clark1991,
abstract = {The CN2 algorithm induces an ordered list of classification rules from examples using entropy as its search heuristic. In this short paper, we describe two improvements to this algorithm. Firstly, we present the use of the Laplacian error estimate as an alternative evaluation function and secondly, we show how unordered as well as ordered rules can be generated. We experimentally demonstrate significantly improved performances resulting from these changes, thus enhancing the usefulness of CN2 as an inductive tool. Comparisons with Quinlan's C4.5 are also made.},
author = {Clark, Peter and Boswell, Robin},
doi = {10.1007/BFb0016999},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1991 - Rule induction with CN2 Some recent improvements.pdf:pdf},
isbn = {3-540-53816-X},
issn = {354053816X},
journal = {Machine learning—EWSL-91},
pages = {151--163},
title = {{Rule induction with CN2: Some recent improvements}},
url = {http://link.springer.com/chapter/10.1007/BFb0017011},
year = {1991}
}
@article{Meng2015,
abstract = {Apache Spark is a popular open-source platform for large-scale data processing that is well-suited for iterative machine learning tasks. In this paper we present MLlib, Spark's open-source distributed machine learning library. MLlib provides efficient functionality for a wide range of learning settings and includes several underlying statistical, optimization, and linear algebra primitives. Shipped with Spark, MLlib supports several languages and provides a high-level API that leverages Spark's rich ecosystem to simplify the development of end-to-end machine learning pipelines. MLlib has experienced a rapid growth due to its vibrant open-source community of over 140 contributors, and includes extensive documentation to support further growth and to let users quickly get up to speed.},
archivePrefix = {arXiv},
arxivId = {1505.06807},
author = {Meng, Xiangrui and Bradley, Joseph and Yavuz, Burak and Sparks, Evan and Venkataraman, Shivaram and Liu, Davies and Freeman, Jeremy and Tsai, DB and Amde, Manish and Owen, Sean and Xin, Doris and Xin, Reynold and Franklin, Michael J. and Zadeh, Reza and Zaharia, Matei and Talwalkar, Ameet},
eprint = {1505.06807},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - MLlib Machine Learning in Apache Spark(2).pdf:pdf},
issn = {1532-4435},
journal = {Journal Of Machine Learning},
pages = {1--7},
title = {{MLlib: Machine Learning in Apache Spark}},
url = {http://www.jmlr.org/papers/volume17/15-237/15-237.pdf},
volume = {17},
year = {2015}
}
@article{Atzmueller2006,
author = {Atzmueller, Martin and Puppe, Frank},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2006 - SD-Map - A fast algorithm for exhaustive subgroup discovery.pdf:pdf},
isbn = {3-540-45374-1},
journal = {Knowledge Discovery in Databases: Pkdd 2006, Proceedings},
pages = {6--17},
title = {{SD-Map - A fast algorithm for exhaustive subgroup discovery}},
url = {{\textless}Go to ISI{\textgreater}://WOS:000241104900001},
volume = {4213},
year = {2006}
}
@article{Chen2007,
abstract = {Open source data mining software represents a new trend in data mining research, education and industrial applications, especially in small and medium enterprises (SMEs). With open source software an enterprise can easily initiate a data mining project using the most current technology. Often the software is available at no cost, allowing the enterprise to instead focus on ensuring their staff can freely learn the data mining techniques and methods. Open source ensures that staff can understand exactly how the algorithms work by examining the source codes, if they so desire, and can also fine tune the algorithms to suit the specific purposes of the enterprise. However, diversity, instability, scalability and poor documentation can be major concerns in using open source data mining systems. In this paper, we survey open source data mining systems currently available on the Internet. We compare 12 open source systems against several aspects such as general characteristics, data source accessibility, data mining functionality, and usability. We discuss advantages and disadvantages of these open source data mining systems.},
author = {Chen, Xiaojun and Ye, Yunming and Williams, Graham and Xu, Xiaofei},
doi = {10.1007/978-3-540-77018-3_2},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2007 - A Survey of Open Source Data Mining Systems.pdf:pdf},
isbn = {3-540-77016-X, 978-3-540-77016-9},
journal = {Emerging Technologies in Knowledge Discovery and Data Mining},
keywords = {data mining,floss,open source software},
number = {60603066},
pages = {3--14},
title = {{A Survey of Open Source Data Mining Systems}},
url = {http://dx.doi.org/10.1007/978-3-540-77018-3{\_}2},
year = {2007}
}
@article{Lavrac2004,
abstract = {This paper investigates how to adapt standard classification rule learning approaches to subgroup discovery. The goal of subgroup discovery is to find rules describing subsets of the population that are sufficiently large and statistically unusual. The paper presents a subgroup discovery algorithm, CN2-SD, developed by modifying parts of the CN2 classification rule learner: its covering algorithm, search heuristic, probabilistic classification of instances, and evaluation measures. Experimental evaluation of CN2-SD on 23 UCI data sets shows substantial reduction of the number of induced rules, increased rule coverage and rule significance, as well as slight improvements in terms of the area under ROC curve, when compared with the CN2 algorithm. Application of CN2-SD to a large traffic accident data set confirms these findings.},
author = {Lavra{\v{c}}, Nada and Kav{\v{s}}ek, Branko and Flach, Peter and Todorovski, Ljup{\v{c}}o},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2004 - Subgroup Discovery with CN2-SD.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
pages = {153--188},
title = {{Subgroup Discovery with CN2-SD}},
url = {http://portal.acm.org/citation.cfm?id=1005332.1005338$\backslash$nhttp://portal.acm.org/citation.cfm?id=1005338$\backslash$nhttp://portal.acm.org/ft{\_}gateway.cfm?id=1005338{\&}type=pdf{\&}CFID=16853164{\&}CFTOKEN=81605303},
volume = {5},
year = {2004}
}
@article{Version2005,
author = {Version, S a S},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2005 - Ser uide.pdf:pdf},
isbn = {1877787191},
number = {November},
pages = {1--17},
title = {{Ser uide}},
year = {2005}
}
@article{Zhang2007,
abstract = {BACKGROUND: Gene expression data usually contains a large number of genes, but a small number of samples. Feature selection for gene expression data aims at finding a set of genes that best discriminate biological samples of different types. In this paper, we present a two-stage selection algorithm by combining ReliefF and mRMR: In the first stage, ReliefF is applied to find a candidate gene set; In the second stage, mRMR method is applied to directly and explicitly reduce redundancy for selecting a compact yet effective gene subset from the candidate set. RESULTS: We perform comprehensive experiments to compare the mRMR-ReliefF selection algorithm with ReliefF, mRMR and other feature selection methods using two classifiers as SVM and Naive Bayes, on seven different datasets. And we also provide all source codes and datasets for sharing with others. CONCLUSION: The experimental results show that the mRMR-ReliefF gene selection algorithm is very effective.},
author = {Zhang, Yi and Ding, Chris and Li, Tao},
doi = {10.1109/BIBE.2007.4375560},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2007 - A two-stage gene selection algorithm by combining ReliefF and mRMR(2).pdf:pdf},
isbn = {1424415098},
issn = {1471-2164},
journal = {Proceedings of the 7th IEEE International Conference on Bioinformatics and Bioengineering, BIBE},
keywords = {Gene selection algorithms,ReliefF,mRMR,mRMR-reliefF},
pages = {164--171},
pmid = {18831793},
title = {{A two-stage gene selection algorithm by combining ReliefF and mRMR}},
volume = {10},
year = {2007}
}
@article{Rossum2012e,
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Logging Cookbook.pdf:pdf},
journal = {Info},
title = {{Logging Cookbook}},
year = {2012}
}
@book{Korhonen2008,
abstract = {In this paper we describe various visualization techniques which have been used or which might be useful in the multiple objective decision making frame- work. Several of the ideas originate from statistics, especially multivariate statistics. Some techniques are simply for illustrating snapshots of a single solution or a set of solutions. Others are used as an essential part of the human-computer interface.},
author = {Korhonen, Pekka and Wallenius, Jyrki},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-88908-3-8},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2008 - Visualization in the multiple objective decision-making framework.pdf:pdf},
isbn = {3540889078},
issn = {03029743},
pages = {195--212},
title = {{Visualization in the multiple objective decision-making framework}},
volume = {5252 LNCS},
year = {2008}
}
@article{Rossum2012c,
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - HOWTO Fetch Internet Resources Using The urllib Package.pdf:pdf},
journal = {Writing},
title = {{HOWTO Fetch Internet Resources Using The urllib Package}},
year = {2012}
}
@article{Anagnostopoulos2008,
abstract = {License plate recognition (LPR) algorithms in images or videos are generally composed of the following three processing steps: 1) extraction of a license plate region; 2) segmentation of the plate characters; and 3) recognition of each character. This task is quite challenging due to the diversity of plate formats and the nonuniform outdoor illumination conditions during image acquisition. Therefore, most approaches work only under restricted conditions such as fixed illumination, limited vehicle speed, designated routes, and stationary backgrounds. Numerous techniques have been developed for LPR in still images or video sequences, and the purpose of this paper is to categorize and assess them. Issues such as processing time, computational power, and recognition rate are also addressed, when available. Finally, this paper offers to researchers a link to a public image database to define a common reference point for LPR algorithmic assessment.},
author = {Anagnostopoulos, Christos Nikolaos E and Anagnostopoulos, Ioannis E. and Psoroulas, Ioannis D. and Loumos, Vassili and Kayafas, Eleftherios},
doi = {10.1109/TITS.2008.922938},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2008 - License plate recognition from still images and video sequences A survey.pdf:pdf},
isbn = {1524-9050},
issn = {15249050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
keywords = {Image processing,License plate identification,License plate recognition (LPR),License plate segmentation,Optical character recognition (OCR)},
number = {3},
pages = {377--391},
title = {{License plate recognition from still images and video sequences: A survey}},
volume = {9},
year = {2008}
}
@article{Dean2008,
abstract = {MapReduce is a programming model and an associated implementation for processing and generating large datasets that is amenable to a broad variety of real-world tasks. Users specify the computation in terms of a map and a reduce function, and the under- lying runtime system automatically parallelizes the computation across large-scale clusters of machines, handles machine failures, and schedules inter-machine communication to make effi- cient use of the network and disks. Programmers find the system easy to use: more than ten thousand distinct MapReduce programs have been implemented internally at Google over the past four years, and an average of one hundred thousand MapReduce jobs are executed on Google's clusters every day, processing a total of more than twenty petabytes of data per day.},
author = {Dean, Jeffrey and Ghemawat, Sanjay},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2008 - MapReduce Simplified Data Processing on Large Clusters.pdf:pdf},
journal = {Communications of the ACM},
number = {1},
pages = {107},
title = {{MapReduce: Simplified Data Processing on Large Clusters}},
url = {http://dl.acm.org/citation.cfm?id=1327452.1327492},
volume = {51},
year = {2008}
}
@article{Malinen2015,
abstract = {Online communities have become a popular and widely studied research topic. As active participation has been acknowledged as essential for the sustainability of the communities, research has focused largely on the most visible participants with the greatest financial value for community providers. However, users can engage with the sites in different ways, which calls for a more diverse classification of participation, instead of a simple active-passive dichotomy. This systematic literature review discusses empirical studies on online community participation. The results indicate that despite the large amount of research conducted on the topic, a theoretical and conceptual framework for user participation remains undefined as most of the research has approached participation in terms of its quantity. The complexity of online participation and its implications for methodology in future studies is discussed.},
author = {Malinen, Sanna},
doi = {10.1016/j.chb.2015.01.004},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Understanding user participation in online communities A systematic literature review of empirical studies.pdf:pdf},
isbn = {0747-5632},
issn = {07475632},
journal = {Computers in Human Behavior},
keywords = {Online communities,Systematic literature review,User participation},
pages = {228--238},
publisher = {Elsevier Ltd},
title = {{Understanding user participation in online communities: A systematic literature review of empirical studies}},
url = {http://dx.doi.org/10.1016/j.chb.2015.01.004},
volume = {46},
year = {2015}
}
@article{Kalousis2006,
abstract = {With the proliferation of extremely high-dimensional data, feature selection algorithms have become indispensable components of the learning process. Strangely, despite extensive work on the stability of learning algorithms, the stability of feature selection algorithms has been relatively neglected. This study is an attempt to fill that gap by quantifying the sensitivity of feature selection algorithms to variations in the training set. We assess the stability of feature selection algorithms based on the stability of the feature preferences that they express in the form of weights-scores, ranks, or a selected feature subset. We examine a number of measures to quantify the stability of feature preferences and propose an empirical way to estimate them. We perform a series of experiments with several feature selection algorithms on a set of proteomics datasets. The experiments allow us to explore the merits of each stability measure and create stability profiles of the feature selection algorithms. Finally, we show how stability profiles can support the choice of a feature selection algorithm.},
author = {Kalousis, Alexandros and Prados, Julien and Hilario, Melanie},
doi = {10.1007/s10115-006-0040-8},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2006 - Stability of feature selection algorithms a study on high-dimensional spaces.pdf:pdf},
issn = {0219-3116},
journal = {Knowledge and Information Systems},
number = {1},
pages = {95--116},
title = {{Stability of feature selection algorithms: a study on high-dimensional spaces}},
url = {http://dx.doi.org/10.1007/s10115-006-0040-8},
volume = {12},
year = {2006}
}
@article{Wu2014,
abstract = {This study attempts to develop a model satisfying the rules of a typical hospital environment based both on published research data and on requirements of a local hospital under study. A mathematical formulation for the studied nurse rostering problem (NRP) is presented first. Due to the combinatorial nature of the NRP model, a particle swarm optimization (PSO) approach is proposed to solve this highly complicated NRP. The structure of the problem constraints is analyzed and used as base for generating workstretch patterns. These patterns serve as the base for generating fast initial solutions, and will later be improved upon by the proposed PSO algorithm. This study also proposes a simple yet effective procedure for attempting possible refinements on the solutions obtained by the PSO before reporting the final solutions. When fair shift assignment is considered as the decision objective, computational results show that the proposed PSO algorithm with refinement procedure is able to produce optimal solutions in all real test problems in a very efficient manner.},
author = {Wu, Tai Hsi and Yeh, Jinn Yi and Lee, Yueh Min},
doi = {10.1016/j.cor.2014.08.016},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - A particle swarm optimization approach with refinement procedure for nurse rostering problem.pdf:pdf},
issn = {03050548},
journal = {Computers and Operations Research},
keywords = {Nurse rostering problem,PSO,Particle swarm optimization,Workstretch pattern},
pages = {52--63},
publisher = {Elsevier},
title = {{A particle swarm optimization approach with refinement procedure for nurse rostering problem}},
url = {http://dx.doi.org/10.1016/j.cor.2014.08.016},
volume = {54},
year = {2014}
}
@article{Hen2008,
abstract = {Data mining has becoming increasingly popular in helping to reveal important knowledge from the organization's databases and has led to the emergence of a variety of data mining tools to help in decision making. Present study described a test bed to investigate five major data mining tools, namely IBM intelligent miner, SPSS Clementine, SAS enterprise miner, oracle data miner and Microsoft business intelligence development studio. Present studies focus on the performance of these tools. Results provide a review of these tools and propose a data mining middleware adopting the strengths of the tools.},
author = {Hen, Lai Ee and Lee, Sai Peck},
doi = {10.3844/jcssp.2008.826.833},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2008 - Performance analysis of data mining tools cumulating with a proposed data mining middleware.pdf:pdf},
issn = {15493636},
journal = {Journal of Computer Science},
keywords = {Knowledge discovery,Performance metrics,Test bed},
number = {10},
pages = {826--833},
title = {{Performance analysis of data mining tools cumulating with a proposed data mining middleware}},
volume = {4},
year = {2008}
}
@article{Triguero2015,
abstract = {The application of data mining and machine learning techniques to biological and biomedicine data continues to be an ubiquitous research theme in current bioinformatics. The rapid advances in biotechnology are allowing us to obtain and store large quantities of data about cells, proteins, genes, etc., that should be processed. Moreover, in many of these problems such as contact map prediction, the problem tackled in this paper, it is difficult to collect representative positive examples. Learning under these circumstances, known as imbalanced big data classification, may not be straightforward for most of the standard machine learning methods. In this work we describe the methodology that won the ECBDL'14 big data challenge for a bioinformatics big data problem. This algorithm, named as ROSEFW-RF, is based on several MapReduce approaches to (1) balance the classes distribution through random oversampling, (2) detect the most relevant features via an evolutionary feature weighting process and a threshold to choose them, (3) build an appropriate Random Forest model from the pre-processed data and finally (4) classify the test data. Across the paper, we detail and analyze the decisions made during the competition showing an extensive experimental study that characterize the way of working of our methodology. From this analysis we can conclude that this approach is very suitable to tackle large-scale bioinformatics classifications problems.},
author = {Triguero, Isaac and del R{\'{i}}o, Sara and L{\'{o}}pez, Victoria and Bacardit, Jaume and Ben{\'{i}}tez, Jos{\'{e}} M. and Herrera, Francisco},
doi = {10.1016/j.knosys.2015.05.027},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - ROSEFW-RF The winner algorithm for the ECBDL'14 big data competition An extremely imbalanced big data bioinformatics problem.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Big data,Bioinformatics,Evolutionary feature selection,Hadoop,Imbalance classification,MapReduce},
month = {oct},
pages = {69--79},
title = {{ROSEFW-RF: The winner algorithm for the ECBDL'14 big data competition: An extremely imbalanced big data bioinformatics problem}},
url = {http://www.sciencedirect.com/science/article/pii/S0950705115002130},
volume = {87},
year = {2015}
}
@article{Cohen1999,
abstract = {some learner that produces a compact, understandable hypothesis--for instance, a rule learning system like CN2 (Clark {\&} Niblett 1989),  ( 1995), or },
author = {Cohen, W and Singer, Y},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1999 - A simple, fast, and effective rule learner.pdf:pdf},
isbn = {0-262-51106-1},
journal = {Proceedings of the National Conference on {\ldots}},
title = {{A simple, fast, and effective rule learner}},
url = {https://www.aaai.org/Papers/AAAI/1999/AAAI99-049.pdf$\backslash$npapers2://publication/uuid/A6C19F1A-905A-44E8-8A57-66D2A989FE86},
year = {1999}
}
@article{Patil2012,
author = {Patil, D J},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - HBR Data Scientist the sexiest job of the 21st century READ.pdf.pdf:pdf},
number = {October},
title = {{HBR Data Scientist the sexiest job of the 21st century READ.pdf}},
year = {2012}
}
@article{Rossum1995,
author = {Rossum, G. van ( Centrum voor Wiskunde en Informatica (CWI))},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1995 - Python tutorial.pdf:pdf},
title = {{Python tutorial}},
year = {1995}
}
@book{Yu2010,
abstract = {Evolutionary algorithms are becoming increasingly attractive across various disciplines, such as operations research, computer science, industrial engineering, electrical engineering, social science and economics. Introduction to Evolutionary Algorithms presents an insightful, comprehensive, and up-to-date treatment of evolutionary algorithms. It covers such hot topics as: • genetic algorithms, • differential evolution, • swarm intelligence, and • artificial immune systems. The reader is introduced to a range of applications, as Introduction to Evolutionary Algorithms demonstrates how to model real world problems, how to encode and decode individuals, and how to design effective search operators according to the chromosome structures with examples of constraint optimization, multiobjective optimization, combinatorial optimization, and supervised/unsupervised learning. This emphasis on practical applications will benefit all students, whether they choose to continue their academic career or to enter a particular industry. Introduction to Evolutionary Algorithms is intended as a textbook or self-study material for both advanced undergraduates and graduate students. Additional features such as recommended further reading and ideas for research projects combine to form an accessible and interesting pedagogical approach to this widely used discipline.},
author = {Yu, Xinjie and Gen, Mitsuo},
booktitle = {Decision Engineering},
doi = {10.1007/978-1-84996-129-5},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2010 - Introduction to Evolutionary Algorithms.pdf:pdf},
isbn = {9781849961295 1619-5736 ;},
issn = {1598-7248},
keywords = {Artificial Intelligence (incl. Robotics).,Artificial intelligence.,Complexity.,Computer simulation.,Control,Control engineering systems.,Engineering.,Mechatronics.,Physics.,Robotics,Simulation and Modeling.},
pmid = {302514},
title = {{Introduction to Evolutionary Algorithms}},
url = {http://www.springer.com/gp/book/9781849961288$\backslash$nhttp://ezproxy1.hw.ac.uk:2048/login?url=http://dx.doi.org/10.1007/978-1-84996-129-5 eBook available from SpringerLink to University staff and students - click here to access.$\backslash$nhttp://www.hw.ac.uk/library/acce},
year = {2010}
}
@article{Kuchling2011,
abstract = {This document is an introductory tutorial to using regular expressions in Python with the re module. It provides a gentler introduction than the corresponding section in the Library Reference. This document is available from http://www.amk.ca/python/howto.},
author = {Kuchling, A. M.},
doi = {10.1017/S0956796802004410},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2011 - Regular Expression HOWTO.pdf:pdf},
journal = {Test},
pages = {1--21},
title = {{Regular Expression HOWTO}},
year = {2011}
}
@article{Jin2008,
abstract = {The MapReduce programming model allows users to easily develop distributed applications in data centers. However, many applications cannot be exactly expressed with MapReduce due to their specific characteristics. For instance, genetic algorithms (GAs) naturally fit into an iterative style. That does not follow the two phase pattern of MapReduce. This paper presents an extension to the MapReduce model featuring a hierarchical reduction phase. This model is called MRPGA (MapReduce for parallel GAs), which can automatically parallelize GAs. We describe the design and implementation of the extended MapReduce model on a .NET-based enterprise grid system in detail. The evaluation of this model with its runtime system is presented using example applications.},
annote = {The map-reduce model is changed to fit the needs...},
author = {Jin, Chao and Vecchiola, Christian and Buyya, Rajkumar},
doi = {10.1109/eScience.2008.78},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2008 - MRPGA an extension of MapReduce for parallelizing Genetic Algorithms.pdf:pdf},
isbn = {9780769535357},
journal = {Proceedings - 4th IEEE International Conference on eScience, eScience 2008},
keywords = {10-2 cites},
mendeley-tags = {10-2 cites},
pages = {214--221},
title = {{MRPGA: an extension of MapReduce for parallelizing Genetic Algorithms}},
year = {2008}
}
@incollection{doi:10.1137/1.9781611972795.100,
abstract = {In this paper we examine the problem of efficient feature evaluation for logistic regression on very large data sets. We present a new forward feature selection heuristic that ranks features by their estimated effect on the resulting model's performance. An approximate optimization, based on backfitting, provides a fast and accurate estimate of each new feature's coefficient in the logistic regression model. Further, the algorithm is highly scalable by parallelizing simultaneously over both features and records, allowing us to quickly evaluate billions of potential features even for very large data sets.},
author = {Singh, Sameer and Kubica, Jeremy and Larsen, Scott and Sorokina, Daria},
booktitle = {Proceedings of the 2009 SIAM International Conference on Data Mining},
chapter = {99},
doi = {10.1137/1.9781611972795.100},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - Parallel large scale feature selection for logistic regression.pdf:pdf;:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - Parallel large scale feature selection for logistic regression(2).pdf:pdf},
isbn = {9781615671090},
pages = {1172--1183},
title = {{Parallel large scale feature selection for logistic regression}},
url = {http://www.additivegroves.net/papers/fslr.pdf$\backslash$nhttp://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/daria/papers/fslr.pdf},
year = {2009}
}
@article{Pawlak1998,
abstract = {This paper gives basic ideas of rough set theory a new approach to data analysis. The lower and uppe r approximation of a set, the basic ope rations of the theory, are intuitively explained and formally de fined. Some applica- tions of rough set theory are briefly outlined and some future problems are outline d.},
author = {Pawlak, Zdzis{\l}aw},
doi = {10.1080/019697298125470},
isbn = {0196972981},
issn = {01969722},
journal = {Journal of Telecommunications and Information Technology},
keywords = {churn modeling,decision rules,rough set},
number = {7},
pages = {7--10},
title = {{Rough set theory and its applications}},
url = {http://www.informaworld.com/openurl?genre=article{\{}{\&}{\}}doi=10.1080/019697298125470{\{}{\&}{\}}magic=crossref},
volume = {29},
year = {1998}
}
@article{Rossum2012b,
abstract = {It has always been possible to use Python for creating web sites, but it was a rather tedious task. Therefore, many frameworks and helper tools have been created to assist developers in creating faster and more robust sites. This HOWTO describes some of the methods used to combine Python with a web server to create dynamic content.$\backslash$r$\backslash$n$\backslash$r$\backslash$nThe Web Server Gateway Interface, or WSGI for short, is defined in PEP 333 and is currently the best way to do Python web programming. While it is great for programmers writing frameworks, a normal web developer does not need to get in direct contact with it. When choosing a framework for web development it is a good idea to choose one which supports WSGI.$\backslash$r$\backslash$n$\backslash$r$\backslash$nOriginally Python web frameworks tended to incorporate all of the services needed to develop web sites as a giant, integrated set of tools. No two web frameworks were interoperable: a program developed for one could not be deployed on a different one without considerable re-engineering work. This led to the development of “minimalist” web frameworks that provided just the tools to communicate between the Python code and the http protocol, with all other services to be added on top via separate components. Some ad hoc standards were developed that allowed for limited interoperability between frameworks, such as a standard that allowed different template engines to be used interchangeably.$\backslash$r$\backslash$n$\backslash$r$\backslash$nSince the advent of WSGI, the Python web framework world has been evolving toward interoperability based on the WSGI standard. Now many web frameworks, whether “full stack” (providing all the tools one needs to deploy the most complex web sites) or minimalist, or anything in between, are built from collections of reusable components that can be used with more than one framework.},
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - HOWTO Use Python in the web.pdf:pdf},
journal = {Interface},
title = {{HOWTO Use Python in the web}},
year = {2012}
}
@article{Peng2005,
abstract = {Feature selection is an important problem for pattern classification systems. We study how to select good features according to the maximal statistical dependency criterion based on mutual information. Because of the difficulty in directly implementing the maximal dependency condition, we first derive an equivalent form, called minimal-redundancy-maximal-relevance criterion (mRMR), for first-order incremental feature selection. Then, we present a two-stage feature selection algorithm by combining mRMR and other more sophisticated feature selectors (e.g., wrappers). This allows us to select a compact set of superior features at very low cost. We perform extensive experimental comparison of our algorithm and other methods using three different classifiers (naive Bayes, support vector machine, and linear discriminate analysis) and four different data sets (handwritten digits, arrhythmia, NCI cancer cell lines, and lymphoma tissues). The results confirm that mRMR leads to promising improvement on feature selection and classification accuracy.},
author = {Peng, Hanchuan and Long, Fuhui and Ding, Chris},
doi = {10.1109/TPAMI.2005.159},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2005 - Feature selection based on mutual information criteria of max-dependency, max-relevance, and min-redundancy.pdf:pdf},
issn = {0162-8828},
journal = {IEEE transactions on pattern analysis and machine intelligence},
keywords = {Algorithms,Artificial Intelligence,Automated,Automated: methods,Cluster Analysis,Computer Simulation,Computer-Assisted,Computer-Assisted: methods,Diagnosis,Humans,Information Storage and Retrieval,Information Storage and Retrieval: methods,Models,Neoplasms,Neoplasms: classification,Neoplasms: diagnosis,Numerical Analysis,Pattern Recognition,Statistical},
month = {aug},
number = {8},
pages = {1226--38},
pmid = {16119262},
shorttitle = {IEEE Transactions on Pattern Analysis and Machine},
title = {{Feature selection based on mutual information: criteria of max-dependency, max-relevance, and min-redundancy.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16119262},
volume = {27},
year = {2005}
}
@article{Clark1989,
author = {Clark, P and Niblett, T},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1989 - The {\{}CN{\}}2 rule induction algorithm.pdf:pdf},
journal = {Machine Learning},
keywords = {cn2,comprehensibility,concept learning,noise,rule induction},
number = {4},
pages = {261--284},
title = {{The {\{}{\{}{\}}CN{\{}{\}}{\}}2 rule induction algorithm}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.3672{\{}{\&}{\}}rep=rep1{\{}{\&}{\}}type=pdf},
volume = {3},
year = {1989}
}
@article{Version2005,
author = {Version, S a S},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2005 - Ser uide.pdf:pdf},
isbn = {1877787191},
number = {November},
pages = {1--17},
title = {{Ser uide}},
year = {2005}
}
@article{Beyer2002,
abstract = {This article gives a comprehensive introduction into one of the main branches of evolutionary computation – the evolution strategies (ES) the history of which dates back to the 1960s in Germany. Starting from a survey of history the philosophical background is explained in order to make understandable why ES are realized in the way they are. Basic ES algorithms and design principles for variation and selection operators as well as theoretical issues are presented, and future branches of ES research are discussed.},
author = {Beyer, Hans-Georg and Beyer, Hans-Georg and Schwefel, Hans-Paul and Schwefel, Hans-Paul},
doi = {10.1023/A:1015059928466},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2002 - Evolution strategies – A comprehensive introduction.pdf:pdf},
isbn = {1567-7818},
issn = {1572-9796},
journal = {Natural Computing},
keywords = {abbreviations,bbh,building block hypothesis,cma,computational intelligence,covariance matrix adaptation,csa,cumulative step-size adaptation,darwinian evolution,design principles for genetic,ea,ec,evolution strategies,evolutionary,evolutionary algorithm,evolutionary computation,operators,optimization},
number = {1},
pages = {3 -- 52},
title = {{Evolution strategies – A comprehensive introduction}},
url = {http://www.springerlink.com/content/2311qapbrwgrcyey},
volume = {1},
year = {2002}
}
@article{Rossum2013,
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - The PythonC API.pdf:pdf},
title = {{The Python/C API}},
year = {2013}
}
@article{Carmona2010,
abstract = {A non-dominated multiobjective evolutionary algorithm for extracting fuzzy rules in subgroup discovery (NMEEF-SD) is described and analyzed in this paper. This algorithm, which is based on the hybridization between fuzzy logic and genetic algorithms, deals with subgroup-discovery problems in order to extract novel and interpretable fuzzy rules of interest, and the evolutionary fuzzy system NMEEF-SD is based on the well-known Non-dominated Sorting Genetic Algorithm II (NSGA-II) model but is oriented toward the subgroup-discovery task using specific operators to promote the extraction of interpretable and high-quality subgroup-discovery rules. The proposal includes different mechanisms to improve diversity in the population and permits the use of different combinations of quality measures in the evolutionary process. An elaborate experimental study, which was reinforced by the use of nonparametric tests, was performed to verify the validity of the proposal, and the proposal was compared with other subgroup discovery methods. The results show that NMEEF-SD obtains the best results among several algorithms studied.},
author = {Carmona, Cristbal Jos?? and Gonz??lez, Pedro and Jesus, Mar??a Jos?? Del and Herrera, Francisco},
doi = {10.1109/TFUZZ.2010.2060200},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2010 - NMEEF-SD Non-dominated multiobjective evolutionary algorithm for extracting fuzzy rules in subgroup discovery.pdf:pdf},
issn = {10636706},
journal = {IEEE Transactions on Fuzzy Systems},
keywords = {Descriptive rule induction,fuzzy rules,genetic fuzzy system,multiobjective evolutionary algorithm,subgroup discovery},
number = {5},
pages = {958--970},
title = {{NMEEF-SD: Non-dominated multiobjective evolutionary algorithm for extracting fuzzy rules in subgroup discovery}},
volume = {18},
year = {2010}
}
@book{Fallis2013a,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fallis, A.G},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - No Title No Title(4).pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{No Title No Title}},
volume = {53},
year = {2013}
}
@article{Yang2014,
author = {Yang, Linjie and Luo, Ping and Loy, Chen Change and Tang, Xiaoou},
file = {:home/raul/CompCars.pdf:pdf},
title = {{A Large-Scale Car Dataset for Fine-Grained Categorization and Verification}},
volume = {1},
year = {2014}
}
@article{Lecun1998,
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
doi = {10.1109/5.726791},
eprint = {1102.0183},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Gradient-Based Learning Applied to Document Recognition.pdf:pdf},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {2D shape variability,Character recognition,Feature extraction,GTN,Hidden Markov models,Machine learning,Multi-layer neural network,Neural networks,Optical character recognition software,Optical computing,Pattern recognition,Principal component analysis,back-propagation,backpropagation,cheque reading,complex decision surface synthesis,convolution,convolutional neural network character recognizers,document recognition,document recognition systems,field extraction,gradient based learning technique,gradient-based learning,graph transformer networks,handwritten character recognition,handwritten digit recognition task,high-dimensional patterns,language modeling,multilayer neural networks,multilayer perceptrons,multimodule systems,optical character recognition,performance measure minimization,segmentation recognition},
number = {11},
pages = {2278--2324},
pmid = {15823584},
publisher = {IEEE},
title = {{Gradient-Based Learning Applied to Document Recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=726791},
volume = {86},
year = {1998}
}
@article{Meeng2011,
author = {Meeng, Marvin and Knobbe, Arno},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2011 - Flexible Enrichment with Cortana – Software Demo.pdf:pdf},
journal = {Proceedings 20th Anual Belgian-Dutch Conference on Machine Learning (BENELEARN 2011)},
keywords = {enrichment,software,subgroup discovery},
pages = {117--120},
title = {{Flexible Enrichment with Cortana – Software Demo}},
year = {2011}
}
@article{Bolon-Canedo2015a,
abstract = {In an era of growing data complexity and volume and the advent of big data, feature selection has a key role to play in helping reduce high-dimensionality in machine learning problems. We discuss the origins and importance of feature selection and outline recent contributions in a range of applications, from DNA microarray analysis to face recognition. Recent years have witnessed the creation of vast datasets and it seems clear that these will only continue to grow in size and number. This new big data scenario offers both opportunities and challenges to feature selection researchers, as there is a growing need for scalable yet efficient feature selection methods, given that existing methods are likely to prove inadequate.},
author = {Bol{\'{o}}n-Canedo, V. and S{\'{a}}nchez-Maro{\~{n}}o, N. and Alonso-Betanzos, A.},
doi = {10.1016/j.knosys.2015.05.014},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Recent advances and emerging challenges of feature selection in the context of big data.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Big data,Feature selection,High dimensionality},
pages = {33--45},
title = {{Recent advances and emerging challenges of feature selection in the context of big data}},
volume = {86},
year = {2015}
}
@article{Fayyad1996,
abstract = {Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges involved in real-world applications of knowledge discovery, and current and future research directions in the field. Copyright {\textcopyright} 1996, American Association for Artificial Intelligence. All rights reserved.},
archivePrefix = {arXiv},
arxivId = {aimag.v17i3.1230},
author = {Fayyad, Usama and Piatetsky-Shapiro, G and Smyth, Padhraic},
doi = {10.1145/240455.240463},
eprint = {aimag.v17i3.1230},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1996 - From data mining to knowledge discovery in databases.pdf:pdf},
isbn = {0-262-56097-6},
issn = {0738-4602},
journal = {AI magazine},
pages = {37--54},
pmid = {12948721},
title = {{From data mining to knowledge discovery in databases}},
url = {http://www.aaai.org/ojs/index.php/aimagazine/article/viewArticle/1230},
year = {1996}
}
@book{Summerfield2010,
abstract = {{\textless}div{\textgreater}{\textless}h3{\textgreater}Review{\textless}/h3{\textgreater}{\textless}p{\textgreater}{\textless}em{\textgreater}“{\ldots}Fulfills the immediate market need for those developers seeking to learn this latest evolutionary version of the Python lineage in a succinct, well-written package.”{\textless}/em{\textgreater}{\textless}/p{\textgreater}{\textless}p{\textgreater}{\textless}em{\textgreater}–Mike Riley, Contributing Editor for Dr. Dobb's{\textless}/em{\textgreater}{\textless}/p{\textgreater}{\textless}p{\textgreater}{\textless}em{\textgreater} {\textless}/em{\textgreater}{\textless}/p{\textgreater}{\textless}p{\textgreater}{\textless}em{\textgreater}“Beyond the introduction to programming in Python 3 in the first chapter, if you progress through the first six chapters in sequence, you'll be well on your way to taking off with using Python independently.”{\textless}/em{\textgreater}{\textless}/p{\textgreater}{\textless}p{\textgreater}{\textless}em{\textgreater}–James Pyles, Technical Writer and Author of the blog “A Million Chimpanzees”{\textless}/em{\textgreater}{\textless}/p{\textgreater}{\textless}p{\textgreater}{\textless}em{\textgreater} {\textless}/em{\textgreater}{\textless}/p{\textgreater}{\textless}p{\textgreater}{\textless}em{\textgreater}“{\ldots}A key recommendation for any serious computer library strong in web programming languages.”{\textless}/em{\textgreater}{\textless}/p{\textgreater}{\textless}p{\textgreater}{\textless}em{\textgreater}–Jim Cox, Midwest Book Review{\textless}/em{\textgreater}{\textless}/p{\textgreater}{\textless}p{\textgreater}{\textless}em{\textgreater} {\textless}/em{\textgreater}{\textless}/p{\textgreater}{\textless}p{\textgreater}{\textless}em{\textgreater}“Summerfield's book is an excellent source to start learning Python 3.”{\textless}/em{\textgreater}{\textless}/p{\textgreater}{\textless}p{\textgreater}{\textless}em{\textgreater}–Anthony J. Duben, Computing Reviews{\textless}/em{\textgreater}{\textless}/p{\textgreater}{\textless}h3{\textgreater}Product Description{\textless}/h3{\textgreater}{\textless}p{\textgreater}{\textless}strong{\textgreater}A Fully Revised Edition Featuring New Material on Coroutines, Debugging, Testing, Parsing, String Formatting, and More{\textless}/strong{\textgreater}{\textless}/p{\textgreater}{\textless}p{\textgreater}Python 3 is the best version of the language yet: It is more powerful, convenient, consistent, and expressive than ever before. Now, leading Python programmer Mark Summerfield demonstrates how to write code that takes full advantage of Python 3's features and idioms. {\textless}strong{\textgreater}{\textless}em{\textgreater}Programming in Python 3, Second Edition, {\textless}/em{\textgreater}{\textless}/strong{\textgreater}brings together all the knowledge you need to write any program, use any standard or third-party Python 3 library, and create new library modules of your own.{\textless}/p{\textgreater}{\textless}p{\textgreater}Summerfield draws on his many years of Python experience to share deep insights into Python 3 development you won't find anywhere else. He begins by illuminating Python's "beautiful heart": the eight key elements of Python you need to write robust, high-performance programs. Building on these core elements, he introduces new topics designed to strengthen your practical expertise-one concept and hands-on example at a time. Coverage includes{\textless}/p{\textgreater}{\textless}ul{\textgreater}{\textless}li{\textgreater}     Developing in Python using procedural, objectoriented, and functional programming paradigms  {\textless}/li{\textgreater} {\textless}li{\textgreater}     Creating custom packages and modules  {\textless}/li{\textgreater} {\textless}li{\textgreater}     Writing and reading binary, text, and XML files, including optional compression, random access, and text and XML parsing  {\textless}/li{\textgreater} {\textless}li{\textgreater}     Leveraging advanced data types, collections, control structures, and functions  {\textless}/li{\textgreater} {\textless}li{\textgreater}     Spreading program workloads across multiple processes and threads  {\textless}/li{\textgreater} {\textless}li{\textgreater}     Programming SQL databases and key--value DBM files  {\textless}/li{\textgreater} {\textless}li{\textgreater}     Debugging techniques-and using Test Driven Development to avoid bugs in the first place  {\textless}/li{\textgreater} {\textless}li{\textgreater}     Utilizing Python's regular expression mini-language and module  {\textless}/li{\textgreater} {\textless}li{\textgreater}     Parsing techniques, including how to use the third-party PyParsing and PLY modules  {\textless}/li{\textgreater} {\textless}li{\textgreater}     Building usable, efficient, GUI-based applications  {\textless}/li{\textgreater} {\textless}li{\textgreater}     Advanced programming techniques, including generators, function and class decorators, context managers, descriptors, abstract base classes, metaclasses, coroutines, and more {\textless}/li{\textgreater}{\textless}/ul{\textgreater}{\textless}p{\textgreater}{\textless}em{\textgreater} {\textless}/em{\textgreater}{\textless}/p{\textgreater}{\textless}p{\textgreater}{\textless}strong{\textgreater}{\textless}em{\textgreater}Programming in Python 3, Second Edition, {\textless}/em{\textgreater}{\textless}/strong{\textgreater}serves as both tutorial and language reference. It assumes some prior programming experience, and is accompanied by extensive downloadable example code-all of it tested with Python 3 on Windows, Linux, and Mac OS X. This edition covers Python 3.0 and 3.1, and due to the Python language moratorium it is also valid for Python 3.2 which has the same {\textless}em{\textgreater}language{\textless}/em{\textgreater} as Python 3.1. {\textless}/p{\textgreater}{\textless}/div{\textgreater}},
author = {Summerfield, Mark},
booktitle = {Text},
doi = {9788441526136},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2010 - Programming in Python 3.pdf:pdf},
isbn = {9780321680563},
issn = {9780137129294 0137129297},
keywords = {ISBN-13:    9780321680563},
pages = {644},
title = {{Programming in Python 3}},
year = {2010}
}
@incollection{Eiras-Franco2015,
address = {Albacete, Spain},
author = {Eiras-Franco, Carlos and Bol{\'{o}}n-Canedo, Ver{\'{o}}nica and Ramos, Sabela and Gonz{\'{a}}lez-Dom{\'{i}}guez, Jorge and Alonso-Betanzos, Amparo and Touri{\~{n}}o, Juan},
booktitle = {Actas de la XVI Conferencia de la Asociaci{\'{o}}n Espa{\~{n}}ola para la Inteligencia Artificial},
file = {:home/raul/Downloads/00949.pdf:pdf},
title = {{Paralelizaci{\'{o}}n de algoritmos de selecci{\'{o}}n de caracter{\'{i}}sticas en la plataforma Weka}},
year = {2015}
}
@article{Rossum2014,
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - Sorting HOW TO.pdf:pdf},
pages = {1--6},
title = {{Sorting HOW TO}},
year = {2014}
}
@misc{Alonzo2015,
author = {Alonzo, Amparo},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Feature Selection for Big Data.pdf:pdf},
title = {{Feature Selection for Big Data}},
year = {2015}
}
@article{Chen2013,
abstract = {Privacy preserving data mining algorithms are crucial for the personal data analysis, such as medical and financial records. This paper focuses on feature selection and proposes a new privacy preserving distributed algorithm, which can effectively select features based on differential privacy and Gini index under the MapReduce framework. At the same time, the theoretic analysis for privacy guarantee is also presented. Some experiments are conducted on bench-mark datasets, the simulation results indicate that during the selection of important features, the proposed algorithm can preserve privacy information to a certain extent with less time cost than on centralized counterpart.},
author = {Chen, Kai and Wan, Wen-qiang and Li, Yun},
doi = {10.1016/S1005-8885(13)60094-1},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - Differentially private feature selection under MapReduce framework.pdf:pdf},
issn = {10058885},
journal = {The Journal of China Universities of Posts and Telecommunications},
keywords = {MapReduce,data mining,differential privacy,feature selection},
month = {oct},
number = {5},
pages = {85--103},
title = {{Differentially private feature selection under MapReduce framework}},
url = {http://www.sciencedirect.com/science/article/pii/S1005888513600941},
volume = {20},
year = {2013}
}
@article{Nebro2014,
author = {Nebro, Aj and Durillo, Jj},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - jMetal 4.5 User Manual.pdf:pdf},
title = {{jMetal 4.5 User Manual}},
url = {http://ftp.jaist.ac.jp/pub/sourceforge/j/project/jm/jmetal/jmetal4.5/jmetal4.5.userManual.pdf},
year = {2014}
}
@article{Bania2015,
author = {Bania, R. K.},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Comparative Review on Classical Rough Set Theory based Feature Selection Methods.pdf:pdf},
journal = {International Journal of Computer Applications},
number = {19},
pages = {31--35},
publisher = {Foundation of Computer Science (FCS)},
title = {{Comparative Review on Classical Rough Set Theory based Feature Selection Methods}},
url = {http://www.ijcaonline.org/archives/volume114/number19/20089-2125},
volume = {114},
year = {2015}
}
@article{Atzmueller2009,
author = {Atzmueller, Martin and Lemmerich, Florian},
doi = {10.1007/978-3-642-04125-9_7},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - Fast subgroup discovery for continuous target concepts.pdf:pdf},
isbn = {3642041248},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {35--44},
title = {{Fast subgroup discovery for continuous target concepts}},
volume = {5722 LNAI},
year = {2009}
}
@article{Kari2008,
abstract = {Natural computing builds a bridge between computer science and natural sciences.},
author = {Kari, Lila and Rozenberg, Grzegorz},
doi = {10.1145/1400181.1400200},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2008 - The many facets of natural computing.pdf:pdf},
isbn = {3540679219},
issn = {00010782},
journal = {Communications of the ACM},
number = {10},
pages = {72},
title = {{The many facets of natural computing}},
volume = {51},
year = {2008}
}
@article{Level,
author = {Level, T H E Minus},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Dive into 3.pdf:pdf},
keywords = {apter -1,d ive i nto,n,p ython,s n ew i,w hat},
title = {{Dive into 3}}
}
@article{Brereton2007,
abstract = {A consequence of the growing number of empirical studies in software engineering is the need to adopt systematic approaches to assessing and aggregating research outcomes in order to provide a balanced and objective summary of research evidence for a particular topic. The paper reports experiences with applying one such approach, the practice of systematic literature review, to the published studies relevant to topics within the software engineering domain. The systematic literature review process is summarised, a number of reviews being undertaken by the authors and others are described and some lessons about the applicability of this practice to software engineering are extracted. The basic systematic literature review process seems appropriate to software engineering and the preparation and validation of a review protocol in advance of a review activity is especially valuable. The paper highlights areas where some adaptation of the process to accommodate the domain-specific characteristics of software engineering is needed as well as areas where improvements to current software engineering infrastructure and practices would enhance its applicability. In particular, infrastructure support provided by software engineering indexing databases is inadequate. Also, the quality of abstracts is poor; it is usually not possible to judge the relevance of a study from a review of the abstract alone. ?? 2006 Elsevier Inc. All rights reserved.},
author = {Brereton, Pearl and Kitchenham, Barbara A. and Budgen, David and Turner, Mark and Khalil, Mohamed},
doi = {10.1016/j.jss.2006.07.009},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2007 - Lessons from applying the systematic literature review process within the software engineering domain.pdf:pdf},
isbn = {0164-1212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Empirical software engineering,Systematic literature review},
number = {4},
pages = {571--583},
publisher = {Elsevier Inc.},
title = {{Lessons from applying the systematic literature review process within the software engineering domain}},
url = {http://dx.doi.org/10.1016/j.jss.2006.07.009},
volume = {80},
year = {2007}
}
@article{Hong1997,
address = {Piscataway, NJ, USA},
author = {Hong, Se June},
doi = {10.1109/69.634751},
issn = {1041-4347},
journal = {IEEE Trans. on Knowl. and Data Eng.},
keywords = {Feature analysis,classification modeling,discretization,feature merit,feature selection.},
month = {sep},
number = {5},
pages = {718--730},
publisher = {IEEE Educational Activities Department},
title = {{Use of Contextual Information for Feature Ranking and Discretization}},
url = {http://dx.doi.org/10.1109/69.634751},
volume = {9},
year = {1997}
}
@article{Du2013,
abstract = {Automatic license plate recognition (ALPR) is the extraction of vehicle license plate information from an image or a sequence of images. The extracted information can be used with or without a database in many applications, such as electronic payment systems (toll payment, parking fee payment), and freeway and arterial monitoring systems for traffic surveillance. The ALPR uses either a color, black and white, or infrared camera to take images. The quality of the acquired images is a major factor in the success of the ALPR. ALPR as a real-life application has to quickly and successfully process license plates under different environmental conditions, such as indoors, outdoors, day or night time. It should also be generalized to process license plates from different nations, provinces, or states. These plates usually contain different colors, are written in different languages, and use different fonts; some plates may have a single color background and others have background images. The license plates can be partially occluded by dirt, lighting, and towing accessories on the car. In this paper, we present a comprehensive review of the state-of-the-art techniques for ALPR. We categorize different ALPR techniques according to the features they used for each stage, and compare them in terms of pros, cons, recognition accuracy, and processing speed. Future forecasts of ALPR are given at the end.},
author = {Du, Shan and Ibrahim, Mahmoud and Shehata, Mohamed and Badawy, Wael},
doi = {10.1109/TCSVT.2012.2203741},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - Automatic license plate recognition (ALPR) A state-of-the-art review.pdf:pdf},
isbn = {1051-8215 VO  - 23},
issn = {10518215},
journal = {IEEE Transactions on Circuits and Systems for Video Technology},
keywords = {Automatic license plate recognition (ALPR),automatic number plate recognition (ANPR),car plate recognition (CPR),optical character recognition (OCR) for cars},
number = {2},
pages = {311--325},
title = {{Automatic license plate recognition (ALPR): A state-of-the-art review}},
volume = {23},
year = {2013}
}
@article{Monroy2014,
abstract = {Introducci{\{}{\'{o}}{\}}n: La investigaci{\{}{\'{o}}{\}}n educativa desde la perspectiva de la corriente SAL (Students´ Approaches to Learning) ha explorado los mecanismos implicados en el aprendizaje, as{\{}{\'{i}}{\}} como los factores que influyen en los enfoques de aprendizaje. Partiendo del modelo 3P de Biggs (1987) y de la revisi{\{}{\'{o}}{\}}n de factores de Baeten, Kyndt, Struyven, y Dochy (2010), el presente estudio realiz{\{}{\'{o}}{\}} una revisi{\{}{\'{o}}{\}}n sistem{\{}{\'{a}}{\}}tica para seleccionar y recuperar evidencias emp{\{}{\'{i}}{\}}ricas de factores personales y contextuales que pueden influir en los enfoques de aprendizaje. M{\{}{\'{e}}{\}}todo: Se sigui{\{}{\'{o}}{\}} el procedimiento de revisi{\{}{\'{o}}{\}}n sistem{\{}{\'{a}}{\}}tica y se extrajeron estudios en funci{\{}{\'{o}}{\}}n de unos criterios de selecci{\{}{\'{o}}{\}}n determinados. Resultados: Muchos de los estudios seleccionados coinciden en sus resultados y evidencian la influencia de diversos factores (motivaci{\{}{\'{o}}{\}}n, conocimientos previos, concepciones de aprendizaje, creencias epistemol{\{}{\'{o}}{\}}gicas, cantidad de trabajo, percepci{\{}{\'{o}}{\}}n de los criterios de evaluaci{\{}{\'{o}}{\}}n, concepciones y m{\{}{\'{e}}{\}}todos de ense{\{}{\~{n}}{\}}anza de los profesores, titulaci{\{}{\'{o}}{\}}n y tipo de tarea y contenido) sobre los enfoques de aprendizaje. Sin embargo, no existen resultados concluyentes sobre otras variables (sexo, edad, cociente intelectual, tipo de evaluaci{\{}{\'{o}}{\}}n). Conclusiones: Se observan desequilibrios en la atenci{\{}{\'{o}}{\}}n prestada al estudio de determinadas variables, as{\{}{\'{i}}{\}} que la presente revisi{\{}{\'{o}}{\}}n puede servir de gu{\{}{\'{i}}{\}}a en cuanto a qu{\{}{\'{e}}{\}} factores requieren mayor nivel de an{\{}{\'{a}}{\}}lisis en futuras investigaciones. El conocimiento fundado sobre los factores involucrados en el aprendizaje universitario permitir{\{}{\'{i}}{\}}a desarrollar acciones que mejoren cualitativamente el aprendizaje y promuevan el aprendizaje significativo.},
author = {Monroy, Fuensanta and Pina, Fuensanta Hern{\'{a}}ndez},
doi = {10.5944/educxx1.17.2.11481},
isbn = {1139-613X},
issn = {2174-5374},
journal = {Educaci{\{}{\'{o}}{\}}n XX1},
keywords = {aprendizaje,enfoques de aprendizaje,ense{\{}{\~{n}}{\}}anza superior,proceso de aprendizaje,revisi{\{}{\'{o}}{\}}n,universidad},
number = {2},
pages = {105--124},
title = {{Factores que influyen en los enfoques de aprendizaje universitario. Una revisi{\{}{\'{o}}{\}}n sistem{\{}{\'{a}}{\}}tica}},
url = {http://e-spacio.uned.es/revistasuned/index.php/educacionXX1/article/view/11481},
volume = {17},
year = {2014}
}
@article{H??st2011,
abstract = {Context: The popularity of the open source software development in the last decade, has brought about an increased interest from the industry on how to use open source components, participate in the open source community, build business models around this type of software development, and learn more about open source development methodologies. There is a need to understand the results of research in this area. Objective: Since there is a need to understand conducted research, the aim of this study is to summarize the findings of research that has ben carried out on usage of open source components and development methodologies by the industry, as well as companies' participation in the open source community. Method: Systematic review through searches in library databases and manual identification of articles from the open source conference. The search was first carried out in May 2009 and then once again in May 2010. Results: In 2009, 237 articles were first found, from which 19 were selected based on content and quality, and in 2010, 76 new articles were found from which four were selected. Twenty three articles were identified in total. Conclusions: The articles could be divided into four categories: open source as part of component based software engineering, business models with open source in commercial organization, company participation in open source development communities, and usage of open source processes within a company. ?? 2010 Elsevier B.V. All rights reserved.},
author = {H??st, Martin and Oru??evi??-Alagi??, Alma},
doi = {10.1016/j.infsof.2010.12.009},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2011 - A systematic review of research on open source software in commercial software product development.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Business models,Commercial,Component based software engineering,Open source software,Proprietary},
number = {6},
pages = {616--624},
title = {{A systematic review of research on open source software in commercial software product development}},
volume = {53},
year = {2011}
}
@article{Mampaey2012,
abstract = {Subgroup discovery systems are concerned with finding interesting patterns in labeled data. How these systems deal with numeric and nominal data has a large impact on the quality of their results. In this paper, we consider two ways to extend the standard pattern language of subgroup discovery: using conditions that test for interval membership for numeric attributes, and value set membership for nominal attributes. We assume a greedy search setting, that is, iteratively refining a given subgroup, with respect to a (convex) quality measure. For numeric attributes, we propose an algorithm that finds the optimal interval in linear (rather than quadratic) time, with respect to the number of examples and split points. Similarly, for nominal attributes, we show that finding the optimal set of values can be achieved in linear (rather than exponential) time, with respect to the number of examples and the size of the domain of the attribute. These algorithms operate by only considering subgroup refinements that lie on a convex hull in ROC space, thus significantly narrowing down the search space. We further provide efficient algorithms specifically for the popular Weighted Relative Accuracy quality measure, taking advantage of some of its properties. Our algorithms are shown to perform well in practice, and furthermore provide additional expressive power leading to higher-quality results.},
author = {Mampaey, Michael and Nijssen, Siegfried and Feelders, Ad and Knobbe, Arno},
doi = {10.1109/ICDM.2012.117},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Efficient algorithms for finding richer subgroup descriptions in numeric and nominal data.pdf:pdf},
isbn = {9780769549057},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
keywords = {Convex functions,Nominal data,Numeric data,ROC analysis,Subgroup discovery},
pages = {499--508},
title = {{Efficient algorithms for finding richer subgroup descriptions in numeric and nominal data}},
year = {2012}
}
@article{Kitchenham2007,
abstract = {The objective of this report is to propose comprehensive guidelines for systematic literature reviews appropriate for software engineering researchers, including PhD students. A systematic literature review is a means of evaluating and interpreting all available research relevant to a particular research question, topic area, or phenomenon of interest. Systematic reviews aim to present a fair evaluation of a research topic by using a trustworthy, rigorous, and auditable methodology. The guidelines presented in this report were derived from three existing guidelines used by medical researchers, two books produced by researchers with social science backgrounds and discussions with researchers from other disciplines who are involved in evidence-based practice. The guidelines have been adapted to reflect the specific problems of software engineering research. The guidelines cover three phases of a systematic literature review: planning the review, conducting the review and reporting the review. They provide a relatively high level description. They do not consider the impact of the research questions on the review procedures, nor do they specify in detail the mechanisms needed to perform meta-analysis.},
author = {Kitchenham, Barbara and Charters, S},
doi = {10.1145/1134285.1134500},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2007 - Guidelines for performing Systematic Literature Reviews in Software Engineering.pdf:pdf},
isbn = {1595933751},
issn = {00010782},
journal = {Engineering},
pages = {1051},
title = {{Guidelines for performing Systematic Literature Reviews in Software Engineering}},
volume = {2},
year = {2007}
}
@article{Press2002,
author = {Press, IOS},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2002 - Projection-based measure for efficient feature selection.pdf:pdf},
issn = {10641246},
journal = {Journal of Intelligent and Fuzzy Systems},
number = {3},
pages = {175--183},
title = {{Projection-based measure for efficient feature selection}},
url = {http://iospress.metapress.com/index/JJTDKMLUV1AE1M81.pdf},
volume = {12},
year = {2002}
}
@article{Rushing2005,
abstract = {Algorithm Development and Mining (ADaM) is a data mining toolkit designed for use with scientific data. It provides classification, clustering and association rule mining methods that are common to many data mining systems. In addition, it provides feature reduction capabilities, image processing, data cleaning and preprocessing capabilities that are of value when mining scientific data. The toolkit is packaged as a suite of independent components, which are designed to work in grid and cluster environments. The toolkit is extensible and scalable, and has been successfully used in several diverse data mining applications. ADaM has also been used in conjunction with other data mining toolkits and with point tools. This paper presents the architecture and design of the ADaM toolkit and discusses its application in detecting cumulus cloud fields in satellite imagery. ?? 2004 Elsevier Ltd. All rights reserved.},
author = {Rushing, John and Ramachandran, Rahul and Nair, Udaysankar and Graves, Sara and Welch, Ron and Lin, Hong},
doi = {10.1016/j.cageo.2004.11.009},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2005 - ADaM A data mining toolkit for scientists and engineers.pdf:pdf},
isbn = {0098-3004},
issn = {00983004},
journal = {Computers and Geosciences},
keywords = {ADaM,Data mining,Grid computing,Python},
number = {5},
pages = {607--618},
title = {{ADaM: A data mining toolkit for scientists and engineers}},
volume = {31},
year = {2005}
}
@article{Wang2007,
abstract = {We propose a new feature selection strategy based on rough sets and particle swarm optimization (PSO). Rough sets have been used as a feature selection method with much success, but current hill-climbing rough set approaches to feature selection are inadequate at finding optimal reductions as no perfect heuristic can guarantee optimality. On the other hand, complete searches are not feasible for even medium-sized datasets. So, stochastic approaches provide a promising feature selection mechanism. Like Genetic Algorithms, PSO is a new evolutionary computation technique, in which each potential solution is seen as a particle with a certain velocity flying through the problem space. The Particle Swarms find optimal regions of the complex search space through the interaction of individuals in the population. PSO is attractive for feature selection in that particle swarms will discover best feature combinations as they fly within the subset space. Compared with GAs, PSO does not need complex operators such as crossover and mutation, it requires only primitive and simple mathematical operators, and is computationally inexpensive in terms of both memory and runtime. Experimentation is carried out, using UCI data, which compares the proposed algorithm with a GA-based approach and other deterministic rough set reduction algorithms. The results show that PSO is efficient for rough set-based feature selection.},
annote = {{\textless}RANK{\textgreater} 2014 - Q2},
author = {Wang, Xiangyang and Yang, Jie and Teng, Xiaolong and Xia, Weijun and Jensen, Richard},
doi = {10.1016/j.patrec.2006.09.003},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2007 - Feature selection based on rough sets and particle swarm optimization.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Feature selection,Genetic algorithms,Hill-climbing method,Particle swarm optimization,Reduct,Rough sets,Stochastic method},
month = {mar},
number = {4},
pages = {459--471},
title = {{Feature selection based on rough sets and particle swarm optimization}},
url = {http://www.sciencedirect.com/science/article/pii/S0167865506002327},
volume = {28},
year = {2007}
}
@book{Witten2011,
abstract = {Practical Machine Learning Tools and Techniques},
author = {Witten, Ian H. and Frank, Eibe and Hall, Mark a.},
booktitle = {Data Mining},
doi = {10.1002/1521-3773(20010316)40:6<9823::AID-ANIE9823>3.3.CO;2-C},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2011 - Data Mining.pdf:pdf},
isbn = {9780123748560},
issn = {14337851},
number = {Tentang Data Mining},
pages = {665},
pmid = {11221713},
title = {{Data Mining}},
volume = {277},
year = {2011}
}
@article{Li2014,
abstract = {The objective of subgroup discovery is to find groups of individuals who are statistically different from others in a large data set. Most existing measures of the quality of subgroups are intuitive and do not precisely capture statistical differences of a group with the other, and their discovered results contain many redundant subgroups. Odds ratio is a statistically sound measure to quantify the statistical difference of two groups for a certain outcome and it is a very suitable measure for quantifying the quality of subgroups. In this paper, we propose a statistically sound framework for statistically non-redundant subgroup discovery: measuring the quality of subgroups by the odds ratio and defining statistically non-redundant subgroups by the error bounds of odds ratios. We show that our proposed method is faster than most existing methods and discovers complete statistically non-redundant subgroups. ?? 2014 Elsevier B.V. All rights reserved.},
author = {Li, Jiuyong and Liu, Jixue and Toivonen, Hannu and Satou, Kenji and Sun, Youqiang and Sun, Bingyu},
doi = {10.1016/j.knosys.2014.04.030},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - Discovering statistically non-redundant subgroups.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Non-redundancy,Odds ratio,Rules,Search space pruning,Subgroups},
pages = {315--327},
publisher = {Elsevier B.V.},
title = {{Discovering statistically non-redundant subgroups}},
url = {http://dx.doi.org/10.1016/j.knosys.2014.04.030},
volume = {67},
year = {2014}
}
@article{Dorigo1996,
author = {Dorigo, M and Maniezzo, V and Colorni, a},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1996 - The ant systems optimization by a colony of cooperative agents.pdf:pdf},
journal = {IEEE Transactions on Man, Machine and Cybernetics-Part B},
number = {1},
pages = {1--13},
title = {{The ant systems: optimization by a colony of cooperative agents}},
volume = {26},
year = {1996}
}
@article{Maranguni??2015,
abstract = {With the ever-increasing development of technology and its integration into users' private and professional life, a decision regarding its acceptance or rejection still remains an open question. A respectable amount of work dealing with the technology acceptance model (TAM), from its first appearance more than a quarter of a century ago, clearly indicates a popularity of the model in the field of technology acceptance. Originated in the psychological theory of reasoned action and theory of planned behavior, TAM has evolved to become a key model in understanding predictors of human behavior toward potential acceptance or rejection of the technology. The main aim of the paper is to provide an up-to-date, well-researched resource of past and current references to TAM-related literature and to identify possible directions for future TAM research. The paper presents a comprehensive concept-centric literature review of the TAM, from 1986 onwards. According to a designed methodology, 85 scientific publications have been selected and classified according to their aim and content into three categories such as (i) TAM literature reviews, (ii) development and extension of TAM, and (iii) modification and application of TAM. Despite a continuous progress in revealing new factors with significant influence on TAM's core variables, there are still many unexplored areas of model potential application that could contribute to its predictive validity. Consequently, four possible future directions for TAM research based on the conducted literature review and analysis are identified and presented.},
author = {Maranguni??, Nikola and Grani??, Andrina},
doi = {10.1007/s10209-014-0348-1},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Technology acceptance model a literature review from 1986 to 2013.pdf:pdf},
isbn = {1020901403},
issn = {16155297},
journal = {Universal Access in the Information Society},
keywords = {Development and extension,Literature review,Modification and application,Technology acceptance model (TAM)},
number = {1},
pages = {81--95},
title = {{Technology acceptance model: a literature review from 1986 to 2013}},
volume = {14},
year = {2015}
}
@article{Akdemir,
abstract = {In this article, we propose several new approaches for post processing a large ensemble of conjunctive rules for supervised, semi-supervised and unsupervised learning problems. We show with various examples that for high dimensional regression problems the models constructed by post processing the rules with partial least squares regression have significantly better prediction performance than the ones produced by the random forest or the rulefit algorithms which use equal weights or weights estimated from lasso regression. When rule ensembles are used for semi-supervised and unsupervised learning, the internal and external measures of cluster validity point to high quality groupings.},
author = {Akdemir, Deniz and Jannink, Jean-Luc},
doi = {10.3233/IDA-140672},
issn = {1088-467X},
journal = {INTELLIGENT DATA ANALYSIS},
keywords = {CLASSIFICATION,Decision trees,SELECTION,VALIDATION,clustering,ensemble learning,rule ensembles,semi-supervised learning},
number = {5},
pages = {857--872},
publisher = {IOS PRESS, NIEUWE HEMWEG 6B, 1013 BG AMSTERDAM, NETHERLANDS},
title = {{Ensemble learning with trees and rules: Supervised, semi-supervised, unsupervised}},
url = {https://apps.webofknowledge.com/full{\_}record.do?product=WOS{\&}search{\_}mode=CitingArticles{\&}qid=3{\&}SID=U2DYtuhHhg5Ogj6Byyu{\&}page=3{\&}doc=28},
volume = {18}
}
@article{Atzmueller,
author = {Atzmueller, Martin and Lemmerich, Florian and Puppe, Frank},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Fast and Effective Subgroup Mining for Continuous Target Variables.pdf:pdf},
title = {{Fast and Effective Subgroup Mining for Continuous Target Variables}}
}
@article{Kavsek2004,
author = {Kavsek, B. and Lavrac, Nada},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2004 - Using subgroup discovery to analyze the UK traffic data.pdf:pdf},
journal = {Metodolo{\v{s}}ki zvezki},
number = {1},
pages = {249--264},
title = {{Using subgroup discovery to analyze the UK traffic data}},
volume = {1},
year = {2004}
}
@article{Demsar2006,
abstract = {Abstract While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams. Keywords: comparative studies, statistical methods, Wilcoxon signed ranks test, Friedman test, multiple comparisons tests},
author = {Dem{\v{s}}ar, Janez},
doi = {10.1016/j.jecp.2010.03.005},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2006 - Statistical Comparisons of Classifiers over Multiple Data Sets.pdf:pdf},
isbn = {9781424450404},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
keywords = {comparative studies,friedman test,multiple comparisons tests,statistical methods,wilcoxon signed ranks test},
pages = {1--30},
pmid = {20451214},
title = {{Statistical Comparisons of Classifiers over Multiple Data Sets}},
url = {http://dl.acm.org/citation.cfm?id=1248547.1248548},
volume = {7},
year = {2006}
}
@article{Clark1989,
author = {Clark, P and Niblett, T},
journal = {Machine Learning},
keywords = {cn2,comprehensibility,concept learning,noise,rule induction},
number = {4},
pages = {261--284},
title = {{The {\{}{\{}{\}}CN{\{}{\}}{\}}2 rule induction algorithm}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.3672{\{}{\&}{\}}rep=rep1{\{}{\&}{\}}type=pdf},
volume = {3},
year = {1989}
}
@article{Carmona2011,
abstract = {This paper describes the application of evolutionary fuzzy systems for subgroup discovery to a medical problem, the study on the type of patients who tend to visit the psychiatric emergency department in a given period of time of the day. In this problem, the objective is to characterise subgroups of patients according to their time of arrival at the emergency department. To solve this problem, several subgroup discovery algorithms have been applied, in order to determine which of them obtains better results. The multi- objective evolutionary algorithm MESDIF for the extraction of fuzzy rules obtains better results and so it has been used to extract interesting information regarding the rate of ad- mission to the psychiatric emergency department.},
author = {Carmona, C. J. and Gonz??lez, P. and del Jesus, M. J. and Nav??o-Acosta, M. and Jim??nez-Trevino, L.},
doi = {10.1007/s00500-010-0670-3},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2011 - Evolutionary fuzzy rule extraction for subgroup discovery in a psychiatric emergency department.pdf:pdf},
isbn = {1432-7643},
issn = {14327643},
journal = {Soft Computing},
keywords = {Evolutionary algorithm,Evolutionary fuzzy system,Fuzzy rules extraction,Psychiatric emergency,Subgroup discovery},
number = {12},
pages = {2435--2448},
title = {{Evolutionary fuzzy rule extraction for subgroup discovery in a psychiatric emergency department}},
volume = {15},
year = {2011}
}
@article{L??pez2015,
abstract = {Classification with big data has become one of the latest trends when talking about learning from the available information. The data growth in the last years has rocketed the interest in effectively acquiring knowledge to analyze and predict trends. The variety and veracity that are related to big data introduce a degree of uncertainty that has to be handled in addition to the volume and velocity requirements. This data usually also presents what is known as the problem of classification with imbalanced datasets, a class distribution where the most important concepts to be learned are presented by a negligible number of examples in relation to the number of examples from the other classes. In order to adequately deal with imbalanced big data we propose the Chi-FRBCS-BigDataCS algorithm, a fuzzy rule based classification system that is able to deal with the uncertainly that is introduced in large volumes of data without disregarding the learning in the underrepresented class. The method uses the MapReduce framework to distribute the computational operations of the fuzzy model while it includes cost-sensitive learning techniques in its design to address the imbalance that is present in the data. The good performance of this approach is supported by the experimental analysis that is carried out over twenty-four imbalanced big data cases of study. The results obtained show that the proposal is able to handle these problems obtaining competitive results both in the classification performance of the model and the time needed for the computation.},
author = {L??pez, Victoria and {Del R??o}, Sara and Ben??tez, Jos?? Manuel and Herrera, Francisco},
doi = {10.1016/j.fss.2014.01.015},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Cost-sensitive linguistic fuzzy rule based classification systems under the MapReduce framework for imbalanced big data.pdf:pdf},
issn = {01650114},
journal = {Fuzzy Sets and Systems},
keywords = {Big data,Cost-sensitive learning,Fuzzy rule based classification systems,Hadoop,Imbalanced datasets,MapReduce},
pages = {5--38},
publisher = {Elsevier B.V.},
title = {{Cost-sensitive linguistic fuzzy rule based classification systems under the MapReduce framework for imbalanced big data}},
url = {http://dx.doi.org/10.1016/j.fss.2014.01.015},
volume = {258},
year = {2015}
}
@article{Level,
author = {Level, T H E Minus},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Dive into 3.pdf:pdf},
keywords = {apter -1,d ive i nto,n,p ython,s n ew i,w hat},
title = {{Dive into 3}}
}
@article{Karg2011,
abstract = {Software quality costs have not received as much attention from the research community as other economic aspects of software development. Over the last three decades, a number of articles on this topic have appeared in a range of journals, but comprehensive overviews of this body of research are not available. For the detailed review of software quality cost research presented in this article, we collect 87 articles published between 1980 and 2009 in 60 leading computing journals. We study the distribution of these articles across research disciplines and journals as well as over time. Moreover, we identify the predominant researchers in the software quality cost domain and the related research clusters. We also classify the articles according to three properties, namely, research topic, research scope, and research approach. This categorization enables us to identify aspects emphasized by previous research on software quality costs and to point out promising future research directions. Our review shows that prevention costs have gained the least attention, in spite of their big cost impact. It also reveals that only one article has targeted multiple companies. Further, we observe that many articles do not empirically validate their findings. This is especially true for those articles dealing with an entire firm. ?? 2010 Elsevier Inc. All rights reserved.},
author = {Karg, Lars M. and Grottke, Michael and Beckhaus, Arne},
doi = {10.1016/j.jss.2010.11.904},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2011 - A systematic literature review of software quality cost research.pdf:pdf},
isbn = {0164-1212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Prevention appraisal failure cost scheme,Quality costs,Software development,Systematic literature review},
number = {3},
pages = {415--427},
title = {{A systematic literature review of software quality cost research}},
volume = {84},
year = {2011}
}
@article{Wang2015,
author = {Wang, Huanjing and Raton, Boca and Raton, Boca},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Stability of Three Forms of Feature Selection Methods on Software Engineering Data.pdf:pdf},
journal = {International Journal of Software Engineering and Knowledge Engineering},
title = {{Stability of Three Forms of Feature Selection Methods on Software Engineering Data}},
year = {2015}
}
@article{Kitchenham2013,
abstract = {Context: Many researchers adopting systematic reviews (SRs) have also published papers discussing problems with the SR methodology and suggestions for improving it. Since guidelines for SRs in software engineering (SE) were last updated in 2007, we believe it is time to investigate whether the guidelines need to be amended in the light of recent research. Objective: To identify, evaluate and synthesize research published by software engineering researchers concerning their experiences of performing SRs and their proposals for improving the SR process. Method: We undertook a systematic review of papers reporting experiences of undertaking SRs and/or discussing techniques that could be used to improve the SR process. Studies were classified with respect to the stage in the SR process they addressed, whether they related to education or problems faced by novices and whether they proposed the use of textual analysis tools. Results: We identified 68 papers reporting 63 unique studies published in SE conferences and journals between 2005 and mid-2012. The most common criticisms of SRs were that they take a long time, that SE digital libraries are not appropriate for broad literature searches and that assessing the quality of empirical studies of different types is difficult. Conclusion: We recommend removing advice to use structured questions to construct search strings and including advice to use a quasi-gold standard based on a limited manual search to assist the construction of search stings and evaluation of the search process. Textual analysis tools are likely to be useful for inclusion/exclusion decisions and search string construction but require more stringent evaluation. SE researchers would benefit from tools to manage the SR process but existing tools need independent validation. Quality assessment of studies using a variety of empirical methods remains a major problem. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
author = {Kitchenham, Barbara and Brereton, Pearl},
doi = {10.1016/j.infsof.2013.07.010},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - A systematic review of systematic review process research in software engineering.pdf:pdf},
isbn = {09505849},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Mapping study,Systematic literature review,Systematic review,Systematic review methodology},
number = {12},
pages = {2049--2075},
publisher = {Elsevier B.V.},
title = {{A systematic review of systematic review process research in software engineering}},
url = {http://dx.doi.org/10.1016/j.infsof.2013.07.010},
volume = {55},
year = {2013}
}
@article{Bacardit2012,
abstract = {MOTIVATION: The prediction of a protein's contact map has become in recent years, a crucial stepping stone for the prediction of the complete 3D structure of a protein. In this article, we describe a methodology for this problem that was shown to be successful in CASP8 and CASP9. The methodology is based on (i) the fusion of the prediction of a variety of structural aspects of protein residues, (ii) an ensemble strategy used to facilitate the training process and (iii) a rule-based machine learning system from which we can extract human-readable explanations of the predictor and derive useful information about the contact map representation.$\backslash$n$\backslash$nRESULTS: The main part of the evaluation is the comparison against the sequence-based contact prediction methods from CASP9, where our method presented the best rank in five out of the six evaluated metrics. We also assess the impact of the size of the ensemble used in our predictor to show the trade-off between performance and training time of our method. Finally, we also study the rule sets generated by our machine learning system. From this analysis, we are able to estimate the contribution of the attributes in our representation and how these interact to derive contact predictions.$\backslash$n$\backslash$nAVAILABILITY: http://icos.cs.nott.ac.uk/servers/psp.html.$\backslash$n$\backslash$nCONTACT: natalio.krasnogor@nottingham.ac.uk$\backslash$n$\backslash$nSUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.},
annote = {ECBDL Large Dataset},
author = {Bacardit, Jaume and Widera, Pawe{\l} and M{\'{a}}rquez-chamorro, Alfonso and Divina, Federico and Aguilar-Ruiz, Jes{\'{u}}s S. and Krasnogor, Natalio},
doi = {10.1093/bioinformatics/bts472},
file = {:home/raul/Datasets/Large/ECBDL14/2012 - Contact map prediction.pdf:pdf},
isbn = {1367-4811},
issn = {13674803},
journal = {Bioinformatics},
number = {19},
pages = {2441--2448},
pmid = {22833524},
title = {{Contact map prediction using a large-scale ensemble of rule sets and the fusion of multiple predicted structural features}},
volume = {28},
year = {2012}
}
@article{Spolaor2016,
abstract = {Each example in a multi-label dataset is associated with multiple labels, which are often correlated. Learning from this data can be improved when dimensionality reduction tasks, such as feature selection, are applied. The standard approach for multi-label feature selection transforms the multi-label dataset into single-label datasets before using traditional feature selection algorithms. However, this approach often ignores label dependence. In this work, we propose an alternative method, LCFS, that constructs new labels based on relations between the original labels. By doing so, the label set from the data is augmented with second-order information before applying the standard approach. To assess LCFS, an experimental evaluation using Information Gain as a measure to estimate the importance of features was carried out on 10 benchmark multi-label datasets. This evaluation compared four LCFS settings with the standard approach, using random feature selection as a reference. For each dataset, the performance of a feature selection method is estimated by the quality of the classifiers built from the data described by the features selected by the method. The results show that a simple LCFS setting gave rise to classifiers similar to, or better than, the ones built using the standard approach. Furthermore, this work also pioneers the use of the systematic review method to survey the related work on multi-label feature selection. The summary of the 99 papers found promotes the idea that exploring label dependence during feature selection can lead to good results.},
author = {Spola{\^{o}}r, Newton and Monard, Maria Carolina and Tsoumakas, Grigorios and Lee, Huei Diana},
doi = {10.1016/j.neucom.2015.07.118},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2016 - A systematic review of multi-label feature selection and a new method based on label construction.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Binary relevance,Feature ranking,Filter feature selection,Information gain,Systematic review},
month = {mar},
pages = {3--15},
title = {{A systematic review of multi-label feature selection and a new method based on label construction}},
url = {http://www.sciencedirect.com/science/article/pii/S0925231215016197},
volume = {180},
year = {2016}
}
@incollection{Pulgar2015,
address = {Albacete, Spain},
author = {Pulgar, F and Jesus, M J},
booktitle = {Actas de la XVI Conferencia de la Asociaci{\'{o}}n Espa{\~{n}}ola para la Inteligencia Artificial},
file = {:home/raul/Downloads/01011.pdf:pdf},
keywords = {big data,genetic algorithm,rbfns,spark},
title = {{GenRBFNSpark : A first implementation in Spark of a genetic algorithm to RBFN design}},
url = {http://simd.albacete.org/actascaepia15/papers/01011.pdf},
year = {2015}
}
@article{Zhang2013,
abstract = {Background: Systematic Literature Reviews (SLRs) have gained significant popularity among Software Engineering (SE) researchers since 2004. Several researchers have also been working on improving the scientific and methodological infrastructure to support SLRs in SE. We argue that there is also an apparent and essential need for evidence-based body of knowledge about different aspects of the adoption of SLRs in SE. Objective: The main objective of this research is to empirically investigate the adoption, value, and use of SLRs in SE research from various perspectives. Method: We used mixed-methods approach (systematically integrating tertiary literature review, semi-structured interviews and questionnaire-based survey) as it is based on a combination of complementary research methods which are expected to compensate each others' limitations. Results: A large majority of the participants are convinced of the value of using a rigourous and systematic methodology for literature reviews in SE research. However, there are concerns about the required time and resources for SLRs. One of the most important motivators for performing SLRs is new findings and inception of innovative ideas for further research. The reported SLRs are more influential compared to the traditional literature reviews in terms of number of citations. One of the main challenges of conducting SLRs is drawing a balance between methodological rigour and required effort. Conclusions: SLR has become a popular research methodology for conducting literature review and evidence aggregation in SE. There is an overall positive perception about this relatively new methodology to SE research. The findings provide interesting insights into different aspects of SLRs. We expect that the findings can provide valuable information to readers about what can be expected from conducting SLRs and the potential impact of such reviews. {\textcopyright} 2013 Elsevier B.V. All rights reserved.},
author = {Zhang, He and {Ali Babar}, Muhammad},
doi = {10.1016/j.infsof.2012.09.008},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - Systematic reviews in software engineering An empirical investigation.pdf:pdf},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Evidence-based software engineering,Methodology adoption,Mixed-methods research,Research methodology,Systematic (literature) reviews,Tertiary study},
number = {7},
pages = {1341--1354},
publisher = {Elsevier B.V.},
title = {{Systematic reviews in software engineering: An empirical investigation}},
url = {http://dx.doi.org/10.1016/j.infsof.2012.09.008},
volume = {55},
year = {2013}
}
@article{Zang2015,
abstract = {A vehicle's license plate is the unique feature by which to identify each individual vehicle. As an important research area of an intelligent transportation system, the recognition of vehicle license plates has been investigated for some decades. An approach based on a visual attention model and deep learning is proposed to handle the problem of Chinese car license plate recognition for traffic videos. We first use a modified visual attention model to locate the license plate, and then the license plate is segmented into seven blocks using a projection method. Two classifiers, which combine the advantages of convolutional neural network-based feature learning and support vector machine for multichannel processing, are designed to recognize Chinese characters, numbers, and alphabet letters, respectively. Experimental results demonstrate that the presented method can achieve high recognition accuracy and works robustly even under the conditions of illumination change and noise contamination.},
author = {Zang, Di and Chai, Zhenliang and Zhang, Junqi and Zhang, Dongdong and Cheng, Jiujun},
doi = {10.1117/1.JEI.24.3.033001},
file = {:home/raul/Downloads/JEI{\_}24{\_}3{\_}033001.pdf:pdf},
issn = {1017-9909},
journal = {Journal of Electronic Imaging},
keywords = {convolutional neural network,deep learning,intelligent transportation system,license plate recognition,visual attention model},
number = {3},
pages = {033001},
title = {{Vehicle license plate recognition using visual attention model and deep learning}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-84929069162{\&}partnerID=tZOtx3y1},
volume = {24},
year = {2015}
}
@article{Demsar2013,
abstract = {Orange is a machine learning and data mining suite for data analysis through Python scripting and visual programming. Here we report on the scripting part, which features interactive data analysis and component-based assembly of data mining procedures. In the selection and design of components, we focus on the flexibility of their reuse: our principal intention is to let the user write simple and clear scripts in Python, which build upon C++ implementations of computationally-intensive tasks. Orange is intended both for experienced users and programmers, as well as for students of data mining.},
author = {Dem{\v{s}}ar, Janez and Curk, Toma{\v{z}} and Erjavec, Ale{\v{s}} and Gorup, {\v{C}}rt and Ho{\v{c}}evar, Toma{\v{z}} and Milutinovi{\v{c}}, Mitar and Mo{\v{z}}ina, Martin and Polajnar, Matija and Toplak, Marko and Stari{\v{c}}, An{\v{z}}e and {\v{S}}tajdohar, Miha and Umek, Lan and {\v{Z}}agar, Lan and {\v{Z}}bontar, Jure and {\v{Z}}itnik, Marinka and Zupan, Bla{\v{z}}},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - Orange data mining toolbox in python.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {The Journal of Machine Learning Research},
keywords = {data mining,machine learning,python,scripting,toolbox},
number = {1},
pages = {2349--2353},
title = {{Orange: data mining toolbox in python}},
url = {http://dl.acm.org/citation.cfm?id=2567709.2567736},
volume = {14},
year = {2013}
}
@article{Bouckaert2013,
author = {Bouckaert, Remco R and Frank, Eibe and Hall, Mark and Kirkby, Richard and Reutemann, Peter and Seewald, Alex and Scuse, David},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - WEKA Manual for Version 3-6-10.pdf:pdf},
isbn = {3-900051-07-0},
title = {{WEKA Manual for Version 3-6-10}},
year = {2013}
}
@article{Yeh2007,
author = {Yeh, Arthur B},
doi = {10.1198/tech.2007.s502},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2007 - A Modern Introduction to Probability and Statistics.pdf:pdf},
isbn = {9781852338961},
issn = {0040-1706},
journal = {Technometrics},
number = {3},
pages = {359--359},
title = {{A Modern Introduction to Probability and Statistics}},
url = {http://amstat.tandfonline.com/doi/pdf/10.1198/tech.2007.s502$\backslash$nhttp://www.tandfonline.com/doi/abs/10.1198/tech.2007.s502},
volume = {49},
year = {2007}
}
@article{Atzmueller2012,
abstract = {This paper presents an overview on the VIKAMINE system for subgroup discovery, pattern mining and analytics. As of VIKAMINE version 2, it is implemented as rich-client platform (RCP) application, based on the Eclipse framework. This provides for a highly-configurable environment, and allows modular extensions using plugins. We present the system, briefly discuss exemplary plugins, and provide a sketch of successful applications.},
author = {Atzmueller, Martin and Lemmerich, Florian},
doi = {10.1007/978-3-642-33486-3_60},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - VIKAMINE - Open-source subgroup discovery, pattern mining, and analytics.pdf:pdf},
isbn = {9783642334856},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Analytics,Open-Source,Pattern Mining,Subgroup Discovery},
number = {PART 2},
pages = {842--845},
title = {{VIKAMINE - Open-source subgroup discovery, pattern mining, and analytics}},
volume = {7524 LNAI},
year = {2012}
}
@article{Fournier-Viger2014,
abstract = {We present SPMF, an open-source data mining library offering implementations of more than 55 data mining algorithms. SPMF is a cross-platform library implemented in Java, specialized for discovering patterns in transaction and sequence databases such as frequent itemsets, association rules and sequential patterns. The source code can be integrated in other Java programs. Moreover, SPMF offers a command line interface and a simple graphical interface for quick testing. The source code is available under the GNU General Public License, version 3. The website of the project offers several resources such as documentation with examples of how to run each algorithm, a developer's guide, performance comparisons of algorithms, data sets, an active forum, a FAQ and a mailing list.},
author = {Fournier-Viger, Philippe},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - SPMF A Java Open-Source Pattern Mining Library.pdf:pdf},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {data mining,database,frequent pattern mining,library,open-source,sequence database,transaction},
pages = {3569--3573},
title = {{SPMF: A Java Open-Source Pattern Mining Library}},
volume = {15},
year = {2014}
}
@article{Reyes2015,
abstract = {Multi-label learning has become an important area of research due to the increasing number of modern applications that contain multi-label data. The multi-label data are structured in a more complex way than single-label data. Consequently the development of techniques that allow the improvement in the performance of machine learning algorithms over multi-label data is desired. The feature weighting and feature selection algorithms are important feature engineering techniques which have a beneficial impact on the machine learning. The ReliefF algorithm is one of the most popular algorithms to feature estimation and it has proved its usefulness in several domains. This paper presents three extensions of the ReliefF algorithm for working in the multi-label learning context, namely ReliefF-ML, PPT-ReliefF and RReliefF-ML. PPT-ReliefF uses a problem transformation method to convert the multi-label problem into a single-label problem. ReliefF-ML and RReliefF-ML adapt the classic ReliefF algorithm in order to handle directly the multi-label data. The proposed ReliefF extensions are evaluated and compared with previous ReliefF extensions on 34 multi-label datasets. The results show that the proposed ReliefF extensions improve preceding extensions and overcome some of their drawbacks. The experimental results are validated using several nonparametric statistical tests and confirm the effectiveness of the proposal for a better multi-label learning.},
author = {Reyes, Oscar and Morell, Carlos and Ventura, Sebasti{\'{a}}n},
doi = {10.1016/j.neucom.2015.02.045},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Scalable extensions of the ReliefF algorithm for weighting and selecting features on the multi-label learning context(2).pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
keywords = {Feature selection,Feature weighting,Label ranking,Multi-label classification,Multi-label learning,ReliefF algorithm},
month = {aug},
pages = {168--182},
title = {{Scalable extensions of the ReliefF algorithm for weighting and selecting features on the multi-label learning context}},
url = {http://www.sciencedirect.com/science/article/pii/S0925231215001940},
volume = {161},
year = {2015}
}
@article{Mikut2011,
abstract = {The development and application of data mining algorithms requires$\backslash$nthe use of powerful software tools. As the number of available tools$\backslash$ncontinues to grow, the choice of the most suitable tool becomes increasingly$\backslash$ndifficult. This paper attempts to support the decision-making process$\backslash$nby discussing the historical development and presenting a range of$\backslash$nexisting state-of-the-art data mining and related tools. Furthermore,$\backslash$nwe propose criteria for the tool categorization based on different$\backslash$nuser groups, data structures, data mining tasks and methods, visualization$\backslash$nand interaction styles, import and export options for data and models,$\backslash$nplatforms, and license policies. These criteria are then used to$\backslash$nclassify data mining tools into nine different types. The typical$\backslash$ncharacteristics of these types are explained and a selection of the$\backslash$nmost important tools is categorized. This paper is organized as follows:$\backslash$nthe first section Historical Development and State-of-the-Art highlights$\backslash$nthe historical development of data mining software until present;$\backslash$nthe criteria to compare data mining software are explained in the$\backslash$nsecond section Criteria for Comparing Data Mining Software. The last$\backslash$nsection Categorization of Data Mining Software into Different Types$\backslash$nproposes a categorization of data mining software and introduces$\backslash$ntypical software tools for the different types. {\^{A}}{\textcopyright} 2011 John Wiley$\backslash$n{\&} Sons, Inc. WIREs Data Mining Knowl Discov 2011 1 431-443 DOI: 10.1002/widm.24},
author = {Mikut, Ralf and Reischl, Markus},
doi = {10.1002/widm.24},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2011 - Data mining tools.pdf:pdf},
isbn = {1942-4795},
issn = {19424787},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
number = {5},
pages = {431--443},
title = {{Data mining tools}},
volume = {1},
year = {2011}
}
@article{Wahbeh2010,
author = {Wahbeh, Abdullah H and Al-radaideh, Qasem a and Al-kabi, Mohammed N and Al-shawakfa, Emad M},
doi = {10.14569/SpecialIssue.2011.010304},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2010 - A Comparison Study between Data Mining Tools over some Classification Methods.pdf:pdf},
issn = {2158107X},
journal = {Ijacsa},
keywords = {-component,data classification,data mining tools,knime,orange,tanagra,wekak},
number = {Special Issue on Artificial Intelligence},
pages = {18--26},
title = {{A Comparison Study between Data Mining Tools over some Classification Methods}},
volume = {2 (8)},
year = {2010}
}
@software{hadoop,
author = {{Apache Software Foundation}},
title = {{Hadoop}},
url = {https://hadoop.apache.org}
}
@article{Guibas1981,
abstract = {This paper studies several topics concerning the way strings can overlap. The key notion of the correlation of two strings is introduced, which is a representation of how the second string can overlap into the first. This notion is then used to state and prove a formula for the generating function that enumerates the q-ary strings of length n which contain none of a given finite set of patterns. Various generalizations of this basic result are also discussed. This formula is next used to study a wide variety of seemingly unrelated problems. The first application is to the nontransitive dominance relations arising out of a probabilistic coin-tossing game. Another application shows that no algorithm can check for the presence of a given pattern in a text without examining essentially all characters of the text in the worst case. Finally, a class of polynomials arising in connection with the main result are shown to be irreducible. ?? 1981.},
author = {Guibas, L. J. and Odlyzko, A. M.},
doi = {10.1016/0097-3165(81)90005-4},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1981 - String overlaps, pattern matching, and nontransitive games.pdf:pdf},
isbn = {0097-3165},
issn = {10960899},
journal = {Journal of Combinatorial Theory, Series A},
number = {2},
pages = {183--208},
title = {{String overlaps, pattern matching, and nontransitive games}},
volume = {30},
year = {1981}
}
@article{Qi2016,
annote = {The paper has clear errors on the algotihm notation, I didn't read it completely, and I couldn't undestand... gives me doubts about its quality,},
author = {Qi, Rong-Zhi and Wang, Zhi-Jian and Li, Shui-Yan},
doi = {10.1007/s11390-016-1635-5},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2016 - A Parallel Genetic Algorithm Based on Spark for Pairwise Test Suite Generation.pdf:pdf},
issn = {1000-9000},
journal = {Journal of Computer Science and Technology},
keywords = {low-quality?,pairwise testing,parallel genetic algorithm,spark,test generation},
mendeley-tags = {low-quality?},
number = {2},
pages = {417--427},
title = {{A Parallel Genetic Algorithm Based on Spark for Pairwise Test Suite Generation}},
url = {http://link.springer.com/10.1007/s11390-016-1635-5},
volume = {31},
year = {2016}
}
@book{Furnkranz2006,
abstract = {The data mining and machine learning communities were surprised when Keogh et al. (2003) pointed out that the k-means cluster centers in subsequence time-series clustering become sinusoidal pseudo-patterns for almost all kinds of input time-series data. Understanding this mechanism is an important open problem in data mining. Our new theoretical approach (based on spectral clustering and translational symmetry) explains why the cluster centers of k-means naturally tend to form sinusoidal patterns.},
author = {F{\"{u}}rnkranz, Johannes and Scheffer, Tobias and Spiliopoulou, Myra and Id{\'{e}}, Tsuyoshi},
booktitle = {Knowledge Discovery in Databases: PKDD 2006},
doi = {10.1007/11871637},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2006 - Knowledge Discovery in Databases PKDD 2006.pdf:pdf},
isbn = {978-3-540-45374-1},
issn = {03029743},
pages = {211--222--222},
title = {{Knowledge Discovery in Databases: PKDD 2006}},
url = {http://www.springerlink.com/content/02736184201m7720/},
volume = {4213},
year = {2006}
}
@article{Shi2015,
author = {Shi, Juwei and Qiu, Yunjie and Minhas, Umar Farooq and Jiao, Limei and Wang, Chen and Reinwald, Berthold and {\"{O}}zcan, Fatma},
doi = {10.14778/2831360.2831365},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Clash of the Titans MapReduce vs. Spark for Large Scale Data Analytics.pdf:pdf},
issn = {2150-8097},
journal = {Proc. VLDB Endow.},
month = {sep},
number = {13},
pages = {2110--2121},
publisher = {VLDB Endowment},
title = {{Clash of the Titans: MapReduce vs. Spark for Large Scale Data Analytics}},
url = {http://dx.doi.org/10.14778/2831360.2831365},
volume = {8},
year = {2015}
}
@article{Ali2013,
author = {Ali, Mostafa Z. and Reynolds, Robert G.},
doi = {10.1007/s00500-013-1169-5},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - Cultural algorithms a Tabu search approach for the optimization of engineering design problems.pdf:pdf},
issn = {1432-7643},
journal = {Soft Computing},
keywords = {constrained engineering,cultural algorithms,knowledge diffusion,knowledge integration,optimization design,tabu search},
number = {8},
pages = {1631--1644},
title = {{Cultural algorithms: a Tabu search approach for the optimization of engineering design problems}},
url = {http://link.springer.com/10.1007/s00500-013-1169-5},
volume = {18},
year = {2013}
}
@article{Press2012,
abstract = {I'm in the process of researching the origin and evolution of data science as a discipline and a profession. Here are the milestones that I have picked up so far, tracking the evolution of the term “data science,” attempts to define it, and some related developments. I would greatly appreciate any pointers to additional key milestones (events, publications, etc.).},
author = {Press, Gil},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - A Very Short History of Data Science.pdf:pdf},
title = {{A Very Short History of Data Science}},
url = {http://whatsthebigdata.com/2012/04/26/a-very-short-history-of-data-science/},
year = {2012}
}
@article{Li2013,
abstract = {AbstractBackground Cloud Computing is increasingly booming in industry with many competing providers and services. Accordingly, evaluation of commercial Cloud services is necessary. However, the existing evaluation studies are relatively chaotic. There exists tremendous confusion and gap between practices and theory about Cloud services evaluation. Aim To facilitate relieving the aforementioned chaos, this work aims to synthesize the existing evaluation implementations to outline the state-of-the-practice and also identify research opportunities in Cloud services evaluation. Method Based on a conceptual evaluation model comprising six steps, the systematic literature review (SLR) method was employed to collect relevant evidence to investigate the Cloud services evaluation step by step. Results This SLR identified 82 relevant evaluation studies. The overall data collected from these studies essentially depicts the current practical landscape of implementing Cloud services evaluation, and in turn can be reused to facilitate future evaluation work. Conclusions Evaluation of commercial Cloud services has become a world-wide research topic. Some of the findings of this SLR identify several research gaps in the area of Cloud services evaluation (e.g.; Elasticity and Security evaluation of commercial Cloud services could be a long-term challenge), while some other findings suggest the trend of applying commercial Cloud services (e.g.; compared with PaaS, IaaS seems more suitable for customers and is particularly important in industry). This SLR study itself also confirms some previous experiences and records new evidence-based software engineering (EBSE) lessons. ?? 2013 Elsevier Inc.},
author = {Li, Zheng and Zhang, He and O'Brien, Liam and Cai, Rainbow and Flint, Shayne},
doi = {10.1016/j.jss.2013.04.021},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - On evaluating commercial Cloud services A systematic review.pdf:pdf},
isbn = {0164-1212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Cloud,Cloud service evaluation,Computing,Systematic literature review},
number = {9},
pages = {2371--2393},
publisher = {Elsevier Inc.},
title = {{On evaluating commercial Cloud services: A systematic review}},
url = {http://dx.doi.org/10.1016/j.jss.2013.04.021},
volume = {86},
year = {2013}
}
@book{Wong2014,
address = {Cham},
annote = {Though the method is efficient in dealing with large-scale feature selection problem, it is only useful to binary feature and class variables},
doi = {10.1007/978-3-319-01766-2},
editor = {Wong, W. Eric and Zhu, Tingshao},
isbn = {978-3-319-01765-5},
publisher = {Springer International Publishing},
series = {Lecture Notes in Electrical Engineering},
title = {{Computer Engineering and Networking}},
url = {http://link.springer.com/10.1007/978-3-319-01766-2},
volume = {277},
year = {2014}
}
@article{Hastie2009,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting-the first comprehensive treatment of this topic in any book. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit. FROM THE REVIEWS: TECHNOMETRICS "This is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
doi = {10.1007/b94608},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - The Elements of Statistical Learning.pdf:pdf},
isbn = {9780387848570},
issn = {03436993},
journal = {Elements},
pages = {337--387},
pmid = {15512507},
title = {{The Elements of Statistical Learning}},
url = {http://www.springerlink.com/index/10.1007/b94608},
volume = {1},
year = {2009}
}
@article{Bolón-Canedo2012,
abstract = {With the advent of high dimensionality, adequate identification of relevant features of the data has become indispensable in real-world scenarios. In this context, the importance of feature selection is beyond doubt and different methods have been developed. However, with such a vast body of algorithms available, choosing the adequate feature selection method is not an easy-to-solve question and it is necessary to check their effectiveness on different situations. Nevertheless, the assessment of relevant features is difficult in real datasets and so an interesting option is to use artificial data. In this paper, several synthetic datasets are employed for this purpose, aiming at reviewing the performance of feature selection methods in the presence of a crescent number or irrelevant features, noise in the data, redundancy and interaction between attributes, as well as a small ratio between number of samples and number of features. Seven filters, two embedded methods, and two wrappers are applied over eleven synthetic datasets, tested by four classifiers, so as to be able to choose a robust method, paving the way for its application to real datasets.},
author = {Bol{\'{o}}n-Canedo, Ver{\'{o}}nica and S{\'{a}}nchez-Maro{\~{n}}o, Noelia and Alonso-Betanzos, Amparo},
doi = {10.1007/s10115-012-0487-8},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - A review of feature selection methods on synthetic data.1007{\_}s1011:1007{\_}s1011},
issn = {0219-3116},
journal = {Knowledge and Information Systems},
number = {3},
pages = {483--519},
title = {{A review of feature selection methods on synthetic data}},
url = {http://dx.doi.org/10.1007/s10115-012-0487-8},
volume = {34},
year = {2012}
}
@article{Gunal2008,
abstract = {Feature selection is an essential topic in the field of pattern recognition. The feature selection strategy has a direct influence on the accuracy and processing time of pattern recognition applications. Features can be evaluated with either univariate approaches, which examine features individually, or multivariate approaches, which consider possible feature correlations and examine features as a group. Although univariate approaches do not take the correlation among features into consideration, they can provide the individual discriminatory power of the features, and they are also much faster than multivariate approaches. Since it is crucial to know which features are more or less informative in certain pattern recognition applications, univariate approaches are more useful in these cases. This paper therefore proposes subspace based separability measures to determine the individual discriminatory power of the features. These measures are then employed to sort and select features in a multi-class manner. The feature selection performances of the proposed measures are evaluated and compared with the univariate forms of classic separability measures (Divergence, Bhattacharyya, Transformed Divergence, and Jeffries–Matusita) on several datasets. The experimental results clearly indicate that the new measures yield comparable or even better performance than the classic ones in terms of classification accuracy and dimension reduction rate.},
author = {Gunal, Serkan and Edizkan, Rifat},
doi = {10.1016/j.ins.2008.06.001},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2008 - Subspace based feature selection for pattern recognition.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Dimension reduction,Feature selection,Pattern recognition,Separability measure,Subspace analysis},
month = {oct},
number = {19},
pages = {3716--3726},
title = {{Subspace based feature selection for pattern recognition}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025508001850},
volume = {178},
year = {2008}
}
@article{Li2015a,
abstract = {Two factors characterize a good feature selection algorithm: its accuracy and stability. This paper aims at introducing a new approach to stable feature selection algorithms. The innovation of this paper centers on a class of stable feature selection algorithms called feature weighting as regularized energy-based learning (FREL). Stability properties of FREL using L1 or L2 regularization are investigated. In addition, as a commonly adopted implementation strategy for enhanced stability, an ensemble FREL is proposed. A stability bound for the ensemble FREL is also presented. Our experiments using open source real microarray data, which are challenging high dimensionality small sample size problems demonstrate that our proposed ensemble FREL is not only stable but also achieves better or comparable accuracy than some other popular stable feature weighting methods.},
author = {Li, Yun and Si, Jennie and Zhou, Guojing and Huang, Shasha and Chen, Songcan},
doi = {10.1109/TNNLS.2014.2341627},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - FREL A Stable Feature Selection Algorithm.pdf:pdf},
issn = {2162-2388},
journal = {IEEE transactions on neural networks and learning systems},
keywords = {Accuracy,Algorithm design and analysis,Energy-based learning,FREL algorithm,FREL stability properties,L1 regularization,L2 regularization,Stability criteria,Training,Training data,Vectors,ensemble,ensemble FREL,feature selection,feature weighting,learning (artificial intelligence),regularized energy-based learning,stability bound,stable feature selection algorithm,uniform weighting stability},
month = {jul},
number = {7},
pages = {1388--402},
pmid = {25134091},
shorttitle = {IEEE Transactions on Neural Networks and Learning},
title = {{FREL: A Stable Feature Selection Algorithm.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/25134091},
volume = {26},
year = {2015}
}
@book{Rothlauf2011,
abstract = {Since we assume that high locality is a general property of real-world problems, modern heuristics must ensure that their design does not destroy the high locality of a problem. If the high locality of a problem is destroyed, straightforward problems turn into difficult problems and cannot be solved better than by random search. Therefore, modern heuristics must ensure that the search operators used fit the metric on the search space and representations have high locality; this means phenotype distances must correspond to genotype distances. The second topic of this chapter is how we can consider knowledge about problem-specific properties of a problem for the design of modern heuristics. For example, we have knowledge about the character and properties of high-quality (or low-quality) solutions. Such problem-specific knowledge can be exploited by introducing a bias into modern heuristics. The bias should consider this knowledge and, for example, concentrate search on solutions that are expected to be of high quality or avoid solutions expected to be of low quality. A bias can be considered in all design elements of modern heuristics namely the representation, the search operator, the fitness function, the initialization, and also the search strategy. However, modern heuristics should only be biased if we have obtained some particular knowledge about an optimization problem or problem instance. If we have no knowledge about properties of a problem, we should not bias modern heuristics as this will mislead the search heuristics.},
author = {Rothlauf, Franz},
booktitle = {Design of Modern Heuristics},
doi = {10.1007/978-3-540-72962-4},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2011 - Design of Modern Heuristics.pdf:pdf},
isbn = {978-3-540-72961-7},
issn = {1619-7127},
pages = {157--171},
title = {{Design of Modern Heuristics}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-72962-4{\_}6$\backslash$nhttp://www.springerlink.com/index/10.1007/978-3-540-72962-4},
year = {2011}
}
@article{Furnkranz2012,
abstract = {Rules – the clearest, most explored and best understood form of knowledge representation – are particularly important for data mining, as they offer the best tradeoff between human and machine understandability. This book presents the fundamentals of rule learning as investigated in classical machine learning and modern data mining. It introduces a feature-based view, as a unifying framework for propositional and relational rule learning, thus bridging the gap between attribute-value learning and inductive logic programming, and providing complete coverage of most important elements of rule learning.The book can be used as a textbook for teaching machine learning, as well as a comprehensive reference to research in the field of inductive rule learning. As such, it targets students, researchers and developers of rule learning algorithms, presenting the fundamental rule learning concepts in sufficient breadth and depth to enable the reader to understand, develop and apply rule learning techniques to real-world data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {F{\"{u}}rnkranz, Johannes and Gamberger, Dragan and Lavra{\v{c}}, Nada},
doi = {10.1007/978-3-540-75197-7},
eprint = {arXiv:1011.1669v3},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Foundations of Rule Learning(4).pdf:pdf},
isbn = {3540751971},
issn = {1467-9280},
number = {2003},
pages = {334},
pmid = {25052830},
title = {{Foundations of Rule Learning}},
url = {https://books.google.co.uk/books/about/Foundations{\_}of{\_}Rule{\_}Learning.html?id=JsTXnjWexCEC{\&}pgis=1},
volume = {6},
year = {2012}
}
@book{Luke2013,
abstract = {Metaheuristics is a rather unfortunate1 term often used to describe a major subfield, indeed the primary subfield, of stochastic optimization. Stochastic optimization is the general class of algorithms and techniques which employ some degree of randomness to find optimal (or as optimal as possible) solutions to hard problems. Metaheuristics are the most general of these kinds of algorithms, and are applied to a very wide range of problems. What kinds of problems? In Jacobellis v. Ohio (1964, regarding obscenity), the United States Supreme Court, Justice Potter Stewart famously wrote, I shall not today attempt further to define the kinds of material I understand to be embraced within that shorthand description; and perhaps I could never succeed in intelligibly doing so. But I know it when I see it, and the motion picture involved in this case is not that. Metaheuristics are applied to I know it when I see it problems. They're algorithms used to find answers to problems when you have very little to help you: you don't know what the optimal solution looks like, you don't know how to go about finding it in a principled way, you have very little heuristic information to go on, and brute-force search is out of the question because the space is too large. But if you're given a candidate solution to your problem, you can test it and assess how good it is. That is, you know a good one when you see it. For example: imagine if you're trying to find an optimal set of robot behaviors for a soccer goalie robot. You have a simulator for the robot and can test any given robot behavior set and assign it a quality (you know a good one when you see it). And you've come up with a definition for what robot behavior sets look like in general. But you have no idea what the optimal behavior set is, nor even how to go about finding it.},
author = {Luke, Sean},
booktitle = {Optimization},
doi = {10.1007/s10710-011-9139-0},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - Essentials of Metaheuristics.pdf:pdf},
isbn = {9781300549628},
issn = {1389-2576},
pages = {1--235},
title = {{Essentials of Metaheuristics}},
url = {http://cs.gmu.edu/{\$}\backslashsim{\$}sean/book/metaheuristics/},
year = {2013}
}
@article{Garnier2007,
abstract = {The roots of swarm intelligence are deeply embedded in the biological study of self-organized behaviors in social insects. From the routing of traffic in telecommunication networks to the design of control algorithms for groups of autonomous robots, the collective behaviors of these animals have inspired many of the foundational works in this emerging research field. For the first issue of this journal dedicated to swarm intelligence, we review the main biological principles that underlie the organization of insects' colonies. We begin with some reminders about the decentralized nature of such systems and we describe the un- derlying mechanisms of complex collective behaviors of social insects, from the concept of stigmergy to the theory of self-organization in biological systems.We emphasize in partic- ular the role of interactions and the importance of bifurcations that appear in the collective output of the colony when some of the system's parameters change. We then propose to categorize the collective behaviors displayed by insect colonies according to four functions that emerge at the level of the colony and that organize its global behavior. Finally, we ad- dress the role of modulations of individual behaviors by disturbances (either environmental or internal to the colony) in the overall flexibility of insect colonies. We conclude that fu- ture studies about self-organized biological behaviors should investigate such modulations to better understand how insect colonies adapt to uncertain worlds.},
author = {Garnier, Simon and Gautrais, Jacques and Theraulaz, Guy},
doi = {10.1007/s11721-007-0004-y},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2007 - The biological principles of swarm intelligence.pdf:pdf},
isbn = {1935-3812},
issn = {1935-3812},
journal = {Swarm Intelligence},
keywords = {self-organization collective,social insects,stigmergy,swarm intelligence},
number = {1},
pages = {3--31},
title = {{The biological principles of swarm intelligence}},
url = {http://link.springer.com/article/10.1007/s11721-007-0004-y},
volume = {1},
year = {2007}
}
@article{Ya2006,
abstract = {It is widely conjectured that the excellent ROC performance of biological vision systems is due in large part to the exploitation of context at each of many levels in a part/whole hierarchy. We propose a mathematical framework (a "composition machine") for constructing probabilistic hierarchical image models, designed to accommodate arbitrary contextual relationships, and we build a demonstration system for reading Massachusetts license plates in an image set collected at Logan Airport. The demonstration system detects and correctly reads more than 98{\%} of the plates, with a negligible rate of false detection. Unlike a formal grammar, the architecture of a composition machine does not exclude the sharing of sub-parts among multiple entities, and does not limit interpretations to single trees (e.g. a scene can have multiple license plates, or no plates at all). In this sense, the architecture is more like a general Bayesian network than a formal grammar. On the other hand, unlike a Bayesian network, the distribution is non-Markovian, and therefore more like a probabilistic context-sensitive grammar. The conceptualization and construction of a composition machine is facilitated by its formulation as the result of a series of non-Markovian perturbations of a "Markov backbone."},
author = {Ya, Jin and Geman, Stuart},
doi = {10.1109/CVPR.2006.86},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2006 - Context and hierarchy in a probabilistic image model.pdf:pdf},
isbn = {0769525970},
issn = {10636919},
journal = {Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition},
pages = {2145--2152},
title = {{Context and hierarchy in a probabilistic image model}},
volume = {2},
year = {2006}
}
@article{Hall1999,
author = {Hall, Mark A.},
doi = {10.1.1.37.4643},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1999 - Correlation-based feature selection for machine learning.pdf:pdf},
journal = {PhD Thesis., Department of Computer Science, Waikato University, New Zealand},
number = {April},
title = {{Correlation-based feature selection for machine learning}},
year = {1999}
}
@article{Ipps2009,
author = {Ipps, Jere H L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - Mikhail A. F.pdf:pdf},
keywords = {- les sites vendiens,arkhangelsk en russie,de la mer blanche,de la r{\{}{\'{e}}{\}}gion de,destruction,ediacaran,fossiles vendiens,la mer blanche -,les pr{\{}{\'{e}}{\}}l{\{}{\`{e}}{\}}vements sauvages,paleo-piracy,paleontological resources,r{\{}{\'{e}}{\}}sum{\{}{\'{e}}{\}},selling fossils,une menace pour les,vendian,{\{}{\'{e}}{\}}diacariens},
pages = {1--36},
title = {{Mikhail A. F}},
volume = {03},
year = {2009}
}
@article{He2015,
author = {He, Hongsheng and Shao, Zhenzhou and Tan, Jindong},
doi = {10.1109/TITS.2015.2437998},
file = {:home/raul/07126977.pdf:pdf},
issn = {15249050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
keywords = {Q1,Vehicle identification,intelligent transportation,traffic surveillance,vehicle tracking},
mendeley-tags = {Q1},
number = {6},
pages = {3182--3192},
title = {{Recognition of Car Makes and Models From a Single Traffic-Camera Image}},
volume = {16},
year = {2015}
}
@article{Garcia2008,
abstract = {In a recently published paper in JMLR, Dem. sar (2006) recommends a set of non-parametric statistical tests and procedures which can be safely used for comparing the performance of classifiers over multiple data sets. After studying the paper, we realize that the paper correctly introduces the basic procedures and some of the most advanced ones when comparing a control method. However, it does not deal with some advanced topics in depth. Regarding these topics, we focus on more powerful proposals of statistical procedures for comparing n x n classifiers. Moreover, we illustrate an easy way of obtaining adjusted and comparable p-values in multiple comparison procedures.},
author = {Garcia, Salvador and Herrera, Francisco},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2008 - An Extension on Statistical Comparisons of Classifiers over Multiple Data Sets for all Pairwise Comparisons.pdf:pdf},
isbn = {1532-4435},
issn = {1532-4435},
journal = {Journal of machine learning research},
keywords = {BONFERRONI PROCEDURE,COMBINATION,ESTIMATORS,HYPOTHESES,TESTS,adjusted p-values,logically related hypotheses,multiple comparisons tests,non-parametric test,statistical methods},
pages = {2677--2694},
title = {{An Extension on "Statistical Comparisons of Classifiers over Multiple Data Sets" for all Pairwise Comparisons}},
url = {http://apps.webofknowledge.com/full{\_}record.do?product=UA{\&}search{\_}mode=GeneralSearch{\&}qid=1{\&}SID=3CbFJ5AB4IgCCklFIBk{\&}page=1{\&}doc=1},
volume = {9},
year = {2008}
}
@article{Pacheco2012,
abstract = {This paper presents a systematic review of relevant published studies related to topics in Requirements Engineering, specifically, concerning stakeholder identification methods in requirements elicitation, dated from 1984 to 2011. Addressing four specific research questions, this systematic literature review shows the following evidence gathered from these studies: current status of stakeholder identification in software requirement elicitation, the best practices recommended for its performance, consequences of incorrect identification in requirements quality, and, aspects which need to be improved. Our findings suggest that the analyzed approaches still have serious limitations in terms of covering all aspects of stakeholder identification as an important part of requirements elicitation. However, through correctly identifying and understanding the stakeholders, it is possible to develop high quality software. ?? 2012 Elsevier Inc. All rights reserved.},
author = {Pacheco, Carla and Garcia, Ivan},
doi = {10.1016/j.jss.2012.04.075},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - A systematic literature review of stakeholder identification methods in requirements elicitation.pdf:pdf},
isbn = {0164-1212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Requirements elicitation,Requirements engineering,Software engineering,Stakeholder identification,Systematic review},
number = {9},
pages = {2171--2181},
publisher = {Elsevier Inc.},
title = {{A systematic literature review of stakeholder identification methods in requirements elicitation}},
url = {http://dx.doi.org/10.1016/j.jss.2012.04.075},
volume = {85},
year = {2012}
}
@article{Puppe2008,
author = {Puppe, Frank and Atzmueller, Martin and Buscher, Georg and H{\"{u}}ttig, Matthias and Luehrs, Hardi and Buscher, Hans-Peter},
doi = {10.3233/978-1-58603-891-5-683},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2008 - Application and Evaluation of a Medical Knowledge System in Sonography ({\{}SONOCONSULT{\}}).pdf:pdf},
isbn = {978-1-58603-891-5},
journal = {ECAI'08/PAIS'08: Proceedings of the 18th European Conference on Artificial Intelligence, including Prestigious Applications of Intelligent Systems},
pages = {683--687},
title = {{Application and Evaluation of a Medical Knowledge System in Sonography ({\{}SONOCONSULT{\}})}},
year = {2008}
}
@article{Kulkarni2012,
abstract = {The License Plate Recognition (LPR) system is one of the most important criterions of mass surveillance method. LPR systems are generally Composed of three steps namely, 1] plate extraction, 2] character Segmentation and 3] Recognition. License Plate Recognition is a crucial task due to the non-uniformity in license plates and the illumination conditions. Most of the developed techniques work under specific conditions such as image capturing angle, illumination, stationary background. All the given LPR techniques vary on the basis of processing time, required computational power and accuracy. Since the absence of any standard, these techniques are incomparable but an efficient path for specified requirements can be judged. Hence we are proposing a Novel idea of Arduino based License Plate Recognition.},
author = {Kulkarni, Parag and Khandebharad, Amit and Khope, Dattatray and Chavan, Pramod U.},
doi = {10.1109/ICoAC.2012.6416843},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - License Plate Recognition A review.pdf:pdf},
isbn = {978-1-4673-5584-1},
journal = {2012 Fourth International Conference on Advanced Computing (ICoAC)},
keywords = {license plate recognition,survey},
pages = {1--8},
title = {{License Plate Recognition: A review}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6416843},
year = {2012}
}
@article{Yang2011,
author = {Yang, Xin-She},
doi = {10.4249/scholarpedia.11472},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2011 - Metaheuristic Optimization.pdf:pdf},
isbn = {978-1-4577-1124-4},
issn = {1941-6016},
journal = {Cambridge University, UK},
number = {2011},
pages = {15},
title = {{Metaheuristic Optimization}},
volume = {6},
year = {2011}
}
@book{Cossio2012,
abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Cossio, Mar{\'{i}}a Laura T and Giesen, Laura F and Araya, Gabriela and P{\'{e}}rez-Cotapos, Mar{\'{i}}a Luisa S and VERGARA, RICARDO L{\'{O}}PEZ and Manca, Maura and Tohme, R A and Holmberg, S D and Bressmann, Tim and Lirio, Daniel Rodrigues and Rom{\'{a}}n, Jelitza Soto and Sol{\'{i}}s, Rodrigo Ganter and Thakur, Sanjay and Rao, S V D Nageswara and Modelado, E L and La, Artificial D E and Durante, Cabeza and Tradici{\'{o}}n, U N A and En, Maya and Espejo, E L and Fuentes, D E L A S and Yucat{\'{a}}n, Universidad Aut{\'{o}}noma De and Lenin, Cruz Moreno and Cian, Laura Franco and Douglas, M Joanne and Plata, La and H{\'{e}}ritier, Fran{\c{c}}oise},
booktitle = {Uma {\{}{\{}{\'{e}}{\}}{\}}tica para quantos?},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {9809069v1},
isbn = {9780874216561},
issn = {0717-6163},
keywords = {Adolescence,Adolescencia,Adolescent,Adolescent Behavior,Adolescent Behavior: psychology,Adult,Agresiones al cuerpo,Attachment to the body,Attaque au corps,Autolesiones deliberadas,Automutilation d{\{}{\{}{\'{e}}{\}}{\}}lib{\{}{\{}{\'{e}}{\}}{\}}r{\{}{\{}{\'{e}}{\}}{\}}e,Body Piercing,Body Piercing: psychology,Body Piercing: statistics {\{}{\&}{\}} numerical data,Body image,CUERPO,Chile,Chile: epidemiology,Cosmetic Techniques,Deliberate self-harm,Epidemiologic Methods,Female,Humans,Image corporelle,Imagen corporal,JUVENTUD,MODIFICACIONES CORPORALES,Male,Motivation,Movement,Risk-Taking,Self Mutilation,Self Mutilation: physiopathology,Self Mutilation: ultrasonography,Sex Distribution,Speech Articulation Tests,Speech Intelligibility,Tattooing,Tattooing: psychology,Tattooing: statistics {\{}{\&}{\}} numerical data,Tongue,Tongue: injuries,Tongue: physiopathology,Tongue: ultrasonography,aesthetics,and on cor-,as none were found,autoinjury and health,body,complications did not,complications from inserting a,constituci{\{}{\{}{\'{o}}{\}}{\}}n del yo,control postural- estabilizaci{\{}{\{}{\'{o}}{\}}{\}}n- v{\{}{\{}{\'{i}}{\}}{\}}as,corporal modifications,corps,cuerpo,culturas juveniles,cultures juv{\{}{\{}{\'{e}}{\}}{\}}niles,epidural,esth{\{}{\{}{\'{e}}{\}}{\}}tique,est{\{}{\{}{\'{e}}{\}}{\}}tica,find any reports of,high resolution images,if neuraxial anes-,ing with neuraxial anesthesia,jeunesse,juvenile cultures,juventud,mecanismos de anteroalimentaci{\{}{\{}{\'{o}}{\}}{\}}n y,modificacio -,needle through a,nes corporales,perforaci{\{}{\{}{\'{o}}{\}}{\}}n corporal,piel,pr{\{}{\{}{\'{a}}{\}}{\}}ctica autolesiva,psicoan{\{}{\{}{\'{a}}{\}}{\}}lisis,research,retroalimentaci{\{}{\{}{\'{o}}{\}}{\}}n,risks management,segunda piel,sensitivas y motoras,spinal,sustainable reconstruction,tattoo,tattooing,tattoos,tatuaje,the literature on tattoos,was reviewed to see,youth},
number = {2},
pages = {81--87},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
title = {{Probability{\{}{\_}{\}}{\{}{\_}{\}}Statistics{\{}{\_}{\}}{\{}{\_}{\}}and{\{}{\_}{\}}Stochastic{\{}{\_}{\}}Processes}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15003161{\$}\backslash{\$}nhttp://cid.oxfordjournals.org/lookup/doi/10.1093/cid/cir991{\$}\backslash{\$}nhttp://www.scielo.cl/pdf/udecada/v15n26/art06.pdf{\$}\backslash{\$}nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84861150233{\{}{\&}{\}}partnerID=tZOtx3y1},
volume = {XXXIII},
year = {2012}
}
@article{Santiago2012,
abstract = {Context: Model-Driven Engineering provides a new landscape for dealing with traceability in software development. Objective: Our goal is to analyze the current state of the art in traceability management in the context of Model-Driven Engineering. Method: We use the systematic literature review based on the guidelines proposed by Kitchenham. We propose five research questions and six quality assessments. Results: Of the 157 relevant studies identified, 29 have been considered primary studies. These studies have resulted in 17 proposals. Conclusion: The evaluation shows that the most addressed operations are storage, CRUD and visualization, while the most immature operations are exchange and analysis traceability information. ?? 2012 Elsevier B.V. All rights reserved.},
author = {Santiago, Iv??n and Jim??nez, ??lvaro and Vara, Juan Manuel and {De Castro}, Valeria and Bollati, Ver??nica A. and Marcos, Esperanza},
doi = {10.1016/j.infsof.2012.07.008},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Model-Driven Engineering as a new landscape for traceability management A systematic literature review.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Model-Driven Engineering,Systematic literature review,Traceability},
number = {12},
pages = {1340--1356},
title = {{Model-Driven Engineering as a new landscape for traceability management: A systematic literature review}},
volume = {54},
year = {2012}
}
@article{Chen2015,
abstract = {Analyses and applications of big data require special technologies to efficiently process large number of data. Mining association rules focus on obtaining relations between data. When mining association rules in big data, conventional methods encounter severe problems incurred by the tremendous cost of computing and inefficiency to achieve the goal. This study proposes an evolutionary algorithm to address these problems, namely Niche-Aided Gene Expression Programming (NGEP). The NGEP algorithm (1) divides individuals to several niches to evolve separately and fuses selected niches according to the similarities of the best individuals to ensure the dispersibility of chromosomes, and (2) adjusts the fitness function to adapt to the needs of the underlying applications. A number of experiments have been performed to compare NGEP with the FP-Growth and Apriori algorithms to evaluate the NGEP's performance in mining association rules with a dataset of measurement for environment pressure (Iris dataset) and an Artificial Simulation Database (ASD). Experimental results indicate that NGEP can efficiently achieve more association rules (36 vs. 33 vs. 25 in Iris dataset experiments and 57 vs. 44 vs. 44 in ASD experiments) with a higher accuracy rate (74.8 vs. 53.2 vs. 50.6{\%} in Iris dataset experiments and 95.8 vs. 77.4 vs. 80.3{\%} in ASD experiments) and the time of computing is also much less than the other two methods.},
author = {Chen, Yunliang and Li, Fangyuan and Fan, Junqing},
doi = {10.1007/s10586-014-0419-3},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Mining association rules in big data with NGEP.pdf:pdf},
issn = {1386-7857},
journal = {Cluster Computing},
keywords = {A-PRIORI ALGORITHM,Association rules,Big data,Gene expression programming,KNOWLEDGE,Niche,SIMULATION},
month = {jan},
number = {2},
pages = {577--585},
publisher = {SPRINGER, 233 SPRING ST, NEW YORK, NY 10013 USA},
title = {{Mining association rules in big data with NGEP}},
url = {https://apps.webofknowledge.com/full{\_}record.do?product=UA{\&}search{\_}mode=GeneralSearch{\&}qid=1{\&}SID=N2j6TVDF3qif1kMBexx{\&}page=2{\&}doc=15},
volume = {18},
year = {2015}
}
@article{EntrepreneurialInsights2015,
author = {{Entrepreneurial Insights}},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Big Data.pdf:pdf},
isbn = {9781617290343},
issn = {0422-2784},
title = {{Big Data}},
url = {http://www.entrepreneurial-insights.com/lexicon/big-data/},
year = {2015}
}
@article{Pieters2010,
author = {Pieters, Barbara F. I. and Knobbe, Arno J. and Dzeroski, Saso},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2010 - Subgroup Discovery in Ranked Data, with an Application to Gene Set Enrichment.pdf:pdf},
journal = {Preference Learning, Workshop at the ECML/PKDD},
title = {{Subgroup Discovery in Ranked Data, with an Application to Gene Set Enrichment}},
url = {http://www.kiminkii.com/publications/pl2010.pdf},
year = {2010}
}
@article{Davis2006,
abstract = {Receiver Operator Characteristic (ROC) curves are commonly used to present results for binary decision problems in machine learning. However, when dealing with highly skewed datasets, Precision-Recall (PR) curves give a more informative picture of an algorithm's performance. We show that a deep connection exists between ROC space and PR space, such that a curve dominates in ROC space if and only if it dominates in PR space. A corollary is the notion of an achievable PR curve, which has properties much like the convex hull in ROC space; we show an efficient algorithm for computing this curve. Finally, we also note differences in the two types of curves are significant for algorithm design. For example, in PR space it is incorrect to linearly interpolate between points. Furthermore, algorithms that optimize the area under the ROC curve are not guaranteed to optimize the area under the PR curve.},
author = {Davis, Jesse and Goadrich, Mark},
doi = {10.1145/1143844.1143874},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2006 - The Relationship Between Precision-Recall and ROC Curves.pdf:pdf},
isbn = {1595933832},
issn = {14710080},
journal = {Proceedings of the 23rd International Conference on Machine learning -- ICML'06},
pages = {233--240},
pmid = {19165215},
title = {{The Relationship Between Precision-Recall and ROC Curves}},
url = {http://portal.acm.org/citation.cfm?doid=1143844.1143874},
year = {2006}
}
@article{Zaki1999,
abstract = {The author surveys the state of the art in parallel and$\backslash$ndistributed association-rule-mining algorithms and uncovers the field's$\backslash$nchallenges and open research problems. This survey can serve as a$\backslash$nreference for both researchers and practitioners},
author = {Zaki, Mohammed J.},
doi = {10.1109/4434.806975},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1999 - Parallel and distributed association mining A survey.pdf:pdf},
isbn = {1092-3063},
issn = {10923063},
journal = {IEEE Concurrency},
number = {4},
pages = {14--25},
title = {{Parallel and distributed association mining: A survey}},
volume = {7},
year = {1999}
}
@incollection{Kubica2011,
author = {Kubica, Jeremy and Singh, Sameer and Sorokina, Daria},
booktitle = {Scaling Up Machine Learning},
doi = {10.1017/CBO9781139042918.018},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2011 - Parallel Large-Scale Feature Selection.pdf:pdf},
isbn = {9781139042918},
number = {February},
pages = {352--370},
title = {{Parallel Large-Scale Feature Selection}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781139042918A143},
year = {2011}
}
@article{Furnkranz1999,
abstract = {This paper is a survey of inductive rule learning algorithms that use a separate-and-conquer strategy. This strategy can be traced back to the AQ learning system and still enjoys popularity as can be seen from its frequent use in inductive logic programming systems. We will put this wide variety of algorithms into a single framework and analyze them along three different dimensions, namely their search, language and overfitting avoidance biases.},
author = {F{\"{u}}rnkranz, Johannes},
doi = {10.1023/A:1006524209794},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1999 - Separate-and-Conquer Rule Learning.pdf:pdf},
isbn = {0269-2821},
issn = {0269-2821},
journal = {Artificial Intelligence Review},
keywords = {covering,inductive logic programming,inductive rule learning},
number = {1},
pages = {3--54},
title = {{Separate-and-Conquer Rule Learning}},
url = {http://dx.doi.org/10.1023/A:1006524209794},
volume = {13},
year = {1999}
}
@article{IgorKononenko,
author = {{Igor Kononenko}, Marko Robnik-Sikonja, Marko Robnik, Uros Pompe},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - ReliefF for estimation and discretization of attributes in classification, regression, and ILP problems.pdf:pdf},
title = {{ReliefF for estimation and discretization of attributes in classification, regression, and ILP problems}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.35.3929}
}
@article{Agrawal1995,
abstract = {We are given a large database of customer transactions, where each transaction consists of customer-id, transaction time, and the items bought in the transaction. We introduce the problem of mining sequential patterns over such databases. We present three algorithms to solve this problem, and empirically evaluate their performance using synthetic data. Two of the proposed algorithms, AprioriSome and AprioriAll, have comparable performance, albeit AprioriSome performs a little better when the minimum number of customers that must support a sequential pattern is low. Scale-up experiments show that both AprioriSome and AprioriAll scale linearly with the number of customer transactions. They also have excellent scale-up properties with respect to the number of transactions per customer and the number of items in a transaction.},
author = {Agrawal, Rakesh and Srikant, Ramakrishnan},
doi = {10.1016/j.jbi.2007.05.004},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1995 - Mining sequential patterns.pdf:pdf},
isbn = {0-8186-6910-1},
issn = {1532-0480},
pages = {3--14},
pmid = {17573243},
title = {{Mining sequential patterns}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=380415$\backslash$nhttp://mail.fukudat.com/w/ja/images/1/12/US5870748.pdf},
year = {1995}
}
@article{Han2000,
abstract = {Mining frequent patterns in transaction databases? time? series databases? and man y other kinds of databases has been studied popularly in data mining researc h? Most of the previous studies adopt an Ap rio ri ?lik e candidate set generation?and?test approac h? Ho ev w er? candidate set generation is still costly ? especially when there exist proli?c patterns and?or long patterns? In this study ?w e propose a no el frequent pattern v tree ?FP?tree ? structure? whic h is an extended pre?x? tree structure for storing compressed? crucial information about frequen t patterns? and dev elop an e?cien t FP?tree? based mining method? FP?gro wth? for mining the c omplete set of fr quent p e atterns b y pattern fragmen t gro wth? E?ciency of mining is ac hiev ed with three tec hniques? ??? a large database is compressed in to a highly condensed? m h smaller data structure? whic uc ha oids costly v ? repeated database scans? ??? our FP?tree ?based mining adopts a pattern fragmen t gro wth method to a oid the costly v generation of a large n um ber of candidate sets? and ??? a partitioning?based? divide?and?conquer method is used to decompose the mining task in to a set of smaller tasks for mining con?ned patterns in conditional databases? whic h dramatically reduces the searc h space? Our performance study sho ws that the FP?gro wth method is e?cien t and scalable for mining both long and short frequen t patterns? and is about an order of magnitude faster than the Ap rio ri algorithm and also faster than some recen tly reported new frequen t pattern mining methods?},
author = {Han, Jiawei and Pei, Jian and Yin, Yiwen},
doi = {10.1145/335191.335372},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2000 - Mining frequent patterns without candidate generation.pdf:pdf},
isbn = {1581132182},
issn = {01635808},
journal = {ACM SIGMOD Record},
keywords = {algorithm,and it was supported,association mining,at simon fraser university,canada,data structure,frequent pattern mining,in part by the,natural sciences,performance improvements,the work was done},
number = {2},
pages = {1--12},
pmid = {24075448},
title = {{Mining frequent patterns without candidate generation}},
volume = {29},
year = {2000}
}
@article{Mahdavi-Hezavehi2013a,
abstract = {Context: Variability is the ability of a software artifact (e.g., a system, component) to be adapted for a specific context, in a preplanned manner. Variability not only affects functionality, but also quality attributes (e.g., security, performance). Service-based software systems consider variability in functionality implicitly by dynamic service composition. However, variability in quality attributes of service-based systems seems insufficiently addressed in current design practices. Objective: We aim at (a) assessing methods for handling variability in quality attributes of service-based systems, (b) collecting evidence about current research that suggests implications for practice, and (c) identifying open problems and areas for improvement. Method: A systematic literature review with an automated search was conducted. The review included studies published between the year 2000 and 2011. We identified 46 relevant studies. Results: Current methods focus on a few quality attributes, in particular performance and availability. Also, most methods use formal techniques. Furthermore, current studies do not provide enough evidence for practitioners to adopt proposed approaches. So far, variability in quality attributes has mainly been studied in laboratory settings rather than in industrial environments. Conclusions: The product line domain as the domain that traditionally deals with variability has only little impact on handling variability in quality attributes. The lack of tool support, the lack of practical research and evidence for the applicability of approaches to handle variability are obstacles for practitioners to adopt methods. Therefore, we suggest studies in industry (e.g., surveys) to collect data on how practitioners handle variability of quality attributes in service-based systems. For example, results of our study help formulate hypotheses and questions for such surveys. Based on needs in practice, new approaches can be proposed. ?? 2012 Elsevier B.V. All rights reserved.},
author = {Mahdavi-Hezavehi, Sara and Galster, Matthias and Avgeriou, Paris},
doi = {10.1016/j.infsof.2012.08.010},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded//2013 - Variability in quality attributes of service-based software systems A systematic literature review.pdf:pdf},
isbn = {09505849},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Quality attributes,Service-based systems,Systematic literature review,Variability},
number = {2},
pages = {320--343},
publisher = {Elsevier B.V.},
title = {{Variability in quality attributes of service-based software systems: A systematic literature review}},
url = {http://dx.doi.org/10.1016/j.infsof.2012.08.010},
volume = {55},
year = {2013}
}
@article{Bhargava2013,
abstract = {Nowadays software are flooded in the market, there is a need for selection criteria of software package that should be available for individuals and organizations. As the number of software continues to grow and new features added to the new software the selection of the most suitable software package is becoming more and more difficult. Wrong or immature decision would result in great loss of time and money. Research is being carried out throughout the world to evaluate the software. Researchers have not reached to consensus to generalize such selection and evaluation criteria. Improper selection of software package can lead to be a costly affair and adversely affect the business.},
author = {Bhargava, Neeraj and Aziz, Atif and Arya, Rajiv and Prof, Associate and Technology, Information},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - Selection Criteria for Data Mining Software A Study.pdf:pdf},
journal = {International Journal of Computer Science Issues},
keywords = {data mining,intelligence,software,software evaluation},
number = {3},
pages = {308--312},
title = {{Selection Criteria for Data Mining Software : A Study}},
volume = {10},
year = {2013}
}
@article{Deb2009,
abstract = {Detecting the region of a license plate is the key component of the vehicle license plate recognition (VLPR) system. A new method is adopted in this paper to analyze road images which often contain vehicles and extract LP from natural properties by finding vertical and horizontal edges from vehicle region. The proposed vehicle license plate detection (VLPD) method consists of three main stages: (1) a novel adaptive image segmentation technique named as sliding concentric windows (SCWs) used for detecting candidate region; (2) color verification for candidate region by using HSI color model on the basis of using hue and intensity in HSI color model verifying green and yellow LP and white LP, respectively; and (3) finally, decom- posing candidate region which contains predetermined LP alphanumeric character by using position histogram to verify and detect vehicle license plate (VLP) region. In the proposed method, input vehicle images are commuted into grey images. Then the candidate regions are found by sliding concentric windows. We detect VLP region which contains predetermined LP color by using HSI color model and LP alphanumeric character by using position histogram. Experimental results show that the proposed method is very effective in coping with different conditions such as poor illumination, varied distances from the vehicle and varied weather.},
author = {Deb, Kaushik and Chae, Hyun Uk and Jo, Kang Hyun},
doi = {10.4304/jcp.4.8.771-777},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - Vehicle license plate detection method based on sliding concentric windows and histogram.pdf:pdf},
issn = {1796203X},
journal = {Journal of Computers},
keywords = {HSI color model and histogram,Vehicle license plate detection (VLPD)},
number = {8},
pages = {771--777},
title = {{Vehicle license plate detection method based on sliding concentric windows and histogram}},
volume = {4},
year = {2009}
}
@article{Agrawal1993,
abstract = {Abstract},
author = {Agrawal, R and Imielinski, T and Swami, A},
doi = {10.1145/170036.170072},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1993 - Mining association rules between sets of items in large databases.pdf:pdf},
isbn = {0897915925},
issn = {01635808},
journal = {ACM SIGMOD Record},
number = {May},
pages = {207--216},
pmid = {21292030},
title = {{Mining association rules between sets of items in large databases}},
volume = {22},
year = {1993}
}
@misc{Frank1998,
abstract = {The two dominant schemes for rule-learning, C4.5 and RIPPER, both operate in two stages. First they induce an initial rule set and then they refine it using a rather complex optimization stage that discards (C4.5) or adjusts (RIPPER) individual rules to make them work better together. In contrast, this paper shows how good rule sets can be learned one rule at a time, without any need for global optimization. We present an algorithm for inferring rules by repeatedly generating partial decision trees, thus combining the two major paradigms for rule generation--creating rules from decision trees and the separate-and-conquer rule-learning technique. The algorithm is straightforward and elegant: despite this, experiments on standard datasets show that it produces rule sets that are as accurate as and of similar size to those generated by C4.5, and more accurate than RIPPER{\&}039;s. Moreover, it operates efficiently, and because it avoids postprocessing, does not suffer the extremely slow performance on pathological example sets for which the C4.5 method has been criticized. 1},
author = {Frank, Eibe and Witten, Ian H},
booktitle = {Work},
doi = {1-55860-556-8},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1998 - Generating accurate rule sets without global optimization.pdf:pdf},
isbn = {1558605568},
issn = {1170-487X},
keywords = {computer science,global optimization,partial decision trees,rules},
pages = {144--151},
title = {{Generating accurate rule sets without global optimization}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.143.8073{\&}rep=rep1{\&}type=pdf},
year = {1998}
}
@inproceedings{Yu2003a,
address = {New York, New York, USA},
author = {Yu, Lei and Liu, Huan},
booktitle = {Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining - KDD '03},
doi = {10.1145/956750.956840},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2003 - Efficiently handling feature redundancy in high-dimensional data.pdf:pdf},
isbn = {1581137370},
keywords = {feature selection,high-dimensional data,redundancy},
month = {aug},
pages = {685},
publisher = {ACM Press},
title = {{Efficiently handling feature redundancy in high-dimensional data}},
url = {http://dl.acm.org/citation.cfm?id=956750.956840},
year = {2003}
}
@article{Yao1997,
abstract = {Evolution strategies are evolutionary algorithms that date back to the 1960s and that are most commonly applied to black-box optimization problems in continuous search spaces. Inspired by biological evolution, their original formulation is based on the application of mutation, recombination and selection in populations of candidate solutions. From the algorithmic viewpoint, evolution strategies are optimization methods that sample new candidate solutions stochastically, most commonly from a multivariate normal probability distribution. Their two most prominent design principles are unbiasedness and adaptive control of parameters of the sample distribution. In this overview the important concepts of success based step-size control, self-adaptation and derandomization are covered, as well as more recent developments like covariance matrix adaptation and natural evolution strategies. The latter give new insights into the fundamental mathematical rationale behind evolution strategies. A broad discussion of theoretical results includes progress rate results on various function classes and convergence proofs for evolution strategies.},
author = {Yao, Xin and Liu, Yong},
doi = {10.1007/BFb0032342},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1997 - Fast evolution strategies.pdf:pdf},
isbn = {3-662-43504-7},
issn = {03248569},
journal = {Control and Cybernetics},
keywords = {Cauchy mutation,Evolutionary strategies,Function optimisation},
number = {3},
pages = {466--491},
title = {{Fast evolution strategies}},
volume = {26},
year = {1997}
}
@article{Rossum2010,
abstract = {Python is an extensible, interpreted, object-oriented programming language. It supports a wide range of applications, from simple text processing scripts to interactive Web browsers. While the Python Reference Manual describes the exact syntax and semantics of the language, it does not describe the standard library that is distributed with the language, and which greatly enhances its immediate usability. This library contains built-in modules (written in C) that provide access to system functionality such as file I/O that would otherwise be inaccessible to Python programmers, as well as modules written in Python that provide standardized solutions for many problems that occur in everyday programming. Some of these modules are explicitly designed to encourage and enhance the portability of Python programs. This library reference manual documents Python's standard library, as well as many optional library modules (which may or may not be available, depending on whether the underlying platform supports them and on the configuration choices made at compile time). It also documents the standard types of the language and its built-in functions and exceptions, many of which are not or incompletely documented in the Reference Manual. This manual assumes basic knowledge about the Python language. For an informal introduction to Python, see the Python Tutorial; the Python Reference Manual remains the highest authority on syntactic and semantic questions. Finally, the manual entitled Extending and Embedding the Python Interpreter describes how to add new extensions to Python and how to embed it in other applications.},
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2010 - The Python Library Reference.pdf:pdf},
issn = {0169-118X},
journal = {October},
pages = {1--1144},
title = {{The Python Library Reference}},
url = {http://scholar.google.com/scholar?q=intitle:Python+Library+Reference{\#}0},
year = {2010}
}
@article{Inza2001,
abstract = {In this paper we perform a comparison among FSS–EBNA, a randomized, population-based and evolutionary algorithm, and two genetic and other two sequential search approaches in the well-known feature subset selection (FSS) problem. In FSS–EBNA, the FSS problem, stated as a search problem, uses the estimation of Bayesian network algorithm (EBNA) search engine, an algorithm within the estimation of distribution algorithm (EDA) approach. The EDA paradigm is born from the roots of the genetic algorithm (GA) community in order to explicitly discover the relationships among the features of the problem and not disrupt them by genetic recombination operators. The EDA paradigm avoids the use of recombination operators and it guarantees the evolution of the population of solutions and the discovery of these relationships by the factorization of the probability distribution of best individuals in each generation of the search. In EBNA, this factorization is carried out by a Bayesian network induced by a cheap local search mechanism. FSS–EBNA can be seen as a hybrid Soft Computing system, a synergistic combination of probabilistic and evolutionary computing to solve the FSS task. Promising results on a set of real Data Mining domains are achieved by FSS–EBNA in the comparison respect to well-known genetic and sequential search algorithms.},
author = {Inza, I{\~{n}}aki and Larra{\~{n}}aga, Pedro and Sierra, Basilio},
doi = {10.1016/S0888-613X(01)00038-X},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2001 - Feature subset selection by Bayesian networks a comparison with genetic and sequential algorithms.pdf:pdf},
issn = {0888613X},
journal = {International Journal of Approximate Reasoning},
keywords = {Bayesian network,Estimation of Bayesian network algorithm,Estimation of distribution algorithm,Feature subset selection,Predictive accuracy,Soft computing},
month = {aug},
number = {2},
pages = {143--164},
title = {{Feature subset selection by Bayesian networks: a comparison with genetic and sequential algorithms}},
url = {http://www.sciencedirect.com/science/article/pii/S0888613X0100038X},
volume = {27},
year = {2001}
}
@article{Zaki1997,
author = {Zaki, Mohammed Javeed and Parthasarathy, Srinivasan and Ogihara, Mitsunori and Wei, Li},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1997 - New Algorithms for Fast Discovery of Association Rules.pdf:pdf},
journal = {In 3rd Intl. Conf. on Knowledge Discovery and Data Mining},
pages = {283--286},
title = {{New Algorithms for Fast Discovery of Association Rules}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.5143},
year = {1997}
}
@article{Gao2012,
author = {Gao, Kehan and Khoshgoftaar, Taghi M and Napolitano, Amri},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Stability of Filter-Based Feature Selection Methods for Imbalanced Software Measurement Data.pdf:pdf},
isbn = {1-891706-31-4},
journal = {International Journal of Software Engineering and Knowledge Engineering},
pages = {74--79},
title = {{Stability of Filter-Based Feature Selection Methods for Imbalanced Software Measurement Data}},
year = {2012}
}
@article{Dogan2014,
abstract = {Context The web has had a significant impact on all aspects of our society. As our society relies more and more on the web, the dependability of web applications has become increasingly important. To make these applications more dependable, for the past decade researchers have proposed various techniques for testing web-based software applications. Our literature search for related studies retrieved 193 papers in the area of web application testing, which have appeared between 2000 and 2013. Objective As this research area matures and the number of related papers increases, it is important to systematically identify, analyze, and classify the publications and provide an overview of the trends and empirical evidence in this specialized field. Methods We systematically review the body of knowledge related to functional testing of web application through a systematic literature review (SLR) study. This SLR is a follow-up and complimentary study to a recent systematic mapping (SM) study that we conducted in this area. As part of this study, we pose three sets of research questions, define selection and exclusion criteria, and synthesize the empirical evidence in this area. Results Our pool of studies includes a set of 95 papers (from the 193 retrieved papers) published in the area of web application testing between 2000 and 2013. The data extracted during our SLR study is available through a publicly-accessible online repository. Among our results are the followings: (1) the list of test tools in this area and their capabilities, (2) the types of test models and fault models proposed in this domain, (3) the way the empirical studies in this area have been designed and reported, and (4) the state of empirical evidence and industrial relevance. Conclusion We discuss the emerging trends in web application testing, and discuss the implications for researchers and practitioners in this area. The results of our SLR can help researchers to obtain an overview of existing web application testing approaches, fault models, tools, metrics and empirical evidence, and subsequently identify areas in the field that require more attention from the research community. {\textcopyright} 2014 Elsevier Inc.},
author = {Doǧan, Serdar and Betin-Can, Aysu and Garousi, Vahid},
doi = {10.1016/j.jss.2014.01.010},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - Web application testing A systematic literature review.pdf:pdf},
isbn = {0164-1212},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Systematic literature review,Testing,Web application},
number = {1},
pages = {174--201},
title = {{Web application testing: A systematic literature review}},
volume = {91},
year = {2014}
}
@article{Herrera2011,
abstract = {Subgroup discovery is a data mining technique which extracts interesting rules with respect to a target variable. An important characteristic of this task is the combination of predictive and descriptive induction. An overview related to the task of subgroup discovery is presented. This review focuses on the foundations, algorithms, and advanced studies together with the applications of subgroup discovery presented throughout the specialised bibliography.},
author = {Herrera, Franciso and Carmona, Crist{\'{o}}bal Jos{\'{e}} and Gonz{\'{a}}lez, Pedro and del Jesus, Mar{\'{i}}a Jos{\'{e}}},
doi = {10.1007/s10115-010-0356-2},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2011 - An overview on subgroup discovery Foundations and applications.pdf:pdf},
issn = {02191377},
journal = {Knowledge and Information Systems},
keywords = {Knowledge discovery,Subgroup discovery},
number = {3},
pages = {495--525},
title = {{An overview on subgroup discovery: Foundations and applications}},
volume = {29},
year = {2011}
}
@phdthesis{Ruiz2006,
author = {Ruiz, Roberto},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2006 - Heur{\'{i}}sticas de selecci{\'{o}}n de atributos para datos de gran dimensionalidad.pdf:pdf},
title = {{Heur{\'{i}}sticas de selecci{\'{o}}n de atributos para datos de gran dimensionalidad}},
year = {2006}
}
@incollection{Eiras-Franco2015,
address = {Albacete, Spain},
author = {Eiras-Franco, Carlos and Bol{\'{o}}n-Canedo, Ver{\'{o}}nica and Ramos, Sabela and Gonz{\'{a}}lez-Dom{\'{i}}guez, Jorge and Alonso-Betanzos, Amparo and Touri{\~{n}}o, Juan},
booktitle = {Actas de la XVI Conferencia de la Asociaci{\{}{\'{o}}{\}}n Espa{\{}{\~{n}}{\}}ola para la Inteligencia Artificial},
file = {:home/raul/Downloads/00949.pdf:pdf},
title = {{Paralelizaci{\{}{\'{o}}{\}}n de algoritmos de selecci{\{}{\'{o}}{\}}n de caracter{\{}{\'{i}}{\}}sticas en la plataforma Weka}},
year = {2015}
}
@article{Zaharia2010,
abstract = {MapReduce and its variants have been highly successful in implementing large-scale data-intensive applications on commodity clusters. However; Spark introduces an abstraction called resilient distributed datasets (RDDs). An RDD is a read-only collection of objects partitioned across a set of machines that can be rebuilt if a partition is lost. Spark can outperform Hadoop by 10x in iterative machine learning jobs; and can be used to interactively query a 39 GB dataset with sub-second response time.; as well as interactive data analysis tools. We propose a new framework called Spark that supports these applications while retaining the scalability and fault tolerance of MapReduce. To achieve these goals; most of these systems are built around an acyclic data flow model that is not suitable for other popular applications. This paper focuses on one such class of applications: those that reuse a working set of data across multiple parallel operations. This includes many iterative machine learning algorithms},
author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
doi = {10.1007/s00256-009-0861-0},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2010 - Spark Cluster Computing with Working Sets.pdf:pdf},
isbn = {1432-2161 (Electronic)$\backslash$n0364-2348 (Linking)},
issn = {03642348},
journal = {HotCloud'10 Proceedings of the 2nd USENIX conference on Hot topics in cloud computing},
pages = {10},
pmid = {20205351},
title = {{Spark : Cluster Computing with Working Sets}},
year = {2010}
}
@article{Ruiz2012,
abstract = {We address the feature subset selection problem for classification tasks. We examine the performance of two hybrid strategies that directly search on a ranked list of features and compare them with two widely used algorithms, the fast correlation based filter (FCBF) and sequential forward selection (SFS). The proposed hybrid approaches provide the possibility of efficiently applying any subset evaluator, with a wrapper model included, to large and high-dimensional domains. The experiments performed show that our two strategies are competitive and can select a small subset of features without degrading the classification error or the advantages of the strategies under study. ?? 2012 Elsevier Ltd. All rights reserved.},
author = {Ruiz, R. and Riquelme, J. C. and Aguilar-Ruiz, J. S. and Garc{\'{i}}a-Torres, M.},
doi = {10.1016/j.eswa.2012.03.061},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Fast feature selection aimed at high-dimensional data via hybrid-sequential-ranked searches.pdf:pdf},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Classification,Data mining,Feature ranking,Feature selection},
month = {sep},
number = {12},
pages = {11094--11102},
title = {{Fast feature selection aimed at high-dimensional data via hybrid-sequential-ranked searches}},
url = {http://www.sciencedirect.com/science/article/pii/S0957417412005842},
volume = {39},
year = {2012}
}
@article{Monroy2014,
abstract = {Introducci{\'{o}}n: La investigaci{\'{o}}n educativa desde la perspectiva de la corriente SAL (Students´ Approaches to Learning) ha explorado los mecanismos implicados en el aprendizaje, as{\'{i}} como los factores que influyen en los enfoques de aprendizaje. Partiendo del modelo 3P de Biggs (1987) y de la revisi{\'{o}}n de factores de Baeten, Kyndt, Struyven, y Dochy (2010), el presente estudio realiz{\'{o}} una revisi{\'{o}}n sistem{\'{a}}tica para seleccionar y recuperar evidencias emp{\'{i}}ricas de factores personales y contextuales que pueden influir en los enfoques de aprendizaje. M{\'{e}}todo: Se sigui{\'{o}} el procedimiento de revisi{\'{o}}n sistem{\'{a}}tica y se extrajeron estudios en funci{\'{o}}n de unos criterios de selecci{\'{o}}n determinados. Resultados: Muchos de los estudios seleccionados coinciden en sus resultados y evidencian la influencia de diversos factores (motivaci{\'{o}}n, conocimientos previos, concepciones de aprendizaje, creencias epistemol{\'{o}}gicas, cantidad de trabajo, percepci{\'{o}}n de los criterios de evaluaci{\'{o}}n, concepciones y m{\'{e}}todos de ense{\~{n}}anza de los profesores, titulaci{\'{o}}n y tipo de tarea y contenido) sobre los enfoques de aprendizaje. Sin embargo, no existen resultados concluyentes sobre otras variables (sexo, edad, cociente intelectual, tipo de evaluaci{\'{o}}n). Conclusiones: Se observan desequilibrios en la atenci{\'{o}}n prestada al estudio de determinadas variables, as{\'{i}} que la presente revisi{\'{o}}n puede servir de gu{\'{i}}a en cuanto a qu{\'{e}} factores requieren mayor nivel de an{\'{a}}lisis en futuras investigaciones. El conocimiento fundado sobre los factores involucrados en el aprendizaje universitario permitir{\'{i}}a desarrollar acciones que mejoren cualitativamente el aprendizaje y promuevan el aprendizaje significativo.},
author = {Monroy, Fuensanta and Pina, Fuensanta Hern{\'{a}}ndez},
doi = {10.5944/educxx1.17.2.11481},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - Factores que influyen en los enfoques de aprendizaje universitario. Una revisi{\'{o}}n sistem{\'{a}}tica.pdf:pdf},
isbn = {1139-613X},
issn = {2174-5374},
journal = {Educaci{\'{o}}n XX1},
keywords = {aprendizaje,enfoques de aprendizaje,ense{\~{n}}anza superior,proceso de aprendizaje,revisi{\'{o}}n,universidad},
number = {2},
pages = {105--124},
title = {{Factores que influyen en los enfoques de aprendizaje universitario. Una revisi{\'{o}}n sistem{\'{a}}tica}},
url = {http://e-spacio.uned.es/revistasuned/index.php/educacionXX1/article/view/11481},
volume = {17},
year = {2014}
}
@article{Brown2012,
author = {Brown, Gavin and Pocock, Adam and Zhao, Ming-Jie and Luj{\'{a}}n, Mikel},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Conditional Likelihood Maximisation A Unifying Framework for Information Theoretic Feature Selection(2).pdf:pdf},
issn = {ISSN 1533-7928},
journal = {Journal of Machine Learning Research},
number = {Jan},
pages = {27--66},
title = {{Conditional Likelihood Maximisation: A Unifying Framework for Information Theoretic Feature Selection}},
url = {http://www.jmlr.org/papers/v13/brown12a.html},
volume = {13},
year = {2012}
}
@article{Rossum2013,
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - The PythonC API.pdf:pdf},
title = {{The Python/C API}},
year = {2013}
}
@article{Lemmerich2009,
author = {Lemmerich, Florian and Atzmueller, Martin},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - Incorporating Exceptions Efficient Mining of e-Relevant Subgroup Patterns.pdf:pdf},
journal = {From Local Patterns to Global Models, Workshop at the ECML/PKDD},
title = {{Incorporating Exceptions: Efficient Mining of e-Relevant Subgroup Patterns}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.182.7068{\&}rep=rep1{\&}type=pdf{\#}page=98},
year = {2009}
}
@book{Furnkranz2012a,
abstract = {Rules – the clearest, most explored and best understood form of knowledge representation – are particularly important for data mining, as they offer the best tradeoff between human and machine understandability. This book presents the fundamentals of rule learning as investigated in classical machine learning and modern data mining. It introduces a feature-based view, as a unifying framework for propositional and relational rule learning, thus bridging the gap between attribute-value learning and inductive logic programming, and providing complete coverage of most important elements of rule learning.The book can be used as a textbook for teaching machine learning, as well as a comprehensive reference to research in the field of inductive rule learning. As such, it targets students, researchers and developers of rule learning algorithms, presenting the fundamental rule learning concepts in sufficient breadth and depth to enable the reader to understand, develop and apply rule learning techniques to real-world data.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {F{\"{u}}rnkranz, Johannes and Gamberger, Dragan and Lavra{\v{c}}, Nada},
doi = {10.1007/978-3-540-75197-7},
eprint = {arXiv:1011.1669v3},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Foundations of Rule Learning.pdf:pdf},
isbn = {3540751971},
issn = {1467-9280},
pages = {334},
pmid = {25052830},
title = {{Foundations of Rule Learning}},
url = {https://books.google.co.uk/books/about/Foundations{\_}of{\_}Rule{\_}Learning.html?id=JsTXnjWexCEC{\&}pgis=1},
volume = {6},
year = {2012}
}
@incollection{Pupo2013,
author = {{Reyes Pupo}, Oscar Gabriel and Morell, Carlos and Soto, Sebasti{\'{a}}n Ventura},
doi = {10.1007/978-3-642-41827-3_66},
pages = {528--535},
publisher = {Springer Berlin Heidelberg},
title = {{ReliefF-ML: An Extension of ReliefF Algorithm to Multi-label Learning}},
url = {http://link.springer.com/10.1007/978-3-642-41827-3{\_}66},
year = {2013}
}
@article{Version2005,
author = {Version, S a S},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2005 - Ser uide.pdf:pdf},
isbn = {1877787191},
number = {November},
pages = {1--17},
title = {{Ser uide}},
year = {2005}
}
@article{Rissino2009,
author = {Rissino, Silvia and Lambert-torres, Germano},
isbn = {9783902613530},
journal = {Data Mining and Knowledge Discovery in Real Life Applications},
number = {February},
pages = {35--58},
title = {{Rough Set Theory – Fundamental Concepts , Principals , Data Extraction , and Applications}},
year = {2009}
}
@article{Rossum2014d,
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - Argument Clinic How-To.pdf:pdf},
title = {{Argument Clinic How-To}},
year = {2014}
}
@article{Taniar2008,
abstract = {Previously, exception rules have been defined as association rules with low support and high confidence. Exception rules are important in data mining, as they form rules that can be categorized as an exception. This is the opposite of general association rules in data mining, which focus on high support and high confidence. In this paper, a new approach to mining exception rules is proposed and evaluated. A relationship between exception and positive/negative association rules is considered, whereby the candidate exception rules are generated based on knowledge of the positive and negative association rules in the database. As a result, the exception rules exist in the form of negative, as well as positive, association. A novel exceptionality measure is proposed to evaluate the candidate exception rules. The candidate exceptions with high exceptionality form the final set of exception rules. Algorithms for mining exception rules are developed and evaluated using an exceptionality measurement, the desired performance of which has been proven.},
author = {Taniar, David and Rahayu, Wenny and Lee, Vincent and Daly, Olena},
doi = {10.1016/j.amc.2008.05.020},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2008 - Exception rules in association rule mining.pdf:pdf},
issn = {00963003},
journal = {Applied Mathematics and Computation},
keywords = {Association rule mining,Association rules,Confidence,Data mining,Exception rules,Exceptionality,Fuzzy association rules,Knowledge discovery,Negative association rules,Support},
month = {nov},
number = {2},
pages = {735--750},
title = {{Exception rules in association rule mining}},
url = {http://www.sciencedirect.com/science/article/pii/S0096300308003524},
volume = {205},
year = {2008}
}
@article{Britos2006,
author = {Britos, Paola and Merlino, Hern{\'{a}}n and Fern{\'{a}}ndez, Enrique and Ochoa, Mar{\'{i}}a Alejandra and Diez, Eduardo and Garc{\'{i}}a-Martinez, Ram{\'{o}}n},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2006 - Tool Selection Methodology in Data Mining.pdf:pdf},
isbn = {970-94770-0-5},
journal = {Memoria T{\'{e}}cnica V Jornadas Iberoamericanas de Ingenier{\'{i}}a de Software e Ingenier{\'{i}}a del Conocimiento},
pages = {85--90},
title = {{Tool Selection Methodology in Data Mining}},
year = {2006}
}
@article{Jorge2006,
author = {Jorge, A.M. and Pereira, F. and Azevedo, P.J.},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2006 - Visual interactive subgroup discovery with numerical properties of interest.pdf:pdf},
isbn = {3540464913},
issn = {16113349},
journal = {Lecture Notes in Computer Science},
pages = {301},
title = {{Visual interactive subgroup discovery with numerical properties of interest}},
url = {http://www.springerlink.com/index/c7413q5208573q64.pdf},
volume = {4265},
year = {2006}
}
@article{Bertsekas2008,
author = {Bertsekas, Dimitri P and Tsitsiklis, John N},
pages = {10--12},
title = {{Posterior P ( {\$}\Theta{\$} = 1 | X = k ) Posterior P ( {\$}\Theta{\$} = 2 | X = k ) Number of heads k Choose {\$}\Theta{\$} = 1 Choose {\$}\Theta{\$} = 2}},
volume = {2008},
year = {2008}
}
@inbook{Teijeiro2016,
address = {Cham},
author = {Teijeiro, Diego and Pardo, Xo{\'{a}}n C. and Gonz{\'{a}}lez, Patricia and Banga, Julio R. and Doallo, Ram{\'{o}}n},
booktitle = {Applications of Evolutionary Computation: 19th European Conference, EvoApplications 2016, Porto, Portugal, March 30 -- April 1, 2016, Proceedings, Part II},
chapter = {Implementi},
doi = {10.1007/978-3-319-31153-1_6},
editor = {Squillero, Giovanni and Burelli, Paolo},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2016 - Applications of Evolutionary Computation 19th European Conference, EvoApplications 2016, Porto, Portugal, March 30 -- April 1,(2).pdf:pdf},
isbn = {978-3-319-31153-1},
pages = {75--90},
publisher = {Springer International Publishing},
title = {{Implementing Parallel Differential Evolution on Spark}},
url = {http://link.springer.com/10.1007/978-3-319-31153-1{\_}6},
year = {2016}
}
@book{Cossio2012,
abstract = {The objective of this case study was to obtain some first-hand information about the functional consequences of a cosmetic tongue split operation for speech and tongue motility. One male patient who had performed the operation on himself was interviewed and underwent a tongue motility assessment, as well as an ultrasound examination. Tongue motility was mildly reduced as a result of tissue scarring. Speech was rated to be fully intelligible and highly acceptable by 4 raters, although 2 raters noticed slight distortions of the sibilants /s/ and /z/. The 3-dimensional ultrasound demonstrated that the synergy of the 2 sides of the tongue was preserved. A notably deep posterior genioglossus furrow indicated compensation for the reduced length of the tongue blade. It is concluded that the tongue split procedure did not significantly affect the participant's speech intelligibility and tongue motility.},
archivePrefix = {arXiv},
arxivId = {arXiv:gr-qc/9809069v1},
author = {Cossio, Mar{\'{i}}a Laura T and Giesen, Laura F and Araya, Gabriela and P{\'{e}}rez-Cotapos, Mar{\'{i}}a Luisa S and VERGARA, RICARDO L{\'{O}}PEZ and Manca, Maura and Tohme, R A and Holmberg, S D and Bressmann, Tim and Lirio, Daniel Rodrigues and Rom{\'{a}}n, Jelitza Soto and Sol{\'{i}}s, Rodrigo Ganter and Thakur, Sanjay and Rao, S V D Nageswara and Modelado, E L and La, Artificial D E and Durante, Cabeza and Tradici{\'{o}}n, U N A and En, Maya and Espejo, E L and Fuentes, D E L A S and Yucat{\'{a}}n, Universidad Aut{\'{o}}noma De and Lenin, Cruz Moreno and Cian, Laura Franco and Douglas, M Joanne and Plata, La and H{\'{e}}ritier, Fran{\c{c}}oise},
booktitle = {Uma {\{}{\'{e}}{\}}tica para quantos?},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {9809069v1},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Probability{\_}{\_}Statistics{\_}{\_}and{\_}Stochastic{\_}Processes.pdf:pdf},
isbn = {9780874216561},
issn = {0717-6163},
keywords = {Adolescence,Adolescencia,Adolescent,Adolescent Behavior,Adolescent Behavior: psychology,Adult,Agresiones al cuerpo,Attachment to the body,Attaque au corps,Autolesiones deliberadas,Automutilation d{\{}{\'{e}}{\}}lib{\{}{\'{e}}{\}}r{\{}{\'{e}}{\}}e,Body Piercing,Body Piercing: psychology,Body Piercing: statistics {\{}{\&}{\}} numerical data,Body image,CUERPO,Chile,Chile: epidemiology,Cosmetic Techniques,Deliberate self-harm,Epidemiologic Methods,Female,Humans,Image corporelle,Imagen corporal,JUVENTUD,MODIFICACIONES CORPORALES,Male,Motivation,Movement,Risk-Taking,Self Mutilation,Self Mutilation: physiopathology,Self Mutilation: ultrasonography,Sex Distribution,Speech Articulation Tests,Speech Intelligibility,Tattooing,Tattooing: psychology,Tattooing: statistics {\{}{\&}{\}} numerical data,Tongue,Tongue: injuries,Tongue: physiopathology,Tongue: ultrasonography,aesthetics,and on cor-,as none were found,autoinjury and health,body,complications did not,complications from inserting a,constituci{\{}{\'{o}}{\}}n del yo,control postural- estabilizaci{\{}{\'{o}}{\}}n- v{\{}{\'{i}}{\}}as,corporal modifications,corps,cuerpo,culturas juveniles,cultures juv{\{}{\'{e}}{\}}niles,epidural,esth{\{}{\'{e}}{\}}tique,est{\{}{\'{e}}{\}}tica,find any reports of,high resolution images,if neuraxial anes-,ing with neuraxial anesthesia,jeunesse,juvenile cultures,juventud,mecanismos de anteroalimentaci{\{}{\'{o}}{\}}n y,modificacio -,needle through a,nes corporales,perforaci{\{}{\'{o}}{\}}n corporal,piel,pr{\{}{\'{a}}{\}}ctica autolesiva,psicoan{\{}{\'{a}}{\}}lisis,research,retroalimentaci{\{}{\'{o}}{\}}n,risks management,segunda piel,sensitivas y motoras,spinal,sustainable reconstruction,tattoo,tattooing,tattoos,tatuaje,the literature on tattoos,was reviewed to see,youth},
number = {2},
pages = {81--87},
pmid = {15003161},
primaryClass = {arXiv:gr-qc},
title = {{Probability{\{}{\_}{\}}{\{}{\_}{\}}Statistics{\{}{\_}{\}}{\{}{\_}{\}}and{\{}{\_}{\}}Stochastic{\{}{\_}{\}}Processes}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15003161{\$}\backslash{\$}nhttp://cid.oxfordjournals.org/lookup/doi/10.1093/cid/cir991{\$}\backslash{\$}nhttp://www.scielo.cl/pdf/udecada/v15n26/art06.pdf{\$}\backslash{\$}nhttp://www.scopus.com/inward/record.url?eid=2-s2.0-84861150233{\{}{\&}{\}}partnerID=tZOtx3y1},
volume = {XXXIII},
year = {2012}
}
@inproceedings{Ekanayake2010,
address = {New York, NY, USA},
author = {Ekanayake, Jaliya and Li, Hui and Zhang, Bingjing and Gunarathne, Thilina and Bae, Seung-Hee and Qiu, Judy and Fox, Geoffrey},
booktitle = {Proceedings of the 19th ACM International Symposium on High Performance Distributed Computing},
doi = {10.1145/1851476.1851593},
isbn = {978-1-60558-942-8},
keywords = {MapReduce,cloud technologies,iterative algorithms},
pages = {810--818},
publisher = {ACM},
series = {HPDC '10},
title = {{Twister: A Runtime for Iterative MapReduce}},
url = {http://doi.acm.org/10.1145/1851476.1851593},
year = {2010}
}
@article{Sigaud2007,
abstract = {Learning classifier systems (LCSs) are rule- based systems that automatically build their ruleset. At the origin of Holland's work, LCSs were seen as a model of the emergence of cognitive abilities thanks to adaptive mechanisms, particularly evolutionary processes. After a renewal of the field more focused on learning, LCSs are now considered as sequential decision problem-solving systems endowed with a generalization property. Indeed, from a Reinforcement Learning point of view, LCSs can be seen as learning systems building a compact representation of their problem thanks to generalization. More recently, LCSs have proved efficient at solving automatic classification tasks. The aim of the present contribution is to describe the state-of- the-art of LCSs, emphasizing recent developments, and focusing more on the sequential decision domain than on automatic classification.},
author = {Sigaud, Olivier and Wilson, Stewart W.},
doi = {10.1007/s00500-007-0164-0},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2007 - Learning classifier systems A survey.pdf:pdf},
issn = {14327643},
journal = {Soft Computing},
keywords = {Generalization,Learning classifier systems,Reinforcement learning},
number = {11},
pages = {1065--1078},
title = {{Learning classifier systems: A survey}},
volume = {11},
year = {2007}
}
@article{Hahsler2005,
abstract = {Mining frequent itemsets and association rules is a popular and well researched approach for discovering interesting relationships between variables in large databases. The R package $\backslash$pkgarules presented in this paper provides a basic infrastructure for creating and manipulating input data sets and for analyzing the resulting itemsets and rules. The package also includes interfaces to two fast mining algorithms, the popular C implementations of Apriori and Eclat by Christian Borgelt. These algorithms can be used to mine frequent itemsets, maximal frequent itemsets, closed frequent itemsets and association rules.},
author = {Hahsler, Michael and Gr{\"{u}}n, Bettina and Hornik, Kurt},
doi = {10.1007/978-1-84800-201-2},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2005 - pkgarules — a Computational Environment for Mining Association Rules and Frequent Item Sets.pdf:pdf},
isbn = {9781848002012},
issn = {1548-7660},
journal = {Journal of Statistical Software},
keywords = {apriori,association rules,data mining,eclat},
number = {15},
pages = {1--25},
title = {{$\backslash$pkgarules — a Computational Environment for Mining Association Rules and Frequent Item Sets}},
url = {http://www.jstatsoft.org/counter.php?id=140{\&}url=v14/i15{\&}ct=2; http://www.jstatsoft.org/counter.php?id=140{\&}url=v14/i15/v14i15.pdf{\&}ct=1},
volume = {14},
year = {2005}
}
@article{CavalcantiNeto2015,
author = {{Cavalcanti Neto}, Edson and {Souza Reboucas}, Elizangela and {Lopes de Moraes}, Jermana and {Luz Gomes}, Samuel and {Pedrosa Reboucas Filho}, Pedro},
doi = {10.1109/TLA.2015.7040658},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Development control parking access using techniques Digital Image Processing and Applied Computational Intelligence.pdf:pdf},
issn = {1548-0992},
journal = {IEEE Latin America Transactions},
keywords = {Applied Computational Intelligence,Biological neural networks,Brazilian pattern,Computational intelligence,Computer Vision,Digital Image Processing,Digital images,Image segmentation,Mechatronics,Pattern recognition,Vehicles,authorisation,car plate detection,car plate recognition,character extraction,character recognition,computational intelligence,development control parking access,digital image processing,feature extraction,movement detection step,object detection,object recognition,parking lot access control,pattern recognition,smart systems,traffic engineering computing},
month = {jan},
number = {1},
pages = {272--276},
publisher = {IEEE},
title = {{Development control parking access using techniques Digital Image Processing and Applied Computational Intelligence}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7040658},
volume = {13},
year = {2015}
}
@book{Fallis2013,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fallis, A.G},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - No Title No Title(3).pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{No Title No Title}},
volume = {53},
year = {2013}
}
@article{Garc??a2010,
abstract = {Experimental analysis of the performance of a proposed method is a crucial and necessary task in an investigation. In this paper, we focus on the use of nonparametric statistical inference for analyzing the results obtained in an experiment design in the field of computational intelligence. We present a case study which involves a set of techniques in classification tasks and we study a set of nonparametric procedures useful to analyze the behavior of a method with respect to a set of algorithms, such as the framework in which a new proposal is developed. Particularly, we discuss some basic and advanced nonparametric approaches which improve the results offered by the Friedman test in some circumstances. A set of post hoc procedures for multiple comparisons is presented together with the computation of adjusted p-values. We also perform an experimental analysis for comparing their power, with the objective of detecting the advantages and disadvantages of the statistical tests described. We found that some aspects such as the number of algorithms, number of data sets and differences in performance offered by the control method are very influential in the statistical tests studied. Our final goal is to offer a complete guideline for the use of nonparametric statistical procedures for performing multiple comparisons in experimental studies. ?? 2009 Elsevier Inc. All rights reserved.},
author = {Garc??a, Salvador and Fern??ndez, Alberto and Luengo, Juli??n and Herrera, Francisco},
doi = {10.1016/j.ins.2009.12.010},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2010 - Advanced nonparametric tests for multiple comparisons in the design of experiments in computational intelligence and data mining.pdf:pdf},
isbn = {0020-0255},
issn = {00200255},
journal = {Information Sciences},
keywords = {Computational intelligence,Data mining,Fuzzy classification systems,Genetics-based machine learning,Multiple comparisons procedures,Nonparametric statistics,Statistical analysis},
number = {10},
pages = {2044--2064},
publisher = {Elsevier Inc.},
title = {{Advanced nonparametric tests for multiple comparisons in the design of experiments in computational intelligence and data mining: Experimental analysis of power}},
url = {http://dx.doi.org/10.1016/j.ins.2009.12.010},
volume = {180},
year = {2010}
}
@article{Bechini2016,
abstract = {Abstract Associative classifiers have proven to be very effective in classification problems. Unfortunately, the algorithms used for learning these classifiers are not able to adequately manage big data because of time complexity and memory constraints. To overcome such drawbacks, we propose a distributed association rule-based classification scheme shaped according to the MapReduce programming model. The scheme mines classification association rules (CARs) using a properly enhanced, distributed version of the well-known FP-Growth algorithm. Once {\{}CARs{\}} have been mined, the proposed scheme performs a distributed rule pruning. The set of survived {\{}CARs{\}} is used to classify unlabeled patterns. The memory usage and time complexity for each phase of the learning process are discussed, and the scheme is evaluated on seven real-world big datasets on the Hadoop framework, characterizing its scalability and achievable speedup on small computer clusters. The proposed solution for associative classifiers turns to be suitable to practically address big datasets even with modest hardware support. Comparisons with two state-of-the-art distributed learning algorithms are also discussed in terms of accuracy, model complexity, and computation time.},
author = {Bechini, Alessio and Marcelloni, Francesco and Segatori, Armando},
doi = {http://dx.doi.org/10.1016/j.ins.2015.10.041},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2016 - A MapReduce solution for associative classification of big data.pdf:pdf},
issn = {0020-0255},
journal = {Information Sciences},
keywords = {Associative classifiers,Big data,Cluster computing frameworks,MapReduce,associative classifiers},
pages = {33--55},
publisher = {Elsevier Inc.},
title = {{A MapReduce solution for associative classification of big data}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025515007793$\backslash$nhttp://linkinghub.elsevier.com/retrieve/pii/S0020025515007793},
volume = {332},
year = {2016}
}
@article{Algorithms1993,
author = {Algorithms, Evolutionary},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1993 - Evolutionary Algorithms for Parameter Optimization.pdf:pdf},
journal = {Evolutionary Computation},
number = {1},
pages = {1--23},
title = {{Evolutionary Algorithms for Parameter Optimization}},
year = {1993}
}
@article{Zaharia2012,
abstract = {We present Resilient Distributed Datasets (RDDs), a distributed memory abstraction that lets programmers perform in-memory computations on large clusters in a fault-tolerant manner. RDDs are motivated by two types of applications that current computing frameworks handle inefficiently: iterative algorithms and interactive data mining tools. In both cases, keeping data in memory can improve performance by an order of magnitude. To achieve fault tolerance efficiently, RDDs provide a restricted form of shared memory, based on coarse-grained transformations rather than fine-grained updates to shared state. However, we show that RDDs are expressive enough to capture a wide class of computations, including recent specialized programming models for iterative jobs, such as Pregel, and new applications that these models do not capture. We have implemented RDDs in a system called Spark, which we evaluate through a variety of user applications and benchmarks.},
archivePrefix = {arXiv},
arxivId = {EECS-2011-82},
author = {Zaharia, Matei and Chowdhury, Mosharaf and Das, Tathagata and Dave, Ankur},
doi = {10.1111/j.1095-8649.2005.00662.x},
eprint = {EECS-2011-82},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Resilient distributed datasets A fault-tolerant abstraction for in-memory cluster computing.pdf:pdf},
isbn = {978-931971-92-8},
issn = {00221112},
journal = {NSDI'12 Proceedings of the 9th USENIX conference on Networked Systems Design and Implementation},
pages = {2--2},
pmid = {2011},
title = {{Resilient distributed datasets: A fault-tolerant abstraction for in-memory cluster computing}},
url = {https://www.usenix.org/system/files/conference/nsdi12/nsdi12-final138.pdf},
year = {2012}
}
@article{Jensen2005,
author = {Jensen, Richard},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2005 - Combining rough and fuzzy sets for feature selection.pdf:pdf},
journal = {Philosophy},
title = {{Combining rough and fuzzy sets for feature selection Doctor of Philosophy School of Informatics University of Edinburgh}},
year = {2005}
}
@article{Peralta2015,
annote = {{\textless}RANK{\textgreater} 2014 - Q3},
author = {Peralta, Daniel and del R{\'{i}}o, Sara and Ram{\'{i}}rez-Gallego, Sergio and Riguero, Isaac and Benitez, Jose M and Herrera, Francisco},
doi = {10.1155/2015/246139},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Evolutionary Feature Selection for Big Data Classification A MapReduce Approach Evolutionary Feature Selection for Big Data Clas.pdf:pdf},
journal = {Mathematical Problems in Engineering},
number = {JANUARY},
title = {{Evolutionary Feature Selection for Big Data Classification : A MapReduce Approach Evolutionary Feature Selection for Big Data Classification : A MapReduce Approach}},
url = {http://sci2s.ugr.es/sites/default/files/2015-hindawi-peralta.pdf},
volume = {2015},
year = {2015}
}
@book{Kavsek2006,
author = {Kav{\v{s}}ek, Branko and Lavra{\v{c}}, Nada},
booktitle = {Applied Artificial Intelligence},
doi = {10.1080/08839510600779688},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2006 - Apriori-Sd Adapting Association Rule Learning To Subgroup Discovery.pdf:pdf},
isbn = {0883951060077},
issn = {0883-9514},
number = {7},
pages = {543--583},
title = {{Apriori-Sd: Adapting Association Rule Learning To Subgroup Discovery}},
url = {http://www.tandfonline.com/doi/abs/10.1080/08839510600779688},
volume = {20},
year = {2006}
}
@article{Kostyukov2013,
abstract = {This paper describes a new and purely functional implementation technique of binary heaps. A binary heap is a tree-based data structure that implements priority queue operations (insert, remove, minimum/maximum) and guarantees at worst logarithmic running time for them. Approaches and ideas described in this paper present a simple and asymptotically optimal implementation of immutable binary heap.},
archivePrefix = {arXiv},
arxivId = {1312.4666},
author = {Kostyukov, Vladimir},
eprint = {1312.4666},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - A Functional Approach to Standard Binary Heaps.pdf:pdf},
pages = {1--8},
title = {{A Functional Approach to Standard Binary Heaps}},
url = {http://arxiv.org/abs/1312.4666v1},
year = {2013}
}
@book{Lutz2010,
abstract = {Python is optimized for quality, productivity, portability, and integration. Hundreds of thousands of Python developers around the world rely on Python for general-purpose tasks, Internet scripting, systems programming, user interfaces, and product customization. Available on all major computing platforms, including commercial versions of Unix, Linux, Windows, and Mac OS X, Python is portable, powerful and remarkable easy to use. With its convenient, quick-reference format, "Python Pocket Reference," 3rd Edition is the perfect on-the-job reference. More importantly, it's now been refreshed to cover the language's latest release, Python 2.4. For experienced Python developers, this book is a compact toolbox that delivers need-to-know information at the flip of a page. This third edition also includes an easy-lookup index to help developers find answers fast! Python 2.4 is more than just optimization and library enhancements; it's also chock full of bug fixes and upgrades. And these changes are addressed in the "Python Pocket Reference," 3rd Edition. New language features, new and upgraded built-ins, and new and upgraded modules and packages--they're all clarified in detail. The "Python Pocket Reference," 3rd Edition serves as the perfect companion to "Learning Python" and "Programming Python,"},
author = {Lutz, Mark},
booktitle = {Chemistry {\&} {\ldots}},
doi = {10.1002/1521-3773(20010316)40:6<9823::AID-ANIE9823>3.3.CO;2-C},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2010 - Python in your pocket.pdf:pdf},
isbn = {9780596158088},
issn = {14337851},
pages = {210},
title = {{Python in your pocket}},
year = {2010}
}
@article{Dzyuba2013,
author = {Dzyuba, Vladimir and {Van Leeuwen}, Matthijs},
doi = {10.1007/978-3-642-41398-8_14},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - Interactive discovery of interesting subgroup sets.pdf:pdf},
isbn = {9783642413971},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Interactive data mining,pattern set mining},
pages = {150--161},
title = {{Interactive discovery of interesting subgroup sets}},
volume = {8207 LNCS},
year = {2013}
}
@article{Aguilar-Ruiz2007,
abstract = {Some of the most influential factors in the quality of the solutions found by an evolutionary algorithm (EA) are a correct coding of the search space and an appropriate evaluation function of the potential solutions. EAs are often used to learn decision rules from datasets, which are encoded as individuals in the genetic population. In this paper, the coding of the search space for the obtaining of those decision rules is approached, i.e., the representation of the individuals of the genetic population and also the design of specific genetic operators. Our approach, called "natural coding," uses one gene per feature in the dataset (continuous or discrete). The examples from the datasets are also encoded into the search space, where the genetic population evolves, and therefore the evaluation process is improved substantially. Genetic operators for the natural coding are formally defined as algebraic expressions. Experiments with several datasets from the University of California at Irvine (UCI) machine learning repository show that as the genetic operators are better guided through the search space, the number of rules decreases considerably while maintaining the accuracy, similar to that of hybrid coding, which joins the well-known binary and real representations to encode discrete and continuous attributes, respectively. The computational cost associated with the natural coding is also reduced with regard to the hybrid representation. Our algorithm, HlDER*, has been statistically tested against C4.5 and C4.5 Rules, and performed well. The knowledge models obtained are simpler, with very few decision rules, and therefore easier to understand, which is an advantage in many domains. The experiments with high-dimensional datasets showed the same good behavior, maintaining the quality of the knowledge model with respect to prediction accuracy.},
author = {Aguilar-Ruiz, Jesus S. and Giraldez, Raul and Riquelme, Jose C.},
doi = {10.1109/TEVC.2006.883466},
issn = {1089-778X},
journal = {IEEE Transactions on Evolutionary Computation},
keywords = {Accuracy,Computational efficiency,Decision rules,Encoding,Evolutionary computation,Genetics,HlDER* algorithm,Machine learning,Performance evaluation,Predictive models,Supervised learning,Testing,algebraic expressions,continuous attributes,decision rule learning,discrete attributes,evolutionary algorithm,evolutionary encoding,evolutionary supervised learning,genetic algorithms,genetic operators,genetic population,knowledge representation,learning (artificial intelligence),machine learning,natural coding,natural encoding,search problems,search space coding,supervised learning},
month = {aug},
number = {4},
pages = {466--479},
shorttitle = {Evolutionary Computation, IEEE Transactions on},
title = {{Natural Encoding for Evolutionary Supervised Learning}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4280854},
volume = {11},
year = {2007}
}
@article{Friedman2008,
abstract = {General regression and classification models are constructed as linear combinations of simple rules derived from the data. Each rule consists of a conjunction of a small number of simple statements concerning the values of individual input variables. These rule ensembles are shown to produce predictive accuracy comparable to the best methods. However, their principle advantage lies in interpretation. Because of its simple form, each rule is easy to understand, as it its influence on individual predictions, selected subsets of predictions, or globally over the entire space of joint input variable values. Similarly, the degree of relevance of the respective input variables can be assessed globally, locally in different regions of the input space, or at individual prediction points. Techniques are presented for automatically identifying those variables that are involved in interactions with other variables, the strength and degree of those interactions, as well as the indentities of the other variables with which they interact. Graphical representations are used to visualize both main and interaction effects.},
author = {Friedman, Jerome H. and Popescu, Bogdan E.},
doi = {10.1214/07-AOAS148},
issn = {1932-6157},
journal = {The Annals of Applied Statistics},
keywords = {REGRESSION,Regression,SHRINKAGE,classification,data mining,interaction effects,learning ensembles,machine learning,rules,variable importance},
month = {sep},
number = {3},
pages = {916--954},
publisher = {INST MATHEMATICAL STATISTICS, 3163 SOMERSET DR, CLEVELAND, OH 44122 USA},
title = {{Predictive learning via rule ensembles}},
url = {https://apps.webofknowledge.com/full{\_}record.do?product=UA{\&}search{\_}mode=GeneralSearch{\&}qid=1{\&}SID=U2DYtuhHhg5Ogj6Byyu{\&}page=1{\&}doc=2},
volume = {2},
year = {2008}
}
@book{Mashayekhi2015,
abstract = {Random forest (RF) is a tree-based learning method, which exhibits a high ability to generalize on real data sets. Nevertheless, a possible limitation of RF is that it generates a forest consisting of many trees and rules, thus it is viewed as a black box model. In this paper, the RF+HC methods for rule extraction from RF are proposed. Once the RF is built, a hill climbing algorithm is used to search for a rule set such that it reduces the number of rules dramatically, which significantly improves comprehensibility of the underlying model built by RF. The proposed methods are evaluated on eighteen UCI and four microarray data sets. Our experimental results show that the proposed methods outperform one of the state-of-the-art methods in terms of scalability and comprehensibility while preserving the same level of accuracy.},
address = {Cham},
author = {Mashayekhi, Morteza and Gras, Robin},
booktitle = {ADVANCES IN ARTIFICIAL INTELLIGENCE (AI 2015)},
doi = {10.1007/978-3-319-18356-5},
editor = {Barbosa, Denilson and Milios, Evangelos},
isbn = {978-3-319-18355-8},
issn = {0302-9743},
keywords = {CLASSIFICATION,ENSEMBLE,Hill climbing,MODEL,Random forest,Rule extraction},
pages = {223--237},
publisher = {Springer International Publishing},
series = {Lecture Notes in Computer Science},
title = {{Advances in Artificial Intelligence}},
url = {https://apps.webofknowledge.com/full{\_}record.do?product=WOS{\&}search{\_}mode=CitingArticles{\&}qid=3{\&}SID=U2DYtuhHhg5Ogj6Byyu{\&}page=2{\&}doc=11},
volume = {9091},
year = {2015}
}
@article{DelaIglesia2013,
author = {de la Iglesia, Beatriz},
doi = {10.1002/widm.1106},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - Evolutionary computation for feature selection in classification problems.pdf:pdf},
issn = {19424787},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
month = {nov},
number = {6},
pages = {381--407},
title = {{Evolutionary computation for feature selection in classification problems}},
url = {http://doi.wiley.com/10.1002/widm.1106},
volume = {3},
year = {2013}
}
@article{Lemmerich2011,
abstract = {This paper presents an approach for modeling location-based profiles of social image media based on tagging information and collaborative geo-reference annotations. We utilize pattern mining techniques for obtaining sets of tags that are specific for the specified point, landmark, or region of interest. Next, we show how these candidate patterns can be presented and visualized for interactive exploration using a combination of general pattern mining visualizations and views specialized on geo-referenced tagging data. We present a case study using publicly available data from the Flickr photo sharing application.},
author = {Lemmerich, Florian and Atzmueller, Martin},
doi = {10.1109/PASSAT/SocialCom.2011.186},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2011 - Modeling location-based profiles of social image media using explorative pattern mining.pdf:pdf},
isbn = {9780769545783},
journal = {Proceedings - 2011 IEEE International Conference on Privacy, Security, Risk and Trust and IEEE International Conference on Social Computing, PASSAT/SocialCom 2011},
pages = {1356--1362},
title = {{Modeling location-based profiles of social image media using explorative pattern mining}},
year = {2011}
}
@article{Gui2015,
abstract = {Frequent itemset mining is an important step of association rules mining. Traditional frequent itemset mining algorithms have certain limitations. For example Apriori algorithm has to scan the input data repeatedly, which leads to high I/O load and low performance, and the FP-Growth algorithm is limited by the capacity of computer's inner stores because it needs to build a FP-tree and mine frequent itemset on the basis of the FP-tree in memory. With the coming of the Big Data era, these limitations are becoming more prominent when confronted with mining large-scale data. In this paper, DPBM, a distributed matrix-based pruning algorithm based on Spark, is proposed to deal with frequent itemset mining. DPBM can greatly reduce the amount of candidate itemset by introducing a novel pruning technique for matrix-based frequent itemset mining algorithm, an improved Apriori algorithm which only needs to scan the input data once. In addition, each computer node reduces greatly the memory usage by implementing DPBM under a latest distributed environment-Spark, which is a lightning-fast distributed computing. The experimental results show that DPBM have better performance than MapReduce-based algorithms for frequent itemset mining in terms of speed and scalability.},
author = {Gui, Feng and Ma, Yunlong and Zhang, Feng and Liu, Min and Li, Fei and Shen, Weiming and Bai, Hua},
doi = {10.1109/CSCWD.2015.7230970},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - A distributed frequent itemset mining algorithm based on Spark.pdf:pdf},
isbn = {978-1-4799-2002-0},
issn = {1573-7543},
journal = {2015 IEEE 19th International Conference on Computer Supported Cooperative Work in Design (CSCWD)},
keywords = {FP-growth algorithm,FP-tree,I/O load,MapReduce,Spark,apriori algorithm,association rules mining,data mining,distributed algorithm,distributed frequent itemset mining algorithm,distributed matrix-based pruning algorithm,frequent itemset mining,input-output programs,matrix algebra,matrix-pruning,trees (mathematics)},
number = {4},
pages = {271--275},
publisher = {Springer US},
title = {{A distributed frequent itemset mining algorithm based on Spark}},
url = {http://ieeexplore.ieee.org/articleDetails.jsp?arnumber=7230970},
volume = {18},
year = {2015}
}
@article{Atzmueller2015,
abstract = {Subgroup discovery is a broadly applicable descriptive datamining technique for identifying interesting subgroups according to some property of interest. This arti- cle summarizes fundamentals of subgroup discovery, before that it also reviews algorithms and further advanced methodological issues. In addition, we briefly discuss tools and applications of subgroup discovery approaches. In that con- text, we also discuss experiences and lessons learned and outline some of the future directions in order to show the advantages and benefits of subgroup discovery. {\textcopyright}},
author = {Atzmueller, Martin},
doi = {10.1002/widm.1144},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Subgroup discovery.pdf:pdf},
issn = {19424795},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
number = {1},
pages = {35--49},
title = {{Subgroup discovery}},
volume = {5},
year = {2015}
}
@article{Carmona2015,
abstract = {This paper proposes a novel algorithm for subgroup discovery task based on genetic programming and fuzzy logic called Fuzzy Genetic Programming-based for Subgroup Discovery (FuGePSD). The genetic programming allows to learn compact expressions with the main objective to obtain rules for describing simple, interesting and interpretable subgroups. This algorithm incorporates specific operators in the search process to promote the diversity between the individuals. The evolutionary scheme of FuGePSD is codified through the genetic cooperative-competitive approach promoting the competition and cooperation between the individuals of the population in order to find out the optimal solutions for the SD task. FuGePSD displays its potential with high-quality results in a wide experimental study performed with respect to others evolutionary algorithms for subgroup discovery. Moreover, the quality of this proposal is applied to a case study related to acute sore throat problems.},
author = {Carmona, C. J. and Ruiz-Rodado, V. and {Del Jesus}, M. J. and Weber, A. and Grootveld, M. and Gonz??lez, P. and Elizondo, D.},
doi = {10.1016/j.ins.2014.11.030},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - A fuzzy genetic programming-based algorithm for subgroup discovery and the application to one problem of pathogenesis of acute so.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Bioinformatics,Evolutionary fuzzy system,Genetic programming,Subgroup discovery},
pages = {180--197},
publisher = {Elsevier Inc.},
title = {{A fuzzy genetic programming-based algorithm for subgroup discovery and the application to one problem of pathogenesis of acute sore throat conditions in humans}},
url = {http://dx.doi.org/10.1016/j.ins.2014.11.030},
volume = {298},
year = {2015}
}
@article{CarrierC.Povel2003,
author = {{Carrier, C., Povel}, O.},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2003 - Characterising data mining software.pdf:pdf},
journal = {Intelligent Data Analysis},
keywords = {business decision support,data mining,tool characterisation,tool comparison},
number = {February},
pages = {181--192},
title = {{Characterising data mining software}},
volume = {7},
year = {2003}
}
@inproceedings{Ma2009,
address = {Montreal, Quebec},
annote = {URL Large Dataset},
author = {Ma, Justin and Saul, Lawrence K and Savage, Stefan and Voelker, Geoffrey M},
booktitle = {Proceedings of the International Conference on Machine Learning (ICML)},
file = {:home/raul/Desktop/url-icml2009.pdf:pdf},
title = {{Identifying Suspicious URLs : An Application of Large-Scale Online Learning}},
year = {2009}
}
@book{Fallis2013d,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fallis, A.G},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - No Title No Title(2).pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{No Title No Title}},
volume = {53},
year = {2013}
}
@article{Luna2014,
abstract = {This paper proposes a novel grammar-guided genetic$\backslash$nprogramming algorithm for subgroup discovery. This$\backslash$nalgorithm, called comprehensible grammar-based$\backslash$nalgorithm for subgroup discovery (CGBA-SD), combines$\backslash$nthe requirements of discovering comprehensible rules$\backslash$nwith the ability to mine expressive and flexible$\backslash$nsolutions owing to the use of a context-free grammar.$\backslash$nEach rule is represented as a derivation tree that$\backslash$nshows a solution described using the language denoted$\backslash$nby the grammar. The algorithm includes mechanisms to$\backslash$nadapt the diversity of the population by self-adapting$\backslash$nthe probabilities of recombination and mutation. We$\backslash$ncompare the approach with existing evolutionary and$\backslash$nclassic subgroup discovery algorithms. CGBA-SD appears$\backslash$nto be a very promising algorithm that discovers$\backslash$ncomprehensible subgroups and behaves better than other$\backslash$nalgorithms as measures by complexity, interest, and$\backslash$nprecision indicate. The results obtained were validated$\backslash$nby means of a series of nonparametric tests.},
author = {Luna, Jos?? Mar??a and Romero, Jos?? Ra??l and Romero, Crist??bal and Ventura, Sebasti??n},
doi = {10.1109/TCYB.2014.2306819},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - On the use of genetic programming for mining comprehensible rules in subgroup discovery.pdf:pdf},
issn = {21682267},
journal = {IEEE Transactions on Cybernetics},
keywords = {Data mining (DM),Genetic programming (GP),Grammar-guided genetic programming (G3P),Subgroup discovery (SD).},
number = {12},
pages = {2329--2341},
title = {{On the use of genetic programming for mining comprehensible rules in subgroup discovery}},
volume = {44},
year = {2014}
}
@article{Kranjc2012,
abstract = {This paper presents an open cloud based platform for composition, execution, and sharing of interactive data mining workflows. It is based on the principles of service-oriented knowledge discovery, and features interactive scientific workflows. In contrast to comparable data mining platforms, our platform runs in all major Web browsers and platforms, including mobile devices. In terms of crowdsourcing, ClowdFlows provides researchers with an easy way to expose and share their work and results, as only an Internet connection and a Web browser are required to access the workflows from anywhere. Practitioners can use ClowdFlows to seamlessly integrate and join different implementations of algorithms, tools and Web services into a coherent workflow that can be executed in a cloud based application. ClowdFlows is also easily extensible during run-time by importing Web services and using them as new workflow components.},
author = {Kranjc, Janez and Podpe{\v{c}}an, Vid and Lavra{\v{c}}, Nada},
doi = {10.1007/978-3-642-33486-3_54},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - ClowdFlows A cloud based scientific workflow platform.pdf:pdf},
isbn = {9783642334856},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {cloud computing,data mining platform,scientific workflows,service-oriented architecture,web application,web services},
number = {PART 2},
pages = {816--819},
title = {{ClowdFlows: A cloud based scientific workflow platform}},
volume = {7524 LNAI},
year = {2012}
}
@article{Kralj2005,
author = {Kralj, Petra and Lavrac, N and Zupan, B},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2005 - Subgroup visualization.pdf:pdf},
journal = {8th International Multiconference Information {\ldots}},
number = {1},
pages = {2--6},
title = {{Subgroup visualization}},
url = {http://kt.ijs.si/petra{\_}kralj/publications/IS-2005-KraljEtAl-SubgroupVizualization.pdf},
year = {2005}
}
@article{R.A.1991,
author = {R.A., Day},
keywords = {7530},
number = {normas publicaci{\{}{\{}{\}}{\{}{\'{o}}{\}}{\{}{\}}{\}}n},
pages = {--------},
title = {{C{\{}{\{}{\}}{\{}{\'{o}}{\}}{\{}{\}}{\}}mo escribir y publicar trabajos cient{\{}{\{}{\}}{\{}{\'{i}}{\}}{\{}{\}}{\}}ficos745}},
year = {1991}
}
@article{Programming1999,
author = {Programming, Functional},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1999 - Functional programming.pdf:pdf},
pages = {25--37},
title = {{Functional programming}},
year = {1999}
}
@article{Li2016,
abstract = {Feature selection, as a data preprocessing strategy, has been proven to be effective and efficient in preparing high-dimensional data for data mining and machine learning problems. The objectives of feature selection include: building simpler and more comprehensible models, improving data mining performance, and preparing clean, understandable data. The recent proliferation of big data has presented some substantial challenges and opportunities of feature selection algorithms. In this survey, we provide a comprehensive and structured overview of recent advances in feature selection research. Motivated by current challenges and opportunities in the big data age, we revisit feature selection research from a data perspective, and review representative feature selection algorithms for generic data, structured data, heterogeneous data and streaming data. Methodologically, to emphasize the differences and similarities of most existing feature selection algorithms for generic data, we generally categorize them into four groups: similarity based, information theoretical based, sparse learning based and statistical based methods. Finally, to facilitate and promote the research in this community, we also present a open-source feature selection repository that consists of most of the popular feature selection algorithms (http://featureselection.asu.edu/). At the end of this survey, we also have a discussion about some open problems and challenges that need to be paid more attention in future research.},
archivePrefix = {arXiv},
arxivId = {1601.07996},
author = {Li, Jundong and Cheng, Kewei and Wang, Suhang and Morstatter, Fred and Trevino, Robert P. and Tang, Jiliang and Liu, Huan},
eprint = {1601.07996},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2016 - Feature Selection A Data Perspective.pdf:pdf},
month = {jan},
title = {{Feature Selection: A Data Perspective}},
url = {http://arxiv.org/abs/1601.07996},
year = {2016}
}
@article{EntrepreneurialInsights2015,
author = {{Entrepreneurial Insights}},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Big Data.pdf:pdf},
isbn = {9781617290343},
issn = {0422-2784},
title = {{Big Data}},
url = {http://www.entrepreneurial-insights.com/lexicon/big-data/},
year = {2015}
}
@article{Novak2009,
abstract = {This paper gives a survey of contrast set mining (CSM), emerging pattern mining (EPM), and subgroup discovery (SD) in a unifying framework named supervised descriptive rule discovery. While all these research areas aim at discovering patterns in the form of rules induced from labeled data, they use different terminology and task definitions, claim to have different goals, claim to use different rule learning heuristics, and use different means for selecting subsets of induced patterns. This paper contributes a novel understanding of these subareas of data mining by presenting a unified terminology, by explaining the apparent differences between the learning tasks as variants of a unique supervised descriptive rule discovery task and by exploring the apparent differences between the approaches. It also shows that various rule learning heuristics used in CSM, EPM and SD algorithms all aim at optimizing a trade off between rule coverage and precision. The commonalities (and differences) between the approaches are showcased on a selection of best known variants of CSM, EPM and SD algorithms. The paper also provides a critical survey of existing supervised descriptive rule discovery visualization methods.},
author = {Novak, Petra Kralj and Lavraˇ, Nada and Webb, Geoffrey I},
doi = {10.1145/1577069.1577083},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - Supervised Descriptive Rule Discovery A Unifying Survey of Contrast Set , Emerging Pattern and Subgroup Mining.pdf:pdf},
isbn = {1532-4435},
issn = {15324435},
journal = {Journal of Machine Learning Research},
keywords = {contrast set mining,descriptive rules,emerging patterns,rule learning,subgroup},
pages = {377--403},
title = {{Supervised Descriptive Rule Discovery : A Unifying Survey of Contrast Set , Emerging Pattern and Subgroup Mining}},
url = {http://eprints.pascal-network.org/archive/00004412/},
volume = {10},
year = {2009}
}
@article{Rossum2012d,
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Logging HOWTO.pdf:pdf},
journal = {Info},
title = {{Logging HOWTO}},
year = {2012}
}
@article{Rossum2014a,
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - Argparse Tutorial.pdf:pdf},
title = {{Argparse Tutorial}},
year = {2014}
}
@article{Schrijver2004,
abstract = {Lecture notes for a combinatorial optimization course.},
author = {Schrijver, Alexander},
doi = {10.1017/CBO9780511616655},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2004 - A course in combinatorial optimization.pdf:pdf},
isbn = {0521811511$\backslash$n0521010128 (pbk.)},
journal = {Lecture Notes},
pages = {222},
pmid = {13450290},
title = {{A course in combinatorial optimization}},
year = {2004}
}
@article{Xue2013,
abstract = {Classification problems often have a large number of features in the data sets, but not all of them are useful for classification. Irrelevant and redundant features may even reduce the performance. Feature selection aims to choose a small number of relevant features to achieve similar or even better classification performance than using all features. It has two main conflicting objectives of maximizing the classification performance and minimizing the number of features. However, most existing feature selection algorithms treat the task as a single objective problem. This paper presents the first study on multi-objective particle swarm optimization (PSO) for feature selection. The task is to generate a Pareto front of nondominated solutions (feature subsets). We investigate two PSO-based multi-objective feature selection algorithms. The first algorithm introduces the idea of nondominated sorting into PSO to address feature selection problems. The second algorithm applies the ideas of crowding, mutation, and dominance to PSO to search for the Pareto front solutions. The two multi-objective algorithms are compared with two conventional feature selection methods, a single objective feature selection method, a two-stage feature selection algorithm, and three well-known evolutionary multi-objective algorithms on 12 benchmark data sets. The experimental results show that the two PSO-based multi-objective algorithms can automatically evolve a set of nondominated solutions. The first algorithm outperforms the two conventional methods, the single objective method, and the two-stage algorithm. It achieves comparable results with the existing three well-known multi-objective algorithms in most cases. The second algorithm achieves better results than the first algorithm and all other methods mentioned previously.},
author = {Xue, Bing and Zhang, Mengjie and Browne, Will N},
doi = {10.1109/TSMCB.2012.2227469},
issn = {2168-2275},
journal = {IEEE transactions on cybernetics},
keywords = {Algorithms,Artificial Intelligence,Decision Support Techniques,Information Storage and Retrieval,Information Storage and Retrieval: methods,Pattern Recognition, Automated,Pattern Recognition, Automated: methods},
month = {dec},
number = {6},
pages = {1656--71},
pmid = {24273143},
shorttitle = {IEEE Transactions on Cybernetics},
title = {{Particle swarm optimization for feature selection in classification: a multi-objective approach.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24273143},
volume = {43},
year = {2013}
}
@incollection{NIPS2012_4824,
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E and Low-rank-decomposition, Index Terms},
booktitle = {Advances in Neural Information Processing Systems 25},
doi = {10.1109/TPAMI.2012.125},
editor = {Pereira, F and Burges, C J C and Bottou, L and Weinberger, K Q},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2004 - Short Papers {\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}{\_}.pdf:pdf},
isbn = {0162-8828 VO - 34},
issn = {0162-8828},
number = {11},
pages = {1097--1105},
pmid = {22641701},
publisher = {Curran Associates, Inc.},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
volume = {23},
year = {2012}
}
@article{Zafra2012,
abstract = {In machine learning the so-called curse of dimensionality, pertinent to many classification algorithms, denotes the drastic increase in computational complexity and classification error with data having a great number of dimensions. In this context, feature selection techniques try to reduce dimensionality finding a new more compact representation of instances selecting the most informative features and removing redundant, irrelevant, and/or noisy features. In this paper, we propose a filter-based feature selection method for working in the multiple-instance learning scenario called ReliefF-MI; it is based on the principles of the well-known ReliefF algorithm. Different extensions are designed and implemented and their performance checked in multiple instance learning. ReliefF-MI is applied as a pre-processing step that is completely independent from the multi-instance classifier learning process and therefore is more efficient and generic than wrapper approaches proposed in this area. Experimental results on five benchmark real-world data sets and 17 classification algorithms confirm the utility and efficiency of this method, both statistically and from the point of view of execution time.},
author = {Zafra, Amelia and Pechenizkiy, Mykola and Ventura, Sebasti{\'{a}}n},
doi = {10.1016/j.neucom.2011.03.052},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - ReliefF-MI An extension of ReliefF to multiple instance learning.pdf:pdf},
issn = {09252312},
journal = {Neurocomputing},
number = {1},
pages = {210--218},
title = {{ReliefF-MI: An extension of ReliefF to multiple instance learning}},
volume = {75},
year = {2012}
}
@article{DelJesus2007,
abstract = {This paper presents a genetic fuzzy system for the data mining task of subgroup discovery, the subgroup discovery iterative genetic algorithm (SDIGA), which obtains fuzzy rules for subgroup discovery in disjunctive normal form. This kind of fuzzy rule allows us to represent knowledge about patterns of interest in an explanatory and understandable form that can be used by the expert. Experimental evaluation of the algorithm and a comparison with other subgroup discovery algorithms show the validity of the proposal. SDIGA is applied to a market problem studied in the University of Mondragon, Spain, in which it is necessary to extract automatically relevant and interesting information that helps to improve fair planning policies. The application of SDIGA to this problem allows us to obtain novel and valuable knowledge for experts.},
author = {del Jesus, Mar{\'{i}}a Jos{\'{e}} and Gonz{\'{a}}lez, Pedro and Herrera, Francisco and Mesonero, Mikel},
doi = {10.1109/TFUZZ.2006.890662},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2007 - Evolutionary fuzzy rule induction process for subgroup discovery A case study in marketing.pdf:pdf},
issn = {10636706},
journal = {IEEE Transactions on Fuzzy Systems},
keywords = {Data mining,Descriptive induction,Evolutionary algorithms,Genetic fuzzy systems,Subgroup discovery},
number = {4},
pages = {578--592},
title = {{Evolutionary fuzzy rule induction process for subgroup discovery: A case study in marketing}},
volume = {15},
year = {2007}
}
@article{Rossum2009,
abstract = {This document describes how to write modules in C or C++ to extend the Python interpreter with new modules. Those modules can define new functions but also new object types and their methods. The document also describes how to embed the Python interpreter in another application, for use as an extension language. Finally, it shows howto compile and link extension modules so that they can be loaded dynamically (at run time) into the interpreter, if the underlying operating system supports this feature.},
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - Extending and Embedding Python.pdf:pdf},
pages = {100},
title = {{Extending and Embedding Python}},
year = {2009}
}
@incollection{García2015,
address = {Cham},
author = {Garc{\'{i}}a, Salvador and Luengo, Juli{\'{a}}n and Herrera, Francisco},
booktitle = {Data Preprocessing in Data Mining},
doi = {10.1007/978-3-319-10247-4_7},
isbn = {978-3-319-10247-4},
pages = {163--193},
publisher = {Springer International Publishing},
title = {{Feature Selection}},
url = {http://dx.doi.org/10.1007/978-3-319-10247-4{\_}7},
year = {2015}
}
@article{Carmona2013a,
abstract = {Subgroup discovery is a broadly applicable data mining technique whose main objective is the search for descriptions of subgroups of data that are statistically unusual with respect to a property of interest. The obtaining of general rules describing as many instances as possible is preferred in subgroup discovery, but this can lead to less accurate descriptions that incorrectly describe some instances. Under certain conditions, these incorrectly-described instances can be grouped into exceptions. A new post-processing methodology for the detection of exceptions associated to previously discovered subgroups is presented in this paper. The purpose is to obtain a new description to improve the accuracy of the initial subgroup and to describe new small spaces in data with unusual behaviour within the subgroup. This post-processing methodology can be applied to the results of any subgroup discovery algorithm. A post-processing multiobjective evolutionary fuzzy system is developed following this methodology, the Multiobjective Evolutionary Fuzzy system for the detection of Exceptions in Subgroups (MEFES). A wide experimental study has been performed, supported by statistical tests, comparing the results obtained by representative subgroup discovery algorithms with those obtained after applying the post-processing algorithm. Finally, MEFES is applied in a real problem related to the description of the behaviour of a type of solar cell in the Concentrating Photovoltaic area providing useful information to the experts. ?? 2013 Elsevier B.V. All rights reserved.},
author = {Carmona, C. J. and Gonz??lez, P. and Garc??a-Domingo, B. and {Del Jesus}, M. J. and Aguilera, J.},
doi = {10.1016/j.knosys.2013.08.001},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - MEFES An evolutionary proposal for the detection of exceptions in subgroup discovery. An application to Concentrating Photovoltai.pdf:pdf},
isbn = {0950-7051},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Concentrating Photovoltaic Technology,Evolutionary fuzzy system,Exception rules,Multiobjective evolutionary algorithm,Subgroup discovery},
pages = {73--85},
publisher = {Elsevier B.V.},
title = {{MEFES: An evolutionary proposal for the detection of exceptions in subgroup discovery. An application to Concentrating Photovoltaic Technology}},
url = {http://dx.doi.org/10.1016/j.knosys.2013.08.001},
volume = {54},
year = {2013}
}
@article{Rissino2009,
author = {Rissino, Silvia and Lambert-torres, Germano},
file = {:home/raul/Downloads/5939.pdf:pdf},
isbn = {9783902613530},
journal = {Data Mining and Knowledge Discovery in Real Life Applications},
number = {February},
pages = {35--58},
title = {{Rough Set Theory – Fundamental Concepts , Principals , Data Extraction , and Applications}},
year = {2009}
}
@article{Alcala-Fdez2011,
abstract = {This work is related to the KEEL (Knowledge Extraction based on Evolutionary Learning) tool, an open source software that supports data management and a designer of experiments. KEEL pays special attention to the implementation of evolutionary learning and soft computing based techniques for Data Mining problems including regression, classification, clustering, pattern mining and so on. The aim of this paper is to present three new aspects of KEEL: KEELdataset, a data set repository which includes the data set partitions in the KEEL format and shows some results of algorithms in these data sets; some guidelines for including new algorithms in KEEL, helping the researchers to make their methods easily accessible to other authors and to compare the results of many approaches already included within the KEEL software; and a module of statistical procedures developed in order to provide to the researcher a suitable tool to contrast the results obtained in any experimental study.Acase of study is given to illustrate a complete case of application within this experimental analysis framework. {\textcopyright} 2011 Old City Publishing, Inc.},
author = {Alcal{\'{a}}-Fdez, J. and Fern{\'{a}}ndez, A. and Luengo, J. and Derrac, J. and Garc{\'{i}}a, S. and S{\'{a}}nchez, L. and Herrera, F.},
doi = {10.1007/s00500-008-0323-y},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2011 - KEEL data-mining software tool Data set repository, integration of algorithms and experimental analysis framework.pdf:pdf},
issn = {15423980},
journal = {Journal of Multiple-Valued Logic and Soft Computing},
keywords = {Data mining,Data set repository,Evolutionary algorithms,Java,Knowledge extraction,Machine learning,data mining,data set repository,evolutionary algorithms,java},
number = {2-3},
pages = {255--287},
title = {{KEEL data-mining software tool: Data set repository, integration of algorithms and experimental analysis framework}},
url = {http://www.scopus.com/inward/record.url?eid=2-s2.0-79951829331{\&}partnerID=tZOtx3y1},
volume = {17},
year = {2011}
}
@article{Pawlak1998,
abstract = {This paper gives basic ideas of rough set theory a new approach to data analysis. The lower and uppe r approximation of a set, the basic ope rations of the theory, are intuitively explained and formally de fined. Some applica- tions of rough set theory are briefly outlined and some future problems are outline d.},
author = {Pawlak, Zdzis{\l}aw},
doi = {10.1080/019697298125470},
file = {:home/raul/Downloads/7.pdf:pdf},
isbn = {0196972981},
issn = {01969722},
journal = {Journal of Telecommunications and Information Technology},
keywords = {churn modeling,decision rules,rough set},
number = {7},
pages = {7--10},
title = {{Rough set theory and its applications}},
url = {http://www.informaworld.com/openurl?genre=article{\&}doi=10.1080/019697298125470{\&}magic=crossref},
volume = {29},
year = {1998}
}
@article{Masramon2015,
abstract = {Feature weighting algorithms try to solve a problem of great impor-tance nowadays in machine learning: The search of a relevance measure for the features of a given domain. This relevance is primarily used for feature selection as feature weighting can be seen as a generalization of it, but it is also useful to better understand a problem's domain or to guide an inductor in its learning process. Relief family of algorithms are proven to be very effective in this task. Some other feature weighting methods are reviewed in order to give some context and then the different existing extensions to the original algorithm are explained. One of Relief's known issues is the performance degradation of its estimates when redundant features are present. A novel theoretical def-inition of redundancy level is given in order to guide the work towards an extension of the algorithm that is more robust against redundancy. A new extension is presented that aims for improving the algorithms perfor-mance. Some experiments were driven to test this new extension against the existing ones with a set of artificial and real datasets and denoted that in certain cases it improves the weight's estimation accuracy.},
annote = {This paper presents an attemp to reduce redundant features problems of ReliefF},
archivePrefix = {arXiv},
arxivId = {arXiv:1509.03755v2},
author = {Masramon, Gabriel Prat and {Belanche Mu{\~{n}}oz}, Llu{\'{i}}s A},
eprint = {arXiv:1509.03755v2},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Toward better feature weighting algorithms a focus on Relief.pdf:pdf},
title = {{Toward better feature weighting algorithms: a focus on Relief}},
year = {2015}
}
@incollection{AngelRodriguez2015,
abstract = {Resumen. Los algoritmos gen{\{}{\'{e}}{\}}ticos distribuidos son una potente herramienta de aprendizaje de reglas de clasificaci{\{}{\'{o}}{\}}n que permite escalar eficientemente el tratamiento de grandes conjuntos de datos. Por otro lado el paradigma MapReduce permite tratar masivamente datos de manera distribuida de un modo simple. Este trabajo revisa brevemente las aportaciones realizadas y propone la implementaci{\{}{\'{o}}{\}}n de MapReduce en un algoritmo distribuido para explotar las posibles ventajas de ambos modelos en un enfoque mixto que permitir{\{}{\'{i}}{\}}a aprovechar las capacidades de aprendizaje distribuido del algoritmo gen{\{}{\'{e}}{\}}tico con la mejora de rendimiento que permite MapReduce. Palabras clave: Algoritmos Gen{\{}{\'{e}}{\}}ticos Distribuidos, MapReduce, Clasificaci{\{}{\'{o}}{\}}n, Programaci{\{}{\'{o}}{\}}n Distribuida, Programaci{\{}{\'{o}}{\}}n Paralela. 1 Introducci{\{}{\'{o}}{\}}n La revoluci{\{}{\'{o}}{\}}n digital ha posibilitado que la captura de datos sea f{\{}{\'{a}}{\}}cil y su almacenamiento tenga un coste muy bajo. Las herramientas tradicionales de gesti{\{}{\'{o}}{\}}n de datos junto con las t{\{}{\'{e}}{\}}cnicas estad{\{}{\'{i}}{\}}sticas no siempre son adecuadas para analizar ingentes vol{\{}{\'{u}}{\}}menes de datos. En el {\{}{\'{a}}{\}}mbito de la miner{\{}{\'{i}}{\}}a de datos, el aprendizaje de conceptos basado en inducci{\{}{\'{o}}{\}}n de ejemplos es una tarea de alta complejidad. {\{}{\'{E}}{\}}ste depende de la topolog{\{}{\'{i}}{\}}a de los datos a analizar y del volumen de {\{}{\'{e}}{\}}stos. A medida que el volumen de datos crece, el tiempo necesario para su tratamiento se incrementa de manera exponencial, adem{\{}{\'{a}}{\}}s de crecer la dificultad de aprendizaje del algoritmo. Existen diversas alternativas para aminorar el tiempo de proceso: una de ellas es la reducci{\{}{\'{o}}{\}}n de datos con el consiguiente riesgo de p{\{}{\'{e}}{\}}rdida de precisi{\{}{\'{o}}{\}}n del modelo; otra es la mejora de la escalabilidad del algoritmo para abarcar el volumen completo de datos, como es el caso de los algoritmos gen{\{}{\'{e}}{\}}ticos distribuidos para},
address = {Albacete, Spain},
author = {{{\'{A}}ngel Rodr{\'{i}}guez}, Miguel and Peregr{\'{i}}n, Antonio},
booktitle = {Actas de la XVI Conferencia de la Asociaci{\{}{\'{o}}{\}}n Espa{\{}{\~{n}}{\}}ola para la Inteligencia Artificial},
file = {:home/raul/Downloads/01021.pdf:pdf},
keywords = {algoritmos gen{\{}{\'{e}}{\}}ticos distribuidos,clasificaci{\{}{\'{o}}{\}}n,mapreduce,programaci{\{}{\'{o}}{\}}n distribuida,programaci{\{}{\'{o}}{\}}n paralela},
title = {{Big Data y Algoritmos Gen{\{}{\'{e}}{\}}ticos Distribuidos en el aprendizaje de reglas de clasificaci{\{}{\'{o}}{\}}n}},
year = {2015}
}
@article{Chen2010,
abstract = {Rough set theory is one of the effective methods to feature selection, which can preserve the meaning of the features. The essence of rough set approach to feature selection is to find a subset of the original features. Since finding a minimal subset of the features is a NP-hard problem, it is necessary to investigate effective and efficient heuristic algorithms. Ant colony optimization (ACO) has been successfully applied to many difficult combinatorial problems like quadratic assignment, traveling salesman, scheduling, etc. It is particularly attractive for feature selection since there is no heuristic information that can guide search to the optimal minimal subset every time. However, ants can discover the best feature combinations as they traverse the graph. In this paper, we propose a new rough set approach to feature selection based on ACO, which adopts mutual information based feature significance as heuristic information. A novel feature selection algorithm is also given. Jensen and Shen proposed a ACO-based feature selection approach which starts from a random feature. Our approach starts from the feature core, which changes the complete graph to a smaller one. To verify the efficiency of our algorithm, experiments are carried out on some standard UCI datasets. The results demonstrate that our algorithm can provide efficient solution to find a minimal subset of the features.},
annote = {{\textless}RANK{\textgreater} 2014 - Q2},
author = {Chen, Yumin and Miao, Duoqian and Wang, Ruizhi},
doi = {10.1016/j.patrec.2009.10.013},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2010 - A rough set approach to feature selection based on ant colony optimization.pdf:pdf},
issn = {01678655},
journal = {Pattern Recognition Letters},
keywords = {Ant colony optimization,Data mining,Feature selection,Mutual information,Rough sets},
month = {feb},
number = {3},
pages = {226--233},
title = {{A rough set approach to feature selection based on ant colony optimization}},
url = {http://www.sciencedirect.com/science/article/pii/S0167865509002888},
volume = {31},
year = {2010}
}
@article{Bergmeir2015,
abstract = {Fuzzy rule-based systems (FRBSs) are a well-known method family within soft computing. They are based on fuzzy concepts to address complex real-world problems. We present the R package frbs which implements the most widely used FRBS models, namely, Mamdani and Takagi Sugeno Kang (TSK) ones, as well as some common variants. In addition a host of learning methods for FRBSs, where the models are constructed from data, are implemented. In this way, accurate and interpretable systems can be built for data analysis and modeling tasks. In this paper, we also provide some examples on the usage of the package and a comparison with other common classi cation and regression methods available in R.},
author = {Bergmeir, Christoph and Ben, Manuel},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - frbs Fuzzy Rule-Based Systems for Classification.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {fuzzy,fuzzy inference systems,fuzzy sets,genetic fuzzy systems,soft computing},
number = {6},
pages = {1--30},
title = {{frbs : Fuzzy Rule-Based Systems for Classification}},
url = {https://www.google.com/url?sa=t{\&}rct=j{\&}q={\&}esrc=s{\&}source=web{\&}cd=3{\&}cad=rja{\&}uact=8{\&}ved=0CC4QFjACahUKEwj6w6CX3YjJAhUKo5QKHV5ZA7o{\&}url=http://www.jstatsoft.org/article/view/v065i06/v65i06.pdf{\&}usg=AFQjCNFS2WBfftNazZX1oNYydV5jsJVTPg},
volume = {65},
year = {2015}
}
@article{Huang2009,
abstract = {ReliefF has proved to be a successful feature selector but when handling a large dataset, it is computationally expensive. We present an optimization using Supervised Model Construction which improves starter selection. Effectiveness has been evaluated using 12 UCI datasets and a clinical diabetes database. Experiments indicate that compared with ReliefF, the proposed method improved computation efficiency whilst maintaining the classification accuracy. In the clinical dataset (20,000 records with 47 features), feature selection via Supervised Model Construction (FSSMC) reduced the processing time by 80{\%}, compared to ReliefF, and maintained accuracy for Naive Bayes, IB1 and C4.5 classifiers.},
author = {Huang, Yue and McCullagh, Paul J. and Black, Norman D.},
doi = {10.1016/j.datak.2009.07.011},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - An optimization of ReliefF for classification in large datasets.pdf:pdf},
issn = {0169023X},
journal = {Data {\&} Knowledge Engineering},
number = {11},
pages = {1348--1356},
title = {{An optimization of ReliefF for classification in large datasets}},
volume = {68},
year = {2009}
}
@article{Duivesteijn2011,
abstract = {Subgroup discovery suffers from the multiple comparisons problem: we search through a large space, hence whenever we report a set of discoveries, this set will generally contain false discoveries. We propose a method to compare subgroups found through subgroup discovery with a statistical model we build for these false discoveries. We determine how much the subgroups we find deviate from the model, and hence statistically validate the found subgroups. Furthermore we propose to use this subgroup validation to objectively compare quality measures used in subgroup discovery, by determining how much the top subgroups we find with each measure deviate from the statistical model generated with that measure. We thus aim to determine how good individual measures are in selecting significant findings. We invoke our method to experimentally compare popular quality measures in several subgroup discovery settings.},
author = {Duivesteijn, Wouter and Knobbe, Arno},
doi = {10.1109/ICDM.2011.65},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2011 - Exploiting false discoveries - Statistical validation of patterns and quality measures in subgroup discovery.pdf:pdf},
isbn = {9780769544083},
issn = {15504786},
journal = {Proceedings - IEEE International Conference on Data Mining, ICDM},
keywords = {Statistical validation,Subgroup discovery},
pages = {151--160},
title = {{Exploiting false discoveries - Statistical validation of patterns and quality measures in subgroup discovery}},
year = {2011}
}
@article{Beretta2011,
abstract = {BACKGROUND
The analysis of survival data allows to evaluate whether in a population the genetic exposure is related to the time until an event occurs. Owing to the complexity of common human diseases, there is the incipient need to develop bioinformatics tools to properly model non-linear high-order interactions in lifetime datasets. These tools, such as the survival dimensionality reduction algorithm, may suffer from extreme computational costs in large-scale datasets. Herein, we address the problem of estimating the quality of attributes, so as to extract relevant features from lifetime datasets and to scale down their size. 

METHODS
The ReliefF algorithm was modified and adjusted to compensate for the loss of information due to censoring, introducing reclassification and weighting schemes. Synthetic lifetime two-locus epistatic datasets of 500 attributes, 400–800 individuals and different degrees of cumulative heritability and censorship were generated. The capability of the survival ReliefF algorithm (sReliefF) and of a tuned sReliefF approach to properly select the causative pair of attributes was evaluated and compared to univariate selection based on Cox scores. 

RESULTS/CONCLUSIONS
sReliefF methods efficiently scaled down the simulated datasets, whilst univariate selection performed no better than random choice. These approaches may help to reduce the computational cost and to improve the classification task of algorithms that model high-order interactions in presence of right-censored data. Availability: http://sourceforge.net/projects/sdrproject/files/sReliefF/.},
author = {Beretta, Lorenzo and Santaniello, Alessandro},
doi = {10.1016/j.jbi.2010.12.003},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
number = {2},
pages = {361--369},
title = {{Implementing ReliefF filters to extract meaningful features from genetic lifetime datasets}},
volume = {44},
year = {2011}
}
@article{Keim2002,
abstract = {Never before in history has data been generated at such high volumes as it is today. Exploring and analyzing the vast volumes of data is becoming increasingly difficult. Information visualization and visual data mining can help to deal with the flood...},
author = {Keim, D a},
doi = {10.1109/2945.981847},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2002 - Information visualization and visual data mining.pdf:pdf},
isbn = {1077-2626},
issn = {10772626},
journal = {IEEE transactions on visualization and computer graphics},
number = {1},
pages = {1--8},
pmid = {20975137},
title = {{Information visualization and visual data mining}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=981847$\backslash$npapers3://publication/doi/10.1109/2945.981847},
volume = {8},
year = {2002}
}
@article{Watters2010,
author = {Watters, Aaron and {Van Rossum}, Guido and Ahlstrom, James C},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2010 - python Curses Programming with Python.pdf:pdf},
isbn = {15558514848},
journal = {Cheat Sheets},
pages = {1--8},
title = {{python Curses Programming with Python}},
year = {2010}
}
@article{Hall2000,
author = {Hall, Mark A.},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2000 - Correlation-based Feature Selection for Discrete and Numeric Class Machine Learning.pdf:pdf},
isbn = {1-55860-707-2},
month = {jun},
pages = {359--366},
publisher = {Morgan Kaufmann Publishers Inc.},
title = {{Correlation-based Feature Selection for Discrete and Numeric Class Machine Learning}},
url = {http://dl.acm.org/citation.cfm?id=645529.657793},
year = {2000}
}
@article{Verma2009,
abstract = {Genetic algorithms (GAs) are increasingly being applied to large scale problems. The traditional MPI-based parallel GAs require detailed knowledge about machine architecture. On the other hand, MapReduce is a powerful abstraction proposed by Google for making scalable and fault tolerant applications. In this paper, we show how genetic algorithms can be modeled into the MapReduce model. We describe the algorithm design and implementation of GAs on Hadoop, an open source implementation of MapReduce. Our experiments demonstrate the convergence and scalability up to 105 variable problems. Adding more resources would enable us to solve even larger problems without any changes in the algorithms and implementation since we do not introduce any performance bottlenecks.},
annote = {In this work the map phase is used to evaluate the population and the reduce phase evolves it.

A random partitioner is used so that the reducers receive random evaluated individuals, this helps in two things: keep local populations diverse, and have a random selection for a tournament. 
Reducers use global state to keep track of the number of procesed individuals and to store the selected individuals and, at the end, crossover them and produce the evolved population.

Obseve that a reduce function is applied for every},
author = {Verma, Abhishek and Llor{\`{a}}, Xavier and Goldberg, David E. and Campbell, Roy H.},
doi = {10.1109/ISDA.2009.181},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - Scaling genetic algorithms using MapReduce.pdf:pdf},
isbn = {9780769538723},
journal = {ISDA 2009 - 9th International Conference on Intelligent Systems Design and Applications},
keywords = {10-2 cites,Genetic algorithms,MapReduce,Scalability},
mendeley-tags = {10-2 cites},
pages = {13--18},
title = {{Scaling genetic algorithms using MapReduce}},
year = {2009}
}
@article{Reyes-Ortiz2015,
abstract = {One of the biggest challenges of the current big data landscape is our inability to process vast amounts of information in a reasonable time. In this work, we explore and compare two distributed computing frameworks implemented on commodity cluster architectures: MPI/OpenMP on Beowulf that is high-performance oriented and exploits multi-machine/multicore infrastructures, and Apache Spark on Hadoop which targets iterative algorithms through in-memory computing. We use the Google Cloud Platform service to create virtual machine clusters, run the frameworks, and evaluate two supervised machine learning algorithms: KNN and Pegasos SVM. Results obtained from experiments with a particle physics data set show MPI/OpenMP outperforms Spark by more than one order of magnitude in terms of processing speed and provides more consistent performance. However, Spark shows better data management infrastructure and the possibility of dealing with other aspects such as node failure and data replication.},
author = {Reyes-Ortiz, Jorge L. and Oneto, Luca and Anguita, Davide},
doi = {10.1016/j.procs.2015.07.286},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Big data analytics in the cloud Spark on Hadoop vs MPIOpenMP on Beowulf.pdf:pdf},
isbn = {18770509 (ISSN)},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {Beowulf,Big data,Cloud,Hadoop,MPI,OpenMP,Parallel computing,Spark,Supervised learning},
number = {1},
pages = {121--130},
publisher = {Elsevier Masson SAS},
title = {{Big data analytics in the cloud: Spark on Hadoop vs MPI/OpenMP on Beowulf}},
url = {http://dx.doi.org/10.1016/j.procs.2015.07.286},
volume = {53},
year = {2015}
}
@article{Jovic2014,
author = {Jovi{\'{c}}, a and Brki{\'{c}}, K and Bogunovi{\'{c}}, N},
doi = {10.1109/MIPRO.2014.6859735},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - An overview of free software tools for general data mining.pdf:pdf},
isbn = {9789532330816},
journal = {37th International Convention MIPRO {\ldots}},
number = {May},
pages = {26--30},
title = {{An overview of free software tools for general data mining}},
url = {http://www.zemris.fer.hr/{~}ajovic/articles/MIPRO 2014{\_}final.pdf},
year = {2014}
}
@article{Tahir2013,
abstract = {Semantic web services are gaining more attention as an important element of the emerging semantic web. Therefore, testing semantic web services is becoming a key concern as an essential quality assurance measure. The objective of this systematic literature review is to summarize the current state of the art of functional testing of semantic web services by providing answers to a set of research questions. The review follows a predefined procedure that involves automatically searching 5 well-known digital libraries. After applying the selection criteria to the results, a total of 34 studies were identified as relevant. Required information was extracted from the studies and summarized. Our systematic literature review identified some approaches available for deriving test cases from the specifications of semantic web services. However, many of the approaches are either not validated or the validation done lacks credibility. We believe that a substantial amount of work remains to be done to improve the current state of research in the area of testing semantic web services. ?? 2013 Elsevier Inc.},
author = {Tahir, Abbas and Tosi, Davide and Morasca, Sandro},
doi = {10.1016/j.jss.2013.06.064},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - A systematic review on the functional testing of semantic web services.pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Functional testing,Semantic web services,Systematic literature review,Testing approach},
number = {11},
pages = {2877--2889},
publisher = {Elsevier Inc.},
title = {{A systematic review on the functional testing of semantic web services}},
url = {http://dx.doi.org/10.1016/j.jss.2013.06.064},
volume = {86},
year = {2013}
}
@article{Rossum2014,
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - Sorting HOW TO.pdf:pdf},
pages = {1--6},
title = {{Sorting HOW TO}},
year = {2014}
}
@article{Kampa2014,
abstract = {Feature selection plays an important role in the successful application of machine learning techniques to large real-world datasets. Avoiding model overfitting, especially when the number of features far exceeds the number of observations, requires selecting informative features and/or eliminating irrelevant ones. Searching for an optimal subset of features can be computationally expensive. Functional magnetic resonance imaging (fMRI) produces datasets with such characteristics creating challenges for applying machine learning techniques to classify cognitive states based on fMRI data. In this study, we present an embedded feature selection framework that integrates sparse optimization for regularization (or sparse regularization) and classification. This optimization approach attempts to maximize training accuracy while simultaneously enforcing sparsity by penalizing the objective function for the coefficients of the features. This process allows many coefficients to become zero, which effectively eliminates their corresponding features from the classification model. To demonstrate the utility of the approach, we apply our framework to three different real-world fMRI datasets. The results show that regularized classifiers yield better classification accuracy, especially when the number of initial features is large. The results further show that sparse regularization is key to achieving scientifically-relevant generalizability and functional localization of classifier features. The approach is thus highly suited for analysis of fMRI data.},
author = {Kampa, K and Mehta, S and Chou, C A and Chaovalitwongse, W A and Grabowski, T J},
doi = {10.1007/s10898-013-0134-2},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - Sparse optimization in feature selection application in neuroimaging.pdf:pdf},
issn = {1573-2916},
journal = {Journal of Global Optimization},
number = {2},
pages = {439--457},
title = {{Sparse optimization in feature selection: application in neuroimaging}},
url = {http://dx.doi.org/10.1007/s10898-013-0134-2},
volume = {59},
year = {2014}
}
@article{Klsgen2002,
author = {Kl�sgen, W},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2002 - Subgroup Discovery.pdf:pdf},
isbn = {0-387-24435-2},
journal = {Handbook of Data Mining and Knowledge Discovery},
pages = {1--3},
title = {{Subgroup Discovery}},
year = {2002}
}
@inbook{Wu2012,
address = {Berlin, Heidelberg},
annote = {Even when the paper's title says that it proposes an adaptation of Relief for removing redundant features, it doesn not presents results comparing the number of redundant features selected by this algorithm and Relief.},
author = {Wu, Tianshu and Xie, Kunqing and Nie, Chengkai and Song, Guojie},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
chapter = {An Adaptio},
doi = {10.1007/978-3-642-31362-2_9},
editor = {Wang, Jun and Yen, Gary G and Polycarpou, Marios M},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - An Adaption of Relief.pdf:pdf},
isbn = {9783642313615},
issn = {03029743},
keywords = {Feature selection,Redudant features,Relief algorithm},
number = {PART 2},
pages = {73--81},
publisher = {Springer Berlin Heidelberg},
title = {{An adaption of relief for redundant feature elimination}},
url = {http://dx.doi.org/10.1007/978-3-642-31362-2{\_}9},
volume = {7368 LNCS},
year = {2012}
}
@article{Araujo2007,
abstract = {Abstract  Statistical natural language processing (NLP) and evolutionary algorithms (EAs) are two very active areas of research which have been combined many times. In general, statistical models applied to deal with NLP tasks require designing specific algorithms to be trained and applied to process new texts. The development of such algorithms may be hard. This makes EAs attractive since they offer a general design, yet providing a high performance in particular conditions of application. In this article, we present a survey of many works which apply EAs to different NLP problems, including syntactic and semantic analysis, grammar induction, summaries and text generation, document clustering and machine translation. This review finishes extracting conclusions about which are the best suited problems or particular aspects within those problems to be solved with an evolutionary algorithm.},
author = {Araujo, Lourdes},
doi = {10.1007/s10462-009-9104-y},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2007 - How evolutionary algorithms are applied to statistical natural language processing.pdf:pdf},
issn = {02692821},
journal = {Artificial Intelligence Review},
keywords = {Evolutionary algorithms,Statistical natural language processing},
number = {4},
pages = {275--303},
title = {{How evolutionary algorithms are applied to statistical natural language processing}},
volume = {28},
year = {2007}
}
@inproceedings{XiaoLiu2015,
annote = {TO-READ: Proposes a modification to handlde redundancy.},
author = {{Xiao Liu}, Xiao and {Xiaoli Wang}, Xiaoli and {Qiang Su}, Qiang},
booktitle = {2015 12th International Conference on Service Systems and Service Management (ICSSSM)},
doi = {10.1109/ICSSSM.2015.7170275},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Feature selection of medical data sets based on RS-RELIEFF.pdf:pdf},
isbn = {978-1-4799-8328-5},
keywords = {Accuracy,Algorithm design and analysis,Classification algorithms,Data analysis,Feature Selection,MATLAB,Medical Data Sets,RELIEFF,RS-RELIEFF,RS-RELIEFF feature selection algorithm,Temperature distribution,Temperature measurement,data analysis,feature reduction,feature selection,intelligent data analysis,mathematical approach,medical data sets,medical information systems,noise features,to-read},
mendeley-tags = {to-read},
month = {jun},
pages = {1--5},
publisher = {IEEE},
title = {{Feature selection of medical data sets based on RS-RELIEFF}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7170275},
year = {2015}
}
@article{Cendrowska1987,
abstract = {The decision tree output of Quinlan's ID3 algorithm is one of its major weaknesses. Not only can it be incomprehensible and difficult to manipulate, but its use in expert systems frequently demands irrelevant information to be supplied. This report argues that the problem lies in the induction algorithm itself and can only be remedied by radically altering the underlying strategy. It describes a new algorithm, PRISM which, although based on ID3, uses a different induction strategy to induce rules which are modular, thus avoiding many of the problems associated with decision trees.},
author = {Cendrowska, Jadzia},
doi = {10.1016/S0020-7373(87)80003-2},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1987 - PRISM An algorithm for inducing modular rules.pdf:pdf},
isbn = {0020-7373},
issn = {00207373},
journal = {International Journal of Man-Machine Studies},
number = {4},
pages = {349--370},
title = {{PRISM: An algorithm for inducing modular rules}},
volume = {27},
year = {1987}
}
@article{Alcala-Fdez2009,
abstract = {This paper introduces a software tool named KEEL which is a software tool to assess evolutionary algorithms for Data Mining problems of various kinds including as regression, classification, unsupervised learning, etc. It includes evolutionary learning algorithms based on different approaches: Pittsburgh, Michigan and IRL, as well as the integration of evolutionary learning techniques with different pre-processing techniques, allowing it to perform a complete analysis of any learning model in comparison to existing software tools. Moreover, KEEL has been designed with a double goal: research and educational.},
author = {Alcal{\'{a}}-Fdez, J. and S{\'{a}}nchez, L. and Garc{\'{i}}a, S. and del Jesus, M. J. and Ventura, S. and Garrell, J. M. and Otero, J. and Romero, C. and Bacardit, J. and Rivas, V. M. and Fern{\'{a}}ndez, J. C. and Herrera, F.},
doi = {10.1007/s00500-008-0323-y},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - KEEL a software tool to assess evolutionary algorithms for data mining problems.pdf:pdf},
issn = {1432-7643, 1433-7479},
journal = {Soft Computing},
keywords = {computer-based education,data mining,evolutionary computation,experimental design,graphical,java,knowledge extraction,machine,programming},
number = {3},
pages = {307--318},
title = {{KEEL: a software tool to assess evolutionary algorithms for data mining problems}},
url = {http://link.springer.com/article/10.1007/s00500-008-0323-y},
volume = {13},
year = {2009}
}
@phdthesis{Jensen2005a,
author = {Jensen, Richard},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2005 - Combining rough and fuzzy sets for feature selection.pdf:pdf},
title = {{Combining rough and fuzzy sets for feature selection}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.101.7352},
year = {2005}
}
@article{Greene2009,
author = {Greene, Casey S and Penrod, Nadia M and Kiralis, Jeff and Moore, Jason H},
doi = {10.1186/1756-0381-2-5},
issn = {1756-0381},
journal = {BioData Mining},
number = {1},
pages = {5},
publisher = {BioMed Central},
title = {{Spatially Uniform ReliefF (SURF) for computationally-efficient filtering of gene-gene interactions}},
url = {http://biodatamining.biomedcentral.com/articles/10.1186/1756-0381-2-5},
volume = {2},
year = {2009}
}
@article{Version2005,
author = {Version, S a S},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2005 - Ser uide.pdf:pdf},
isbn = {1877787191},
number = {November},
pages = {1--17},
title = {{Ser uide}},
year = {2005}
}
@book{Poli2008,
abstract = {A Field Guide to Genetic Programing is aimed at those new to genetic programming},
author = {Poli, Riccardo and Langdon, W.B. and McPhee, N.F.},
booktitle = {Wyvern},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2008 - A Field Guide to Genetic Programing.pdf:pdf},
isbn = {978-1-4092-0073-4},
keywords = {genetic algorithms,genetic programming},
number = {March},
pages = {8},
title = {{A Field Guide to Genetic Programing}},
url = {http://www.essex.ac.uk/wyvern/2008-04/Wyvern April 08 7126.pdf},
year = {2008}
}
@article{Bradley1997,
abstract = {In this paper we investigate the use of the area under the receiver operating characteristic (ROC) curve (AUC) as a performance measure for machine learning algorithms. As a case study we evaluate six machine learning algorithms (C4.5, Multiscale Classifier, Perceptron, Multi-layer Perceptron, k-Nearest Neighbours, and a Quadratic Discriminant Function) on six “real world” medical diagnostics data sets. We compare and discuss the use of AUC to the more conventional overall accuracy and find that AUC exhibits a number of desirable properties when compared to overall accuracy: increased sensitivity in Analysis of Variance (ANOVA) tests; a standard error that decreased as both AUC and the number of test samples increased; decision threshold independent; and it is invariant to a priori class probabilities. The paper concludes with the recommendation that AUC be used in preference to overall accuracy for “single number” evaluation of machine learning algorithms.},
author = {Bradley, Andrew P.},
doi = {10.1016/S0031-3203(96)00142-2},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1997 - The use of the area under the ROC curve in the evaluation of machine learning algorithms.pdf:pdf},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
number = {7},
pages = {1145--1159},
pmid = {17569110},
title = {{The use of the area under the ROC curve in the evaluation of machine learning algorithms}},
volume = {30},
year = {1997}
}
@article{Robnik2003,
author = {Robnik-{\v{S}}ikonja, Marko and Kononenko, Igor},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2003 - Theoretical and empirical analysis of ReliefF and RReliefF.pdf:pdf},
journal = {Machine learning},
number = {1-2},
pages = {23--69},
publisher = {Springer},
title = {{Theoretical and empirical analysis of ReliefF and RReliefF}},
volume = {53},
year = {2003}
}
@article{Carmona2014,
author = {Carmona, Crist??bal J. and Gonz??lez, Pedro and del Jesus, Mar??a Jos?? and Herrera, Francisco},
doi = {10.1002/widm.1118},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - Overview on evolutionary subgroup discovery Analysis of the suitability and potential of the search performed by evolutionary alg.pdf:pdf},
issn = {19424787},
journal = {Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery},
number = {2},
pages = {87--103},
title = {{Overview on evolutionary subgroup discovery: Analysis of the suitability and potential of the search performed by evolutionary algorithms}},
volume = {4},
year = {2014}
}
@article{Ziarko2008,
abstract = {The article introduces the basic ideas and investigates the probabilistic version of rough set theory. It relies on both classification knowledge and probabilistic knowledge in analysis of rules and attributes. Rough approximation evaluative measures and one-way and two-way inter-set dependency measures are proposed and adopted to probabilistic rule evaluation. A new probabilistic dependency measure for attributes is also introduced and proven to have the monotonicity property. This property makes it possible for the measure to be used to optimize and evaluate attribute-based representations through computation of probabilistic measures of attribute reduct, core and significance factors.},
author = {Ziarko, Wojciech},
doi = {10.1016/j.ijar.2007.06.014},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2008 - Probabilistic approach to rough sets.pdf:pdf},
issn = {0888613X},
journal = {International Journal of Approximate Reasoning},
keywords = {Data dependencies,Data mining,Data reduction,Machine learning,Probabilistic rough sets,Rough sets},
month = {oct},
number = {2},
pages = {272--284},
title = {{Probabilistic approach to rough sets}},
url = {http://www.sciencedirect.com/science/article/pii/S0888613X07001508},
volume = {49},
year = {2008}
}
@book{Garcia2015,
abstract = {In this chapter, we consider instance selection as an important focusing task in the data reduction phase of knowledge discovery and data mining. First of all, we define a broader perspective on concepts and topics related with instance selection (Sect. 8.1). Due to the fact that instance selection has been distinguished over the years as two type of tasks, depending on the data mining method applied later, we clearly separate it into two processes: training set selection and prototype selection. Theses trends are explained in Sect. 8.2. Thereafter, and focusing on prototype selection, we present a unifying framework that covers existing properties obtaining as a result a complete taxonomy (Sect. 8.3). The description of the operation as the most well known and some recent instance and/or prototype selection methods are provided in Sect. 8.4. Advanced and recent approaches that incorporate novel solutions based of hybridizations with other types of data reduction techniques or similar solutions are collected in Sect. 8.5. Finally, we summarize example evaluation results for prototype selection in an exhaustive experimental comparative analysis in Sect. 8.6. {\textcopyright} Springer International Publishing Switzerland 2015.},
author = {Garc{\'{i}}a, Salvador and Luengo, Juli{\'{a}}n and Herrera, Francisco},
booktitle = {Intelligent Systems Reference Library},
doi = {10.1007/978-3-319-10247-4},
isbn = {9783319102467},
issn = {18684408},
title = {{Data Preprocessing in Data Mining}},
volume = {72},
year = {2015}
}
@article{Kitchenham2009,
abstract = {Background: In 2004 the concept of evidence-based software engineering (EBSE) was introduced at the ICSE04 conference. Aims: This study assesses the impact of systematic literature reviews (SLRs) which are the recommended EBSE method for aggregating evidence. Method: We used the standard systematic literature review method employing a manual search of 10 journals and 4 conference proceedings. Results: Of 20 relevant studies, eight addressed research trends rather than technique evaluation. Seven SLRs addressed cost estimation. The quality of SLRs was fair with only three scoring less than 2 out of 4. Conclusions: Currently, the topic areas covered by SLRs are limited. European researchers, particularly those at the Simula Laboratory appear to be the leading exponents of systematic literature reviews. The series of cost estimation SLRs demonstrate the potential value of EBSE for synthesising evidence and making it available to practitioners. {\textcopyright} 2008 Elsevier B.V. All rights reserved.},
author = {Kitchenham, Barbara and {Pearl Brereton}, O. and Budgen, David and Turner, Mark and Bailey, John and Linkman, Stephen},
doi = {10.1016/j.infsof.2008.09.009},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - Systematic literature reviews in software engineering - A systematic literature review.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Cost estimation,Evidence-based software engineering,Systematic literature review,Systematic review quality,Tertiary study},
number = {1},
pages = {7--15},
publisher = {Elsevier B.V.},
title = {{Systematic literature reviews in software engineering - A systematic literature review}},
url = {http://dx.doi.org/10.1016/j.infsof.2008.09.009},
volume = {51},
year = {2009}
}
@article{Gao:2011:CSM:1967098.1967103,
address = {New York, NY, USA},
author = {Gao, Kehan and Khoshgoftaar, Taghi M and Wang, Huanjing and Seliya, Naeem},
doi = {10.1002/spe.1043},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - Achieving high and consistent rendering performance of java AWTSwing on multiple platforms.pdf:pdf},
issn = {0038-0644},
journal = {Softw. Pract. Exper.},
keywords = {attribute selection,defect prediction,feature ranking,feature subset selection,search-based software engineering,software metric,software quality},
month = {apr},
number = {5},
pages = {579--606},
publisher = {John Wiley {\&} Sons, Inc.},
title = {{Choosing Software Metrics for Defect Prediction: An Investigation on Feature Selection Techniques}},
url = {http://dx.doi.org/10.1002/spe.1043},
volume = {41},
year = {2011}
}
@article{Fallis2013e,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fallis, A.G},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - No Title No Title.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {Journal of Chemical Information and Modeling},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{No Title No Title}},
volume = {53},
year = {2013}
}
@inproceedings{Taleb2015,
author = {Taleb, Imene and Mammar, Madani Ould},
booktitle = {12th International Symposium on Programming and Systems, ISPS 2015},
doi = {10.1109/ISPS.2015.7244982},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Parking access system based on face recognition.pdf:pdf},
isbn = {9781479976997},
keywords = {Face detection,Linear Discriminant Analysis (LDA),Moving object,Principal Component Analysis (PCA),access control,face recognition},
month = {apr},
pages = {176--180},
publisher = {IEEE},
title = {{Parking access system based on face recognition}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7244982},
year = {2015}
}
@article{Ward2008,
author = {Ward, Greg},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2008 - Installing Python Modules.pdf:pdf},
journal = {Distribution},
pages = {1--14},
title = {{Installing Python Modules}},
year = {2008}
}
@article{Haughton2003,
abstract = {We present to the statistical community an overview of five data mining packages with the intent of leaving the reader with a sense of the different capabilities, the ease or difficulty of use, and the user interface of each package. We are not attempting to perform a controlled comparison of the algorithms in each package to decide which has the strongest predictive power, but instead hope to give an idea of the approach to predictive modeling used in each of them. The packages are compared in the areas of descriptive statistics and graphics, predictive mod- els, and association (market basket) analysis. As expected, the packages affiliated with the most popular statistical software packages (SAS and SPSS) provide the broad- est range of features with remarkably similar modeling and in- terface approaches, whereas the other packages all have their special sets of features and specific target audiences whom we believe each of the packages will serve well. It is essential that an organization considering the purchase of a data mining pack- age carefully evaluate the available options and choose the one that provides the best fit with its particular needs},
author = {Haughton, Dominique and Deichmann, Joel and Eshghi, Abdolreza and Sayek, Selin and Teebagy, Nicholas and Topi, Heikki},
doi = {10.1198/0003130032486},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2003 - A Review of Software Packages for Data Mining.pdf:pdf},
isbn = {00031305 (ISSN)},
issn = {0003-1305},
journal = {The American Statistician},
keywords = {abdolreza e shghi,and heikki t opi,dominique h aughton,eview of software packages,for data mining,joel d eichmann,nicholas t eebagy,rithms in each package,selin s ayek,the strongest pre-,to decide which has},
number = {4},
pages = {290--309},
title = {{A Review of Software Packages for Data Mining}},
url = {http://www.tandfonline.com/doi/abs/10.1198/0003130032486},
volume = {57},
year = {2003}
}
@article{Wen2011,
abstract = {An algorithm for license plate recognition (LPR) applied to the intelligent transportation system is proposed on the basis of a novel shadow removal technique and character recognition algorithms. This paper has two major contributions. One contribution is a new binary method, i.e., the shadow removal method, which is based on the improved Bernsen algorithm combined with the Gaussian filter. Our second contribution is a character recognition algorithm known as support vector machine (SVM) integration. In SVM integration, character features are extracted from the elastic mesh, and the entire address character string is taken as the object of study, as opposed to a single character. This paper also presents improved techniques for image tilt correction and image gray enhancement. Our algorithm is robust to the variance of illumination, view angle, position, size, and color of the license plates when working in a complex environment. The algorithm was tested with 9026 images, such as natural-scene vehicle images using different backgrounds and ambient illumination particularly for low-resolution images. The license plates were properly located and segmented as 97.16{\%} and 98.34{\%}, respectively. The optical character recognition system is the SVM integration with different character features, whose performance for numerals, Kana, and address recognition reached 99.5{\%}, 98.6{\%}, and 97.8{\%}, respectively. Combining the preceding tests, the overall performance of success for the license plate achieves 93.54{\%} when the system is used for LPR in various complex conditions.},
author = {Wen, Ying and Lu, Yue and Yan, Jingqi and Zhou, Zhenyu and {Von Deneen}, Karen M. and Shi, Pengfei},
doi = {10.1109/TITS.2011.2114346},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2011 - An algorithm for license plate recognition applied to intelligent transportation system.pdf:pdf},
isbn = {1524-9050 VO  - 12},
issn = {15249050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
keywords = {Bernsen algorithm,character recognition,feature extraction,license plate recognition (LPR),support vector machine (SVM)},
number = {3},
pages = {830--845},
title = {{An algorithm for license plate recognition applied to intelligent transportation system}},
volume = {12},
year = {2011}
}
@misc{Michalski1969,
author = {Michalski, Ryszard S.},
booktitle = {Proceedings of the V International Symposium on Information Processing (FCIP 69)},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1969 - On the Quasi-Minimal Solution of the General Covering Problem.pdf:pdf},
pages = {125--128},
title = {{On the Quasi-Minimal Solution of the General Covering Problem}},
year = {1969}
}
@incollection{Sun2014,
abstract = {Feature selection is an important research topic in machine learning and pattern recognition. It is effective in reducing dimensionality, removing irrelevant data, increasing learning accuracy, and improving result comprehensibility. However, in recent years, data has become increasingly larger in both number of instances and number of features in many applications. Classical feature selection method is out of work in processing large-scale dataset because of expensive computational cost. For improving computational speed, parallel feature selection is taken as the efficient method. MapReduce is an efficient distributional computing model to process large-scale data mining problems. In this paper, a parallel feature selection method based on MapReduce model is proposed. Large-scale dataset is partitioned into sub-datasets. Feature selection is operated on each computational node. Selected feature variables are combined into one feature vector in Reduce job. The parallel feature selection method is scalable. The efficiency of the method is illustrated through example analysis. {\textcopyright} 2014 Springer International Publishing Switzerland.},
address = {Cham},
annote = {Though the method is efficient in dealing with large-scale feature selection problem, it is only useful to binary feature and class variables.

AN iterative mutual reduction algorithm is proposed, no real comparisons are made.},
author = {Sun, Zhanquan},
booktitle = {Lecture Notes in Electrical Engineering},
chapter = {Parallel F},
doi = {10.1007/978-3-319-01766-2-35},
editor = {Wong, W Eric and Zhu, Tingshao},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - Parallel feature selection based on MapReduce.pdf:pdf},
isbn = {9783319017655},
issn = {18761119},
pages = {299--306},
publisher = {Springer International Publishing},
title = {{Parallel feature selection based on MapReduce}},
url = {http://dx.doi.org/10.1007/978-3-319-01766-2{\_}35},
volume = {277 LNEE},
year = {2014}
}
@book{Porzak2013,
author = {Porzak, Jim},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - Nina Zumel John Mount.pdf:pdf},
isbn = {9781617291562},
pages = {1--23},
title = {{Nina Zumel John Mount}},
url = {papers2://publication/uuid/C052A221-AC3A-420C-8C9D-6ACAF92F461A},
year = {2013}
}
@article{Wang2016,
author = {Wang, Yong and Ke, Wenlong and Tao, Xiaoling},
doi = {10.3390/info7010006},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2016 - A Feature Selection Method for Large-Scale Network Traffic Classification Based on Spark.pdf:pdf},
issn = {2078-2489},
journal = {Information},
keywords = {to-read},
mendeley-tags = {to-read},
month = {feb},
number = {1},
pages = {6},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{A Feature Selection Method for Large-Scale Network Traffic Classification Based on Spark}},
url = {http://www.mdpi.com/2078-2489/7/1/6},
volume = {7},
year = {2016}
}
@book{Nicolas2014,
author = {Nicolas, Patrick R.},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - 00. Scala for Machine Learning.pdf:pdf},
isbn = {9781783558742},
pages = {520},
title = {{00. Scala for Machine Learning}},
year = {2014}
}
@article{Fernandez2015,
abstract = {Evolutionary Fuzzy Systems are a successful hybridization between fuzzy systems and Evolutionary Algorithms. They integrate both the management of imprecision/uncertainty and inherent interpretability of Fuzzy Rule Based Systems, with the learning and adaptation capabilities of evolutionary optimization. Over the years, many different approaches in Evolutionary Fuzzy Systems have been developed for improving the behavior of fuzzy systems, either acting on the Fuzzy Rule Base Systems' elements, or by defining new approaches for the evolutionary components. All these efforts have enabled Evolutionary Fuzzy Systems to be successfully applied in several areas of Data Mining and engineering. In accordance with the former, a wide number of applications have been also taken advantage of these types of systems. However, with the new advances in computation, novel problems and challenges are raised every day. All these issues motivate researchers to make an effort in releasing new ways of addressing them with Evolutionary Fuzzy Systems. In this paper, we will review the progression of Evolutionary Fuzzy Systems by analyzing their taxonomy and components. We will also stress those problems and applications already tackled by this type of approach. We will present a discussion on the most recent and difficult Data Mining tasks to be addressed, and which are the latest trends in the development of Evolutionary Fuzzy Systems.},
author = {Fern{\'{a}}ndez, Alberto and L{\'{o}}pez, Victoria and {Del Jesus}, Mar{\'{i}}a Jos{\'{e}} and Herrera, Francisco},
doi = {10.1016/j.knosys.2015.01.013},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Revisiting Evolutionary Fuzzy Systems Taxonomy, applications, new trends and challenges.pdf:pdf},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Big Data,Data Mining,Evolutionary Fuzzy Systems,Fuzzy Rule Based Systems,Multi-Objective Evolutionary Fuzzy Systems,New trends,Scalability,Taxonomy},
pages = {109--121},
title = {{Revisiting Evolutionary Fuzzy Systems: Taxonomy, applications, new trends and challenges}},
volume = {80},
year = {2015}
}
@article{Connolly2012,
abstract = {This paper examines the literature on computer games and serious games in regard to the potential positive impacts of gaming on users aged 14 years or above, especially with respect to learning, skill enhancement and engagement. Search terms identified 129 papers reporting empirical evidence about the impacts and outcomes of computer games and serious games with respect to learning and engagement and a multidimensional approach to categorizing games was developed. The findings revealed that playing computer games is linked to a range of perceptual, cognitive, behavioural, affective and motivational impacts and outcomes. The most frequently occurring outcomes and impacts were knowledge acquisition/content understanding and affective and motivational outcomes. The range of indicators and measures used in the included papers are discussed, together with methodological limitations and recommendations for further work in this area.},
author = {Connolly, Thomas M and Boyle, Elizabeth A and Macarthur, Ewan and Hainey, Thomas and Boyle, James M},
doi = {10.1016/j.compedu.2012.03.004},
isbn = {0360-1315},
issn = {0360-1315},
journal = {Computers {\{}{\&}{\}} Education},
number = {2},
pages = {661--686},
publisher = {Elsevier Ltd},
title = {{Computers {\{}{\&}{\}} Education A systematic literature review of empirical evidence on computer games and serious games}},
url = {http://dx.doi.org/10.1016/j.compedu.2012.03.004},
volume = {59},
year = {2012}
}
@article{Rossum2012g,
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Porting Python 2 Code to Python 3.pdf:pdf},
journal = {Strategy},
pages = {3--8},
title = {{Porting Python 2 Code to Python 3}},
year = {2012}
}
@article{Borgelt2007,
abstract = {This paper describes a Java-based graphical user interface to a large number of data analysis programs the first author has written in C over the years. In addition, this toolbox is equipped with basic visualization capabilities, like scatter plots and bar charts, but also with specialized visualization modules for decision and regression trees as well as prototype-based classifiers. The architecture is like a toolbox: individual tools refer to the different data analysis methods. All parts of this toolbox (Java as well as C based) are free and open software under the Gnu Lesser (Library) Public License.},
author = {Borgelt, Christian and Rodr{\'{i}}guez, Gil Gonz{\'{a}}lez},
doi = {10.1109/FUZZY.2007.4295654},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2007 - FrIDA - A free intelligent data analysis toolbox.pdf:pdf},
isbn = {1424412102},
issn = {10987584},
journal = {IEEE International Conference on Fuzzy Systems},
pages = {11--15},
title = {{FrIDA - A free intelligent data analysis toolbox}},
volume = {5},
year = {2007}
}
@article{Rivals2003,
author = {Rivals, Isabelle and Personnaz, L{\'{e}}on},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2003 - MLPs (Mono-Layer Polynomials and Multi-Layer Perceptrons) for Nonlinear Modeling(2).pdf:pdf},
issn = {ISSN 1533-7928},
journal = {Journal of Machine Learning Research},
number = {Mar},
pages = {1383--1398},
title = {{MLPs (Mono-Layer Polynomials and Multi-Layer Perceptrons) for Nonlinear Modeling}},
url = {http://www.jmlr.org/papers/v3/rivals03a.html},
volume = {3},
year = {2003}
}
@book{Karau,
author = {Karau, Holden and Konwinski, Andy and Wendell, Patrick and Zaharia, Matei},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Holden Karau, Andy Konwinski, Patrick Wendell {\&} Matei Zaharia.pdf:pdf},
isbn = {9781449358624},
title = {{Holden Karau, Andy Konwinski, Patrick Wendell {\&} Matei Zaharia}}
}
@book{Fallis2013c,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fallis, A.G},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - No Title No Title(6).pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{No Title No Title}},
volume = {53},
year = {2013}
}
@article{Back1993,
abstract = {Evolutionary Programming and Evolution Strategies, rather similar representatives of a class of probabilistic optimization algorithms gleaned from the model of organic evolution, are discussed and compared to each other with respect to similarities and differences of their basic components as well as their performance in some experimental runs. Theoretical results on global convergence, step size control for a strictly convex, quadratic function and an extension of the convergence rate theory for Evolution Strategies are presented and discussed with respect to their implications on Evolutionary Programming.},
author = {B{\"{a}}ck, T and Rudolph, G and Schwefel, H.-P.},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1993 - Evolutionary programming and evolution strategies similarities and differences.pdf:pdf},
journal = {Proceedings of the 2{\^{}}{\{}nd{\}} International Conference on Evolutionary Programming},
pages = {11--22},
title = {{Evolutionary programming and evolution strategies: similarities and differences}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.42.3637},
year = {1993}
}
@article{Lemmerich2010,
author = {Lemmerich, Florian and Rohlfs, Mathias and Atzmueller, Martin},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2010 - Fast Discovery of Relevant Subgroup Patterns.pdf:pdf},
isbn = {9781577354475},
journal = {Proceedings of the 23rd Florida Artificial Intelligence Research Society Conference (FLAIRS)},
keywords = {Special Track: Data Mining},
number = {FLAIRS},
pages = {428--433},
title = {{Fast Discovery of Relevant Subgroup Patterns}},
url = {http://www.aaai.org/ocs/index.php/FLAIRS/2010/paper/viewPDFInterstitial/1262/1789},
year = {2010}
}
@article{Andor2016,
abstract = {We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. The key insight is based on a novel proof illustrating the label bias problem and showing that globally normalized models can be strictly more expressive than locally normalized models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1603.06042v1},
author = {Andor, Daniel and Alberti, Chris and Weiss, David and Severyn, Aliaksei and Presta, Alessandro and Ganchev, Kuzman and Petrov, Slav and Collins, Michael},
eprint = {arXiv:1603.06042v1},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2016 - Globally Normalized Transition-Based Neural Networks.pdf:pdf},
keywords = {()},
title = {{Globally Normalized Transition-Based Neural Networks}},
year = {2016}
}
@article{Aguilar-Ruiz2003,
abstract = {This paper describes an approach based on evolutionary algorithms, hierarchical decision rules (HIDER), for learning rules in continuous and discrete domains. The algorithm produces a hierarchical set of rules, that is, the rules are sequentially obtained and must therefore be tried until one is found whose conditions are satisfied. Thus, the number of rules may be reduced because the rules could be inside of one another. The evolutionary algorithm uses both real and binary coding for the individuals of the population. We tested our system on real data from the UCI repository, and the results of a ten-fold cross-validation are compared to C4.5s, C4.5Rules, See5s, and See5Rules. The experiments show that HIDER works well in practice.},
author = {Aguilar-Ruiz, J S and Riquelme, J C and Toro, M},
doi = {10.1109/TSMCB.2002.805696},
issn = {1083-4419},
journal = {IEEE transactions on systems, man, and cybernetics. Part B, Cybernetics : a publication of the IEEE Systems, Man, and Cybernetics Society},
keywords = {Classification tree analysis,Data structures,Databases,Decision trees,Evolutionary computation,HIDER,Nearest neighbor searches,Neural networks,Performance evaluation,Supervised learning,System testing,binary coding,database management systems,databases,decision trees,evolutionary algorithms,evolutionary learning,genetic algorithms,hierarchical decision rules,learning (artificial intelligence),learning rules,optimization,probability,supervised learning},
month = {jan},
number = {2},
pages = {324--31},
pmid = {18238181},
shorttitle = {Systems, Man, and Cybernetics, Part B: Cybernetics},
title = {{Evolutionary learning of hierarchical decision rules.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18238181},
volume = {33},
year = {2003}
}
@article{Mahdavi-Hezavehi2013,
abstract = {Context: Variability is the ability of a software artifact (e.g., a system, component) to be adapted for a specific context, in a preplanned manner. Variability not only affects functionality, but also quality attributes (e.g., security, performance). Service-based software systems consider variability in functionality implicitly by dynamic service composition. However, variability in quality attributes of service-based systems seems insufficiently addressed in current design practices. Objective: We aim at (a) assessing methods for handling variability in quality attributes of service-based systems, (b) collecting evidence about current research that suggests implications for practice, and (c) identifying open problems and areas for improvement. Method: A systematic literature review with an automated search was conducted. The review included studies published between the year 2000 and 2011. We identified 46 relevant studies. Results: Current methods focus on a few quality attributes, in particular performance and availability. Also, most methods use formal techniques. Furthermore, current studies do not provide enough evidence for practitioners to adopt proposed approaches. So far, variability in quality attributes has mainly been studied in laboratory settings rather than in industrial environments. Conclusions: The product line domain as the domain that traditionally deals with variability has only little impact on handling variability in quality attributes. The lack of tool support, the lack of practical research and evidence for the applicability of approaches to handle variability are obstacles for practitioners to adopt methods. Therefore, we suggest studies in industry (e.g., surveys) to collect data on how practitioners handle variability of quality attributes in service-based systems. For example, results of our study help formulate hypotheses and questions for such surveys. Based on needs in practice, new approaches can be proposed. ?? 2012 Elsevier B.V. All rights reserved.},
author = {Mahdavi-Hezavehi, Sara and Galster, Matthias and Avgeriou, Paris},
doi = {10.1016/j.infsof.2012.08.010},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded//2013 - Variability in quality attributes of service-based software systems A systematic literature review.pdf:pdf},
isbn = {09505849},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Quality attributes,Service-based systems,Systematic literature review,Variability},
number = {2},
pages = {320--343},
publisher = {Elsevier B.V.},
title = {{Variability in quality attributes of service-based software systems: A systematic literature review}},
url = {http://dx.doi.org/10.1016/j.infsof.2012.08.010},
volume = {55},
year = {2013}
}
@article{Bu2010,
abstract = {The growing demand for large-scale data mining and data anal- ysis applications has led both industry and academia to design new types of highly scalable data-intensive computing platforms. MapReduce and Dryad are two popular platforms in which the dataflow takes the form of a directed acyclic graph of operators. These platforms lack built-in support for iterative programs, which arise naturally in many applications including data mining, web ranking, graph analysis, model fitting, and so on. This paper presents HaLoop, a modified version of the Hadoop MapReduce framework that is designed to serve these applications. HaLoop not only extends MapReduce with programming support for it- erative applications, it also dramatically improves their efficiency by making the task scheduler loop-aware and by adding various caching mechanisms. We evaluated HaLoop on real queries and real datasets. Compared with Hadoop, on average, HaLoop reduces query runtimes by 1.85, and shuffles only 4{\%} of the data between mappers and reducers.},
archivePrefix = {arXiv},
arxivId = {arXiv:1209.2191v1},
author = {Bu, Yingyi and Howe, Bill and Ernst, Michael D},
doi = {10.14778/1920841.1920881},
eprint = {arXiv:1209.2191v1},
isbn = {0000000000000},
issn = {2150-8097},
journal = {Proceedings of the VLDB Endowment},
number = {1-2},
pages = {285--296},
title = {{HaLoop: Efficient Iterative Data Processing on Large Clusters}},
volume = {3},
year = {2010}
}
@article{Li2015,
abstract = {Subgroup discovery is a task at the intersection of predictive and descriptive induction, aiming at identifying subgroups that have the most unusual statistical (distributional) characteristics with respect to a property of interest. Although a great deal of work has been devoted to the topic, one remaining problem concerns the redundancy of subgroup descriptions, which often effectively convey very similar information. In this paper, we propose a quadratic programming based approach to reduce the amount of redundancy in the subgroup rules. Experimental results on 12 datasets show that the resulting subgroups are in fact less redundant compared to standard methods. In addition, our experiments show that the computational costs are significantly lower than the costs of other methods compared in the paper. {\textcopyright} 2013 Springer Science+Business Media New York.},
author = {Li, Rui and Perneczky, Robert and Drzezga, Alexander and Kramer, Stefan},
doi = {10.1007/s10844-013-0284-1},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Efficient redundancy reduced subgroup discovery via quadratic programming.pdf:pdf},
isbn = {9783642334917},
issn = {15737675},
journal = {Journal of Intelligent Information Systems},
keywords = {Mutual information,Quadratic programming,Redundancy,Rule learning,Subgroup discovery},
number = {2},
pages = {271--288},
title = {{Efficient redundancy reduced subgroup discovery via quadratic programming}},
volume = {44},
year = {2015}
}
@incollection{AngelRodriguez2015,
abstract = {Resumen. Los algoritmos gen{\'{e}}ticos distribuidos son una potente herramienta de aprendizaje de reglas de clasificaci{\'{o}}n que permite escalar eficientemente el tratamiento de grandes conjuntos de datos. Por otro lado el paradigma MapReduce permite tratar masivamente datos de manera distribuida de un modo simple. Este trabajo revisa brevemente las aportaciones realizadas y propone la implementaci{\'{o}}n de MapReduce en un algoritmo distribuido para explotar las posibles ventajas de ambos modelos en un enfoque mixto que permitir{\'{i}}a aprovechar las capacidades de aprendizaje distribuido del algoritmo gen{\'{e}}tico con la mejora de rendimiento que permite MapReduce. Palabras clave: Algoritmos Gen{\'{e}}ticos Distribuidos, MapReduce, Clasificaci{\'{o}}n, Programaci{\'{o}}n Distribuida, Programaci{\'{o}}n Paralela. 1 Introducci{\'{o}}n La revoluci{\'{o}}n digital ha posibilitado que la captura de datos sea f{\'{a}}cil y su almacenamiento tenga un coste muy bajo. Las herramientas tradicionales de gesti{\'{o}}n de datos junto con las t{\'{e}}cnicas estad{\'{i}}sticas no siempre son adecuadas para analizar ingentes vol{\'{u}}menes de datos. En el {\'{a}}mbito de la miner{\'{i}}a de datos, el aprendizaje de conceptos basado en inducci{\'{o}}n de ejemplos es una tarea de alta complejidad. {\'{E}}ste depende de la topolog{\'{i}}a de los datos a analizar y del volumen de {\'{e}}stos. A medida que el volumen de datos crece, el tiempo necesario para su tratamiento se incrementa de manera exponencial, adem{\'{a}}s de crecer la dificultad de aprendizaje del algoritmo. Existen diversas alternativas para aminorar el tiempo de proceso: una de ellas es la reducci{\'{o}}n de datos con el consiguiente riesgo de p{\'{e}}rdida de precisi{\'{o}}n del modelo; otra es la mejora de la escalabilidad del algoritmo para abarcar el volumen completo de datos, como es el caso de los algoritmos gen{\'{e}}ticos distribuidos para},
address = {Albacete, Spain},
author = {{{\'{A}}ngel Rodr{\'{i}}guez}, Miguel and Peregr{\'{i}}n, Antonio},
booktitle = {Actas de la XVI Conferencia de la Asociaci{\'{o}}n Espa{\~{n}}ola para la Inteligencia Artificial},
file = {:home/raul/Downloads/01021.pdf:pdf},
keywords = {algoritmos gen{\'{e}}ticos distribuidos,clasificaci{\'{o}}n,mapreduce,programaci{\'{o}}n distribuida,programaci{\'{o}}n paralela},
title = {{Big Data y Algoritmos Gen{\'{e}}ticos Distribuidos en el aprendizaje de reglas de clasificaci{\'{o}}n}},
year = {2015}
}
@article{Lavrac2005,
author = {Lavra{\v{c}}, N.},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2005 - Subgroup discovery techniques and applications.pdf:pdf},
isbn = {3-540-26076-5},
issn = {03029743},
journal = {Advances in Knowledge Discovery and Data Mining},
pages = {13--20},
title = {{Subgroup discovery techniques and applications}},
url = {http://www.springerlink.com/index/N9AQMYRYP9UTMRVW.pdf},
year = {2005}
}
@article{Srikant1996,
abstract = {The problem of mining sequential patterns was recently in-troduced in We are given a database of sequences, where each se-quence is a list of transactions ordered by transaction-time, and each transaction is a set of items. The problem is to discover all sequential patterns with a user-speciied minimum support, where the support of a pattern is the number of data-sequences that contain the pattern. An example of a sequential pattern is of customers bough F oundation' anRingworld' in one transaction, followed bSecond Foundation' in a later transaction". We generalize the problem as follows. First, we add time constraints that specify a minimum anddor maximum time period between adjacent elements in a pattern. Second, we relax the restric-tion that the items in an element of a sequential pattern must come from the same transaction, instead allowing the items to be present i n a set of transactions whose transaction-times are within a user-speciied time window. Third, given a user-deened taxonomy is-a hierarchy on items, we allow sequential patterns to include items across all levels of the taxonomy. We present GSP, a new algorithm that discovers these generalized se-quential patterns. Empirical evaluation using synthetic and real-life data indicates that GSP is much faster than the AprioriAll algorithm pre-sented in GSP scales linearly with the number of data-sequences, and has very good scale-up properties with respect to the average data-sequence size.},
author = {Srikant, R and Agrawal, E},
doi = {10.1109/ICDE.1995.380415},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1996 - Mining Sequential Patterns Generalization and Performance Improvements.pdf:pdf},
isbn = {978-3-540-61057-1},
issn = {978-3-540-61057-1},
journal = {5th International Conference on Extending Database Technology (EDBT '96)},
pages = {3--17},
title = {{Mining Sequential Patterns: Generalization and Performance Improvements}},
year = {1996}
}
@article{Zhao2003,
author = {Zhao, Qiankun},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2003 - Association Rule Mining A Survey.pdf:pdf},
journal = {Survey Paper},
number = {2003116},
pages = {1--20},
title = {{Association Rule Mining : A Survey}},
year = {2003}
}
@article{Bertsekas2008,
author = {Bertsekas, Dimitri P and Tsitsiklis, John N},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2008 - Posterior P ( $\Theta$ = 1 X = k ) Posterior P ( $\Theta$ = 2 X = k ) Number of heads k Choose $\Theta$ = 1 Choose $\Theta$ = 2.pdf:pdf},
pages = {10--12},
title = {{Posterior P ( $\Theta$ = 1 | X = k ) Posterior P ( $\Theta$ = 2 | X = k ) Number of heads k Choose $\Theta$ = 1 Choose $\Theta$ = 2}},
volume = {2008},
year = {2008}
}
@article{Connolly2012,
abstract = {This paper examines the literature on computer games and serious games in regard to the potential positive impacts of gaming on users aged 14 years or above, especially with respect to learning, skill enhancement and engagement. Search terms identified 129 papers reporting empirical evidence about the impacts and outcomes of computer games and serious games with respect to learning and engagement and a multidimensional approach to categorizing games was developed. The findings revealed that playing computer games is linked to a range of perceptual, cognitive, behavioural, affective and motivational impacts and outcomes. The most frequently occurring outcomes and impacts were knowledge acquisition/content understanding and affective and motivational outcomes. The range of indicators and measures used in the included papers are discussed, together with methodological limitations and recommendations for further work in this area.},
author = {Connolly, Thomas M and Boyle, Elizabeth A and Macarthur, Ewan and Hainey, Thomas and Boyle, James M},
doi = {10.1016/j.compedu.2012.03.004},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Computers {\&} Education A systematic literature review of empirical evidence on computer games and serious games.pdf:pdf},
isbn = {0360-1315},
issn = {0360-1315},
journal = {Computers {\&} Education},
number = {2},
pages = {661--686},
publisher = {Elsevier Ltd},
title = {{Computers {\&} Education A systematic literature review of empirical evidence on computer games and serious games}},
url = {http://dx.doi.org/10.1016/j.compedu.2012.03.004},
volume = {59},
year = {2012}
}
@article{Anagnostopoulos2006,
abstract = {In this paper, a new algorithm for vehicle license plate identification is proposed, on the basis of a novel adaptive image segmentation technique (sliding concentric windows) and connected component analysis in conjunction with a character recognition neural network. The algorithm was tested with 1334 natural-scene gray-level vehicle images of different backgrounds and ambient illumination. The camera focused in the plate, while the angle of view and the distance from the vehicle varied according to the experimental setup. The license plates properly segmented were 1287 over 1334 input images (96.5{\%}). The optical character recognition system is a two-layer probabilistic neural network (PNN) with topology 108-180-36, whose performance for entire plate recognition reached 89.1{\%}. The PNN is trained to identify alphanumeric characters from car license plates based on data obtained from algorithmic image processing. Combining the above two rates, the overall rate of success for the license-plate-recognition algorithm is 86.0{\%}. A review in the related literature presented in this paper reveals that better performance (90{\%} up to 95{\%}) has been reported, when limitations in distance, angle of view, illumination conditions are set, and background complexity is low},
author = {Anagnostopoulos, Christos Nikolaos E and Anagnostopoulos, Ioannis E. and Loumos, Vassili and Kayafas, Eleftherios},
doi = {10.1109/TITS.2006.880641},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2006 - A license plate-recognition algorithm for intelligent transportation system applications.pdf:pdf},
issn = {15249050},
journal = {IEEE Transactions on Intelligent Transportation Systems},
keywords = {Image processing,License plate recognition (LPR),Optical character recognition (OCR),Probabilistic neural network (PNN)},
number = {3},
pages = {377--391},
title = {{A license plate-recognition algorithm for intelligent transportation system applications}},
volume = {7},
year = {2006}
}
@article{Ipps2009,
author = {Ipps, Jere H L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - Mikhail A. F.pdf:pdf},
keywords = {- les sites vendiens,arkhangelsk en russie,de la mer blanche,de la r{\{}{\{}{\'{e}}{\}}{\}}gion de,destruction,ediacaran,fossiles vendiens,la mer blanche -,les pr{\{}{\{}{\'{e}}{\}}{\}}l{\{}{\{}{\`{e}}{\}}{\}}vements sauvages,paleo-piracy,paleontological resources,r{\{}{\{}{\'{e}}{\}}{\}}sum{\{}{\{}{\'{e}}{\}}{\}},selling fossils,une menace pour les,vendian,{\{}{\{}{\'{e}}{\}}{\}}diacariens},
pages = {1--36},
title = {{Mikhail A. F}},
volume = {03},
year = {2009}
}
@article{Su2015,
author = {Su, Boyang and Shao, Jie and Zhou, Jianying and Zhang, Xiaoteng and Mei, Lin},
file = {:home/raul/Downloads/146-29250.pdf:pdf},
keywords = {deep convolutional nerual networks,network in,vehicle color recognition},
number = {Jimet},
pages = {790--793},
title = {{Vehicle Color Recognition in The Surveillance with Deep Convolutional Neural Networks}},
year = {2015}
}
@article{Mcmillan2003,
author = {Mcmillan, Gordon},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2003 - Socket Programming HOWTO.pdf:pdf},
journal = {History},
keywords = {C++ programming},
title = {{Socket Programming HOWTO}},
year = {2003}
}
@article{King1998,
abstract = {Fourteen desktop data mining tools (or tool modules) ranging in$\backslash$nprice from US{\$}75 to {\$}25,000 (median {\&}lt;{\$}1,000) were evaluated by four\backslashnundergraduates inexperienced at data mining, a relatively experienced\backslashngraduate student, and a professional data mining consultant. The tools\backslashnran under the Microsoft Windows 95, Microsoft Windows NT, or Macintosh\backslashnSystem 7.5 operating systems, and employed decision trees, rule\backslashninduction, neural networks, or polynomial networks to solve two binary\backslashnclassification problems, a multi-class classification problem, and a\backslashnnoiseless estimation problem. Twenty evaluation criteria and a\backslashnstandardized procedure for assessing tool qualities were developed and\backslashnapplied. The traits were collected in five categories: capability,\backslashnlearnability/usability, interoperability, flexibility, and accuracy.\backslashnPerformance in each of these categories was rated on a six-point ordinal\backslashnscale, to summarize their relative strengths and weaknesses. This paper\backslashnsummarizes a lengthy technical report (Gomolka et al., 1998), which\backslashndetails the evaluation procedure and the scoring of all component\backslashncriteria. This information should be useful to analysts selecting data\backslashnmining tools to employ, as well as to developers aiming to produce\backslashnbetter data mining products},
author = {King, M.a. and Elder, J.F., Iv and Gomolka, B. and Schmidt, E. and Summers, M. and Toop, K.},
doi = {10.1109/ICSMC.1998.725108},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1998 - Evaluation of fourteen desktop data mining tools.pdf:pdf},
isbn = {0-7803-4778-1},
issn = {1062-922X},
journal = {SMC'98 Conference Proceedings. 1998 IEEE International Conference on Systems, Man, and Cybernetics (Cat. No.98CH36218)},
pages = {2927--2932},
title = {{Evaluation of fourteen desktop data mining tools}},
volume = {3},
year = {1998}
}
@article{Ar,
author = {Ar, Py},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Traducido y empaquetado por la comunidad de Python Argentina.pdf:pdf},
title = {{Traducido y empaquetado por la comunidad de Python Argentina}},
url = {http://www.python.org.ar}
}
@article{Daly2004,
author = {Daly, Olena and Taniar, David},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2004 - Exception Rules Mining Based on Negative Association Rules.pdf:pdf},
isbn = {3-540-22060-7},
journal = {12th International Conference on Computational Science and Applications},
pages = {543--552},
title = {{Exception Rules Mining Based on Negative Association Rules}},
volume = {3046},
year = {2004}
}
@book{RogeriodosSantosAlves;AlexSoaresdeSouza2014,
abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {et all {Rog{\'{e}}rio dos Santos Alves; Alex Soares de Souza}},
booktitle = {Igarss 2014},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - No Title No Title.pdf:pdf},
isbn = {9780874216561},
issn = {13514180},
keywords = {high resolution images,research,risks management,sustainable reconstruction},
number = {1},
pages = {1--5},
pmid = {15991970},
title = {{No Title No Title}},
year = {2014}
}
@incollection{Bolon-Canedo2013,
author = {Bol{\'{o}}n-Canedo, Ver{\'{o}}nica and S{\'{a}}nchez-Maro{\~{n}}o, Noelia and Cervi{\~{n}}o-Rabu{\~{n}}al, Joana},
doi = {10.1007/978-3-642-40643-0_13},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - Scaling Up Feature Selection A Distributed Filter Approach.pdf:pdf},
keywords = {to-read},
mendeley-tags = {to-read},
pages = {121--130},
publisher = {Springer Berlin Heidelberg},
title = {{Scaling Up Feature Selection: A Distributed Filter Approach}},
url = {http://link.springer.com/10.1007/978-3-642-40643-0{\_}13},
year = {2013}
}
@book{Blitzstein2015,
abstract = {Assuming one semester of calculus, this textbook introduces probability to undergraduate students who want to learn statistics. It clearly explains the importance of widely used distributions in statistics, such as normal, binomial, and Poisson, and explores how they are all connected. The book makes the distributions easier to remember, understand, and work with by illustrating natural applications where they arise, including applications of MCMC. R is used to perform statistical calculations.},
author = {Blitzstein, Joseph K. and Hwang, Jessica},
doi = {http://dx.doi.org/10.1016/S1363-4127(97)81322-2},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Introduction to Probability.pdf:pdf},
isbn = {9781466575592 146657559X 9781498704786 1498704786 1466575573 9781466575578},
issn = {0743-6661},
keywords = {Probability,Probability theory},
pages = {589},
pmid = {10148467},
title = {{Introduction to Probability}},
year = {2015}
}
@article{Rodriguez2013,
abstract = {Context Although many papers have been published on software defect prediction techniques, machine learning approaches have yet to be fully explored. Objective In this paper we suggest using a descriptive approach for defect prediction rather than the precise classification techniques that are usually adopted. This allows us to characterise defective modules with simple rules that can easily be applied by practitioners and deliver a practical (or engineering) approach rather than a highly accurate result. Method We describe two well-known subgroup discovery algorithms, the SD algorithm and the CN2-SD algorithm to obtain rules that identify defect prone modules. The empirical work is performed with publicly available datasets from the Promise repository and object-oriented metrics from an Eclipse repository related to defect prediction. Subgroup discovery algorithms mitigate against characteristics of datasets that hinder the applicability of classification algorithms and so remove the need for preprocessing techniques. Results The results show that the generated rules can be used to guide testing effort in order to improve the quality of software development projects. Such rules can indicate metrics, their threshold values and relationships between metrics of defective modules. Conclusions The induced rules are simple to use and easy to understand as they provide a description rather than a complete classification of the whole dataset. Thus this paper represents an engineering approach to defect prediction, i.e., an approach which is useful in practice, easily understandable and can be applied by practitioners. ?? 2013 Elsevier B.V. All rights reserved.},
author = {Rodriguez, Daniel and Ruiz, Roberto and Riquelme, Jose C. and Harrison, Rachel},
doi = {10.1016/j.infsof.2013.05.002},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - A study of subgroup discovery approaches for defect prediction.pdf:pdf},
isbn = {0950-5849},
issn = {09505849},
journal = {Information and Software Technology},
keywords = {Defect prediction,Imbalanced datasets,Rules,Subgroup discovery},
number = {10},
pages = {1810--1822},
title = {{A study of subgroup discovery approaches for defect prediction}},
volume = {55},
year = {2013}
}
@article{Huttunen2016,
abstract = {In this paper we study automatic recognition of cars of four types: Bus, Truck, Van and Small car. For this problem we consider two data driven frameworks: a deep neural network and a support vector machine using SIFT features. The accuracy of the methods is validated with a database of over 6500 images, and the resulting prediction accuracy is over 97 {\%}. This clearly exceeds the accuracies of earlier studies that use manually engineered feature extraction pipelines.},
archivePrefix = {arXiv},
arxivId = {1602.07125},
author = {Huttunen, Heikki and Yancheshmeh, Fatemeh Shokrollahi and Chen, Ke},
eprint = {1602.07125},
file = {:home/raul/1602.07125.pdf:pdf},
isbn = {9781509018208},
journal = {arXiv preprint arXiv:1602.07125},
title = {{Car Type Recognition with Deep Neural Networks}},
url = {http://arxiv.org/abs/1602.07125},
year = {2016}
}
@article{Konijn2013,
author = {Konijn, Rob M. and Duivesteijn, Wouter and Kowalczyk, Wojtek and Knobbe, Arno},
doi = {10.1007/978-3-642-37453-1_1},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - Discovering local subgroups, with an application to fraud detection.pdf:pdf},
isbn = {9783642374524},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
number = {PART 1},
pages = {1--12},
title = {{Discovering local subgroups, with an application to fraud detection}},
volume = {7818 LNAI},
year = {2013}
}
@article{Berlanga2006,
author = {Berlanga, Francisco and {Del Jesus}, Mar{\'{i}}a Jos{\'{e}} and Gonz{\'{a}}lez, Pedro and Herrera, Francisco and Mesonero, Mikel},
doi = {10.1007/11790853_27},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2006 - Multiobjective evolutionary induction of subgroup discovery fuzzy rules A case study in marketing.pdf:pdf},
isbn = {978-3-540-36036-0},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Data mining,Descriptive induction,Genetic fuzzy systems,Multiobjective evolutionary algorithms,Subgroup discovery},
pages = {337--349},
title = {{Multiobjective evolutionary induction of subgroup discovery fuzzy rules: A case study in marketing}},
volume = {4065 LNAI},
year = {2006}
}
@article{Agrawal1994,
abstract = {We consider the problem of discovering association rules between items in a large database of sales transactions. We present two new algorithms for solving this problem that are fundamentally di erent from the known algo- rithms. Empirical evaluation shows that these algorithms outperform the known algorithms by factors ranging from three for small problems to more than an order of mag- nitude for large problems. We also show how the best features of the two proposed algorithms can be combined into a hybrid algorithm, called AprioriHybrid. Scale-up experiments show that AprioriHybrid scales linearly with the number of transactions. AprioriHybrid also has ex- cellent scale-up properties with respect to the transaction size and the number of items in the database.},
author = {Agrawal, Rakesh and Srikant, Ramakrishnan},
doi = {10.1.1.40.6757},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1994 - Fast algorithms for mining association rules.pdf:pdf},
isbn = {1-55860-153-8},
issn = {15426270},
journal = {Proceeding VLDB '94 Proceedings of the 20th International Conference on Very Large Data Bases},
pages = {487--499},
pmid = {18094348},
title = {{Fast algorithms for mining association rules}},
url = {https://www.it.uu.se/edu/course/homepage/infoutv/ht08/vldb94{\_}rj.pdf},
volume = {1215},
year = {1994}
}
@misc{Lichman2013,
author = {Lichman, M},
institution = {University of California, Irvine, School of Information and Computer Sciences},
title = {{UCI Machine Learning Repository}},
url = {http://archive.ics.uci.edu/ml},
year = {2013}
}
@article{Zhang2010,
abstract = {Emerald publication, then please use our Emerald for Authors service. Information about how to choose which publication to write for and submission guidelines are available for all. Please visit www.emeraldinsight.com/authors for more information. About Emerald www.emeraldinsight.com With over forty years' experience, Emerald Group Publishing is a leading independent publisher of global research with impact in business, society, public policy and education. In total, Emerald publishes over 275 journals and more than 130 book series, as well as an extensive range of online products and services. Emerald is both COUNTER 3 and TRANSFER compliant. The organization is a partner of the Committee on Publication Ethics (COPE) and also works with Portico and the LOCKSS initiative for digital archive preservation. Abstract Purpose – The purpose of this paper is to review and compare selected software for data mining, text mining (TM), and web mining that are not available as free open-source software. Design/methodology/approach – Selected softwares are compared with their common and unique features. The software for data mining are SAS w Enterprise Minere, Megaputer PolyAnalyst w 5.0, NeuralWare Predict},
author = {Zhang, Q. and Segall, R S},
doi = {10.1108/03684921011036835},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2010 - Review of data, text and web mining software.pdf:pdf},
issn = {0368-492X},
journal = {Kybernetes},
keywords = {computer software,cybernetics,data collection,database management systems,internet,paper type research paper},
number = {4},
pages = {625--655},
title = {{Review of data, text and web mining software}},
url = {http://dx.doi.org/10.1108/03684921011036835},
volume = {39},
year = {2010}
}
@article{Queiros2015,
abstract = {Ambient-assisted living (AAL) is, nowadays, an important research and development area, foreseen as an important instrument to face the demographic aging. The acceptance of the AAL paradigm is closely related to the quality of the available systems, namely in terms of intel-ligent functions for the user interaction. In that context, usability and accessibility are crucial issues to consider. This paper presents a systematic literature review of AAL technologies, products and services with the objective of establishing the current position regarding user interaction and how are end users involved in the AAL development and evaluation processes. For this purpose, a systematic review of the literature on AAL was undertaken. A total of 1,048 articles were analyzed, 111 of which were mainly related to user interaction and 132 of which described practical AAL systems applied in a specified context and with a well-defined aim. Those articles classified as user interaction and systems were further characterized in terms of objectives, target users, users' involvement, usability and accessibility issues, settings to be applied, technologies used and development stages. The results show the need to improve the integration and interoperability of the existing technologies and to promote user-centric developments with a strong involvement of end users, namely in what concerns usability and accessibility issues.},
author = {Queir{\'{o}}s, Alexandra and Silva, Anabela and Alvarelh{\~{a}}o, Joaquim and Rocha, Nelson Pacheco and Teixeira, Ant{\'{o}}nio},
doi = {10.1007/s10209-013-0328-x},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Usability, accessibility and ambient-assisted living a systematic literature review.pdf:pdf},
isbn = {1615-5289},
issn = {16155297},
journal = {Universal Access in the Information Society},
keywords = {Accessibility,Ambient-assisted living,Systematic literature review,Usability,User interaction},
number = {1},
pages = {57--66},
title = {{Usability, accessibility and ambient-assisted living: a systematic literature review}},
volume = {14},
year = {2015}
}
@book{Dekking2005,
author = {Dekking, F.M. and Kraaikamp, C. and Lopuhaa, H.P. and Meester, L.E.},
booktitle = {Technometrics},
doi = {10.1198/tech.2007.s502},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2005 - A Modern Introduction to Probability and Statistics.pdf:pdf},
isbn = {9781852338961},
pages = {399--410},
title = {{A Modern Introduction to Probability and Statistics}},
url = {http://amstat.tandfonline.com/doi/pdf/10.1198/tech.2007.s502},
year = {2005}
}
@article{Hall2009,
abstract = {More than twelve years have elapsed since the first public release of WEKA. In that time, the software has been rewritten entirely from scratch, evolved substantially and now accompanies a text on data mining [35]. These days, WEKA enjoys widespread acceptance in both academia and business, has an active community, and has been downloaded more than 1.4 million times since being placed on Source- Forge in April 2000. This paper provides an introduction to the WEKA workbench, reviews the history of the project, and, in light of the recent 3.6 stable release, briefly discusses what has been added since the last stable version (Weka 3.4) released in 2003.},
author = {Hall, Mark and Frank, Eibe and Holmes, Geoffrey and Pfahringer, Bernhard and Reutemann, Peter and Witten, Ian H},
doi = {10.1145/1656274.1656278},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - The WEKA data mining software.pdf:pdf},
isbn = {1931-0145},
issn = {19310145},
journal = {ACM SIGKDD Explorations Newsletter},
number = {1},
pages = {10},
title = {{The WEKA data mining software}},
url = {http://portal.acm.org/citation.cfm?doid=1656274.1656278},
volume = {11},
year = {2009}
}
@article{Knobbe2012,
abstract = {In most databases, it is possible to identify small partitions of the data where the observed distribution is notably different from that of the database as a whole. In classical subgroup discovery, one considers the distribution of a single nominal attribute, and exceptional subgroups show a surprising increase in the occurrence of one of its values. In this paper, we introduce Exceptional Model Mining (EMM), a framework that allows for more complicated target concepts. Rather than finding subgroups based on the distribution of a single target attribute, EMM finds subgroups where a model fitted to that subgroup is somehow excep- tional. We discuss regression as well as classification models, and define quality measures that determine how exceptional a given model on a subgroup is. Our framework is general enough to be applied to many types of models, even from other paradigms such as association analysis and graphical modeling.},
author = {Knobbe, Arno and Feelders, Ad and Leman, Dennis},
doi = {10.1007/978-3-642-23242-8_9},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Exceptional Model Mining.pdf:pdf},
isbn = {9783642232404},
issn = {18684394},
journal = {Intelligent Systems Reference Library},
pages = {183--198},
title = {{Exceptional Model Mining}},
volume = {24},
year = {2012}
}
@article{Rossum2012,
author = {Rossum, Guido Van and Drake, Fred L},
doi = {10.1093/aje/kwq356},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Unicode HOWTO.pdf:pdf},
issn = {14766256},
journal = {History},
pages = {1--12},
pmid = {20870797},
title = {{Unicode HOWTO}},
year = {2012}
}
@article{Clark1989,
author = {Clark, P. and Niblett, T.},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1989 - The {\{}CN{\}}2 rule induction algorithm.pdf:pdf},
journal = {Machine Learning},
keywords = {cn2,comprehensibility,concept learning,noise,rule induction},
number = {4},
pages = {261--284},
title = {{The {\{}CN{\}}2 rule induction algorithm}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.51.3672{\&}rep=rep1{\&}type=pdf},
volume = {3},
year = {1989}
}
@book{Kirk2015,
abstract = {Learn how to apply test-driven development (TDD) to machine-learning algorithms—and catch mistakes that could sink your analysis. In this practical guide, author Matthew Kirk takes you through the principles of TDD and machine learning, and shows you how to apply TDD to several machine-learning algorithms, including Naive Bayesian classifiers and Neural Networks. Machine-learning algorithms often have tests baked in, but they can't account for human errors in coding. Rather than blindly rely on machine-learning results as many researchers have, you can mitigate the risk of errors with TDD and write clean, stable machine-learning code. If you're familiar with Ruby 2.1, you're ready to start. • Apply TDD to write and run tests before you start coding • Learn the best uses and tradeoffs of eight machine learning algorithms • Use real-world examples to test each algorithm through engaging, hands-on exercises • Understand the similarities between TDD and the scientific method for validating solutions • Be aware of the risks of machine learning, such as underfitting and overfitting data • Explore techniques for improving your machine-learning models or data extraction},
author = {Kirk, Matthew},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Thoughtful Machine Learning.pdf:pdf},
isbn = {9781449374068},
pages = {217},
publisher = {O'Reilly Media},
title = {{Thoughtful Machine Learning}},
year = {2015}
}
@phdthesis{Ruiz2006,
author = {Ruiz, Roberto},
title = {{Heur{\{}{\'{i}}{\}}sticas de selecci{\{}{\'{o}}{\}}n de atributos para datos de gran dimensionalidad}},
year = {2006}
}
@article{Carmona2013,
abstract = {Abstract Extraction of biologically-meaningful knowledge is one of the important and challenging tasks in bioinformatics, in particular computational analysis of DNA and protein sequences, in order to identify biological function(s) and behaviour(s) of newly-extracted sequences. Computational intelligence techniques in corporation with sequence-driven features have been applied to tackle the problem and help classify different functional classes of the sequences. In order to study this problem, subgroup discovery algorithms together with a signal processing-based feature extraction method are applied, where the sequences are represented as a signal. The applicability of this method has been studied through four different Neuraminidase genes of Influenza A subtypes, H1N1, H2N2, H3N2 and H5N1. The results yielded not only higher predictive accuracy over these four classes of the proteins but also interpretable rule-based representation of the descriptive model with a significantly reduced feature set driven by means of the signal processing method. Subgroup discovery technique based on evolutionary fuzzy systems is expected to open new areas of research in bioinformatics and further help identify and understand more focused therapeutic protein targets.},
author = {Carmona, C J and Chrysostomou, C and Seker, H and del Jesus, M J},
doi = {http://dx.doi.org/10.1016/j.asoc.2013.04.011},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - Fuzzy rules for describing subgroups from Influenza A virus using a multi-objective evolutionary algorithm.pdf:pdf},
issn = {1568-4946},
journal = {Applied Soft Computing},
keywords = {Evolutionary fuzzy system,Influenza A virus,Multi-objective evolutionary algorithm,Neuraminidase,Subgroup discovery},
number = {8},
pages = {3439--3448},
publisher = {Elsevier B.V.},
title = {{Fuzzy rules for describing subgroups from Influenza A virus using a multi-objective evolutionary algorithm}},
url = {http://www.sciencedirect.com/science/article/pii/S1568494613001300},
volume = {13},
year = {2013}
}
@article{Coffin2000,
abstract = {Statistical analysis is a powerful tool to apply when evaluating the performance of computer implementations of algorithms and heuristics. Yet many computational studies in the literature do not use this tool to maximum effectiveness. This paper examines the types of data that arise in computational comparisons and presents appropriate techniques for analyzing such data sets. Case studies of computational tests from the open literature are re-evaluated using the proposed methods in order to illustrate the value of statistical analysis for gaining insight into the behavior of the tested algorithms.},
author = {Coffin, Marie and Saltzman, Matthew J.},
doi = {10.1287/ijoc.12.1.24.11899},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2000 - Statistical Analysis of Computational Tests of Algorithms and Heuristics.pdf:pdf},
isbn = {1091-9856},
issn = {10919856},
journal = {INFORMS Journal on Computing},
keywords = {Algorithms,Heuristics,Statistical analysis},
number = {1},
pages = {24--44},
title = {{Statistical Analysis of Computational Tests of Algorithms and Heuristics}},
volume = {12},
year = {2000}
}
@article{Wang2012,
author = {Wang, Huanjing and Khoshgoftaar, Taghi M and Wald, Randall and Napolitano, Amri},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - An Empirical Study of Software Metric Selection Techniques for Defect Prediction.pdf:pdf},
isbn = {1-891706-31-4},
journal = {International Journal of Software Engineering and Knowledge Engineering},
pages = {94--99},
title = {{An Empirical Study of Software Metric Selection Techniques for Defect Prediction}},
year = {2012}
}
@article{Pawlak2007a,
abstract = {Worldwide, there has been a rapid growth in interest in rough set theory and its applications in recent years. Evidence of this can be found in the increasing number of high-quality articles on rough sets and related topics that have been published in a variety of international journals, symposia, workshops, and international conferences in recent years. In addition, many international workshops and conferences have included special sessions on the theory and applications of rough sets in their programs. Rough set theory has led to many interesting applications and extensions. It seems that the rough set approach is fundamentally important in artificial intelligence and cognitive sciences, especially in research areas such as machine learning, intelligent systems, inductive reasoning, pattern recognition, mereology, knowledge discovery, decision analysis, and expert systems. In the article, we present the basic concepts of rough set theory and point out some rough set-based research directions and applications.},
author = {Pawlak, Zdzis{\l}aw and Skowron, Andrzej},
doi = {10.1016/j.ins.2006.06.003},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2007 - Rudiments of rough sets.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {(in)Discernibility,Approximation spaces,Boolean reasoning,Decision rules,Dependencies of attributes,Information and decision systems,Reducts,Rough membership functions,Rough sets,Set approximations,Vague concepts},
month = {jan},
number = {1},
pages = {3--27},
title = {{Rudiments of rough sets}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025506001484},
volume = {177},
year = {2007}
}
@article{Baran2015,
author = {Baran, Remigiusz and Glowacz, Andrzej and Matiolanski, Andrzej},
doi = {10.1007/s11042-013-1545-2},
file = {:home/raul/art{\%}3A10.1007{\%}2Fs11042-013-1545-2.pdf:pdf},
issn = {1380-7501},
journal = {Multimedia Tools and Applications},
month = {jun},
number = {12},
pages = {4269--4288},
publisher = {Springer US},
title = {{The efficient real- and non-real-time make and model recognition of cars}},
url = {http://link.springer.com/10.1007/s11042-013-1545-2},
volume = {74},
year = {2015}
}
@article{Ceglar2006,
abstract = {The task of finding correlations between items in a dataset, association mining, has received considerable attention over the last decade. This article presents a survey of association mining fundamentals, detailing the evolution of association mining algorithms from the seminal to the state-of-the-art. This survey focuses on the fundamental principles of association mining, that is, itemset identification, rule generation, and their generic optimizations.},
author = {Ceglar, Aaron and Roddick, John F.},
doi = {10.1145/1132956.1132958},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2006 - Association mining.pdf:pdf},
isbn = {0360-0300},
issn = {03600300},
journal = {ACM Computing Surveys},
number = {2},
pages = {5--es},
title = {{Association mining}},
volume = {38},
year = {2006}
}
@article{Kononenko1994,
abstract = {European Conference on Machine Learning Catania, Italy, April 6–8, 1994 Proceedings},
author = {Kononenko, Igor},
doi = {10.1007/3-540-57868-4},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Estimating Attributes Analysis and Extensions of RELIEF.pdf:pdf},
isbn = {978-3-540-57868-0},
issn = {03029743},
journal = {Machine Learning: ECML-94},
pages = {171--182},
publisher = {Springer Verlag},
title = {{Estimating attributes: Analysis and extensions of RELIEF}},
url = {http://www.springerlink.com/index/10.1007/3-540-57868-4},
volume = {784},
year = {1994}
}
@article{Ipps2009,
author = {Ipps, Jere H L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - Mikhail A. F.pdf:pdf},
keywords = {- les sites vendiens,arkhangelsk en russie,de la mer blanche,de la r{\{}{\'{e}}{\}}gion de,destruction,ediacaran,fossiles vendiens,la mer blanche -,les pr{\{}{\'{e}}{\}}l{\{}{\`{e}}{\}}vements sauvages,paleo-piracy,paleontological resources,r{\{}{\'{e}}{\}}sum{\{}{\'{e}}{\}},selling fossils,une menace pour les,vendian,{\{}{\'{e}}{\}}diacariens},
pages = {1--36},
title = {{Mikhail A. F}},
volume = {03},
year = {2009}
}
@article{Level,
author = {Level, T H E Minus},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Dive into 3.pdf:pdf},
keywords = {apter -1,d ive i nto,n,p ython,s n ew i,w hat},
title = {{Dive into 3}}
}
@book{Garnaat2011,
author = {Garnaat, Mitch},
booktitle = {Xtemp-01},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2011 - Python And AWS Cookbook.pdf:pdf},
isbn = {9781449305444},
title = {{Python And AWS Cookbook}},
year = {2011}
}
@article{Puppe2008,
author = {Puppe, Frank and Atzmueller, Martin and Buscher, Georg and H{\"{u}}ttig, Matthias and Luehrs, Hardi and Buscher, Hans-Peter},
doi = {10.3233/978-1-58603-891-5-683},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2008 - Application and Evaluation of a Medical Knowledge System in Sonography ({\{}SONOCONSULT{\}}).pdf:pdf},
isbn = {978-1-58603-891-5},
journal = {ECAI'08/PAIS'08: Proceedings of the 18th European Conference on Artificial Intelligence, including Prestigious Applications of Intelligent Systems},
pages = {683--687},
title = {{Application and Evaluation of a Medical Knowledge System in Sonography ({\{}{\{}{\}}SONOCONSULT{\{}{\}}{\}})}},
year = {2008}
}
@article{Rossum2014,
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - Sorting HOW TO.pdf:pdf},
pages = {1--6},
title = {{Sorting HOW TO}},
year = {2014}
}
@article{Williams2009,
abstract = {Data mining delivers insights, pat- terns, and descriptive and predictive models from the large amounts of data available today in many organisations. The data miner draws heavily on methodologies, techniques and al- gorithms from statistics, machine learning, and computer science. R increasingly provides a powerful platform for data mining. However, scripting and programming is sometimes a chal- lenge for data analysts moving into data mining. The Rattle package provides a graphical user in- terface specifically for data mining using R. It also provides a stepping stone toward using R as a programming language for data analysis.},
author = {Williams, Graham J},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - Rattle A Data Mining GUI for R.pdf:pdf},
isbn = {2073-4859},
issn = {20734859},
journal = {The R Journal},
number = {December},
pages = {45--55},
title = {{Rattle : A Data Mining GUI for R}},
url = {http://journal.r-project.org/archive/2009-2/RJournal{\_}2009-2{\_}Williams.pdf},
volume = {1},
year = {2009}
}
@article{Ipps2009,
author = {Ipps, Jere H L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - Mikhail A. F.pdf:pdf},
keywords = {- les sites vendiens,arkhangelsk en russie,de la mer blanche,de la r{\'{e}}gion de,destruction,ediacaran,fossiles vendiens,la mer blanche -,les pr{\'{e}}l{\`{e}}vements sauvages,paleo-piracy,paleontological resources,r{\'{e}}sum{\'{e}},selling fossils,une menace pour les,vendian,{\'{e}}diacariens},
pages = {1--36},
title = {{Mikhail A. F}},
volume = {03},
year = {2009}
}
@article{Zarour2015,
abstract = {Software process assessment (SPA) is an effective tool to understand an organization's process quality and to explore improvement opportunities. However, the knowledge that underlies the best practices required to develop assessment methods, either lightweight or heavyweight methods, is unfortunately scattered throughout the literature. This paper presents the results of a systematic literature review to organize those recognized as the best practices in a way that helps SPA researchers and practitioners in designing and implementing their assessmentmethods. Such practices are presented in the literature as assessment requirements, success factors, observations, and lessons learned. Consequently, a set of 38 best practices has been collected and classified into five main categories, namely practices related to SPA methods, support tools, procedures, documentation, and users. While this collected set of best practices is important for designing lightweight as well as heavyweight assessment methods, it is of utmost importance in designing lightweight assessment methods, as the design of which depends on individual experience.},
author = {Zarour, Mohammad and Abran, Alain and Desharnais, Jean Marc and Alarifi, Abdulrahman},
doi = {10.1016/j.jss.2014.11.041},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - An investigation into the best practices for the successful design and implementation of lightweight software process assessment.pdf:pdf},
issn = {01641212},
journal = {Journal of Systems and Software},
keywords = {Assessment method design,Software process assessment,Systematic literature review},
pages = {180--192},
publisher = {Elsevier Ltd.},
title = {{An investigation into the best practices for the successful design and implementation of lightweight software process assessment methods: A systematic literature review}},
url = {http://dx.doi.org/10.1016/j.jss.2014.11.041},
volume = {101},
year = {2015}
}
@article{EntrepreneurialInsights2015,
author = {{Entrepreneurial Insights}},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Big Data.pdf:pdf},
isbn = {9781617290343},
issn = {0422-2784},
title = {{Big Data}},
url = {http://www.entrepreneurial-insights.com/lexicon/big-data/},
year = {2015}
}
@article{Rychtyckyj2013,
abstract = {Evolutionary computation has been successfully applied in a variety of problem domains and applications. In this paper we discuss the use of a specific form of evolutionary computation known as cultural algorithms that has been applied successfully in various real-world applications to solve problems of a very dynamic and complex nature. Cultural algorithms introduce a learning component into an evolutionary framework that influences the search strategy and is in turn modified by the best-performing members of the population during the entire process. Cultural algorithms have been used in various applications, including fraud analysis for automotive accident claims, the re-engineering of a dynamic automobile manufacturing knowledge base, the modeling of pricing strategies for automobiles in a multi-agent environment and for data mining.},
author = {Rychtyckyj, N. and Ostrowski, D. and Schleis, G. and Reynolds, R. G.},
doi = {10.1109/SIS.2003.1202266},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - Using cultural algorithms in industry.pdf:pdf},
isbn = {0780379144},
journal = {2003 IEEE Swarm Intelligence Symposium, SIS 2003 - Proceedings},
keywords = {Automobile manufacture,Computer industry,Cultural differences,Evolutionary computation,Information technology,Laboratories,Manufacturing industries,Pricing,Vehicle dynamics,Vehicles},
pages = {187--192},
title = {{Using cultural algorithms in industry}},
year = {2013}
}
@article{Fraz2014,
author = {Fraz, Muhammad and Edirisinghe, Eran A. and Sarfraz, M. Saquib},
doi = {10.1109/ICPR.2014.76},
file = {:home/raul/06976787.pdf:pdf},
isbn = {9781479952083},
issn = {10514651},
journal = {Proceedings - International Conference on Pattern Recognition},
keywords = {CORE-B,Dense Features,Fisher Vectors,Mid-level representation,Vehicle Make and Model Recognition},
mendeley-tags = {CORE-B},
pages = {393--398},
title = {{Mid-level-representation based lexicon for vehicle make and model recognition}},
year = {2014}
}
@article{Rossum2014b,
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - An introduction to the ipaddress module.pdf:pdf},
pages = {4--9},
title = {{An introduction to the ipaddress module}},
year = {2014}
}
@article{Rossum2013,
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - The PythonC API.pdf:pdf},
title = {{The Python/C API}},
year = {2013}
}
@article{Gamberger2004,
abstract = {Finding disease markers (classifiers) from gene expression data by machine learning algorithms is characterized by a high risk of overfitting the data due the abundance of attributes (simultaneously measured gene expression values) and shortage of available examples (observations). To avoid this pitfall and achieve predictor robustness, state-of-the-art approaches construct complex classifiers that combine relatively weak contributions of up to thousands of genes (attributes) to classify a disease. The complexity of such classifiers limits their transparency and consequently the biological insights they can provide. The goal of this study is to apply to this domain the methodology of constructing simple yet robust logic-based classifiers amenable to direct expert interpretation. On two well-known, publicly available gene expression classification problems, the paper shows the feasibility of this approach, employing a recently developed subgroup discovery methodology. Some of the discovered classifiers allow for novel biological interpretations. ?? 2004 Elsevier Inc. All rights reserved.},
author = {Gamberger, Dragan and Lavra??, Nada and ??elezn??, Filip and Tolar, Jakub},
doi = {10.1016/j.jbi.2004.07.007},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2004 - Induction of comprehensible models for gene expression datasets by subgroup discovery methodology.pdf:pdf},
issn = {15320464},
journal = {Journal of Biomedical Informatics},
keywords = {Comprehensible classification,Disease markers,Gene expression measurements,Machine learning,Subgroup discovery},
number = {4},
pages = {269--284},
pmid = {15465480},
title = {{Induction of comprehensible models for gene expression datasets by subgroup discovery methodology}},
volume = {37},
year = {2004}
}
@article{Yu2003,
abstract = {Feature selection, as a preprocessing step to machine learning, is effective in reducing dimensionality, removing irrelevant data, increasing learning accuracy, and improving result comprehensibility. However, the recent increase of dimensionality of data poses a severe challenge to many existing feature selection methods with respect to efficiency and effectiveness. In this work, we introduce a novel concept, predominant correlation, and propose a fast filter method which can identify relevant features as well as redundancy among relevant features without pairwise correlation analysis. The efficiency and effectiveness of our method is demonstrated through extensive comparisons with other methods using real-world data of high dimensionality.},
author = {Yu, Lei and Liu, Huan},
doi = {citeulike-article-id:3398512},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2003 - Feature Selection for High-Dimensional Data A Fast Correlation-Based Filter Solution.pdf:pdf},
isbn = {1577351894},
issn = {01469592},
journal = {International Conference on Machine Learning (ICML)},
pages = {1--8},
title = {{Feature Selection for High-Dimensional Data: A Fast Correlation-Based Filter Solution}},
url = {http://www.aaai.org/Papers/ICML/2003/ICML03-111.pdf},
year = {2003}
}
@article{Level,
author = {Level, T H E Minus},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Dive into 3.pdf:pdf},
keywords = {apter -1,d ive i nto,n,p ython,s n ew i,w hat},
title = {{Dive into 3}}
}
@article{Fister2013,
abstract = {Swarm intelligence and bio-inspired algorithms form a hot topic in the developments of new algorithms inspired by nature. These nature-inspired metaheuristic algorithms can be based on swarm intelligence, biological systems, physical and chemical systems. Therefore, these algorithms can be called swarm-intelligence-based, bio-inspired, physics-based and chemistry-based, depending on the sources of inspiration. Though not all of them are efficient, a few algorithms have proved to be very efficient and thus have become popular tools for solving real-world problems. Some algorithms are insufficiently studied. The purpose of this review is to present a relatively comprehensive list of all the algorithms in the literature, so as to inspire further research.},
archivePrefix = {arXiv},
arxivId = {1307.4186},
author = {Fister, Iztok and Yang, Xin-She and Fister, Iztok and Brest, Janez and Fister, Du{\v{s}}an},
eprint = {1307.4186},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - A Brief Review of Nature-Inspired Algorithms for Optimization.pdf:pdf},
keywords = {bio-inspired algorithms,chemistry algorithms,optimization,physics,swarm intelligence},
number = {3},
pages = {1--7},
title = {{A Brief Review of Nature-Inspired Algorithms for Optimization}},
url = {http://arxiv.org/abs/1307.4186},
volume = {80},
year = {2013}
}
@inbook{Li2016,
address = {Paris},
author = {Li, An-da and He, Zhen},
booktitle = {Proceedings of the 22nd International Conference on Industrial Engineering and Engineering Management 2015},
doi = {10.2991/978-94-6239-180-2_3},
editor = {Qi, Ershi and Shen, Jiang and Dou, Runliang},
isbn = {978-94-6239-180-2},
pages = {23--30},
publisher = {Atlantis Press},
title = {{ReliefF Based Forward Selection Algorithm to Identify CTQs for Complex Products}},
url = {http://link.springer.com/10.2991/978-94-6239-180-2{\_}3},
year = {2016}
}
@incollection{Chávez2016,
address = {Cham},
author = {Ch{\'{a}}vez, Francisco and Fern{\'{a}}ndez, Francisco and Benavides, C{\'{e}}sar and Lanza, Daniel and Villegas, Juan and Trujillo, Leonardo and Olague, Gustavo and Rom{\'{a}}n, Graciela},
booktitle = {Applications of Evolutionary Computation: 19th European Conference, EvoApplications 2016, Porto, Portugal, March 30 -- April 1, 2016, Proceedings, Part II},
chapter = {ECJ+HADOOP},
doi = {10.1007/978-3-319-31153-1_7},
editor = {Squillero, Giovanni and Burelli, Paolo},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2016 - Applications of Evolutionary Computation 19th European Conference, EvoApplications 2016, Porto, Portugal, March 30 -- April 1, 20.pdf:pdf},
isbn = {9783319311524},
issn = {16113349},
pages = {91--106},
publisher = {Springer International Publishing},
title = {{ECJ+HADOOP: An Easy Way to Deploy Massive Runs of Evolutionary Algorithms}},
url = {http://link.springer.com/10.1007/978-3-319-31153-1{\_}7},
year = {2016}
}
@article{Rossum2015,
author = {Rossum, Guido Van},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Python Frequently Asked Questions.pdf:pdf},
title = {{Python Frequently Asked Questions}},
year = {2015}
}
@article{Grosskreutz2009,
abstract = {Subgroup discovery is a Knowledge Discovery task that aims at finding subgroups of a population with high generality and distributional unusualness. While several subgroup discovery algorithms have been presented in the past, they focus on databases with nominal attributes or make use of discretization to get rid of the numerical attributes. In this paper, we illustrate why the replacement of numerical attributes by nominal attributes can result in suboptimal results. Thereafter, we present a new subgroup discovery algorithm that prunes large parts of the search space by exploiting bounds between related numerical subgroup descriptions. The same algorithm can also be applied to ordinal attributes. In an experimental section, we show that the use of our new pruning scheme results in a huge performance gain when more that just a few split-points are considered for the numerical attributes},
author = {Grosskreutz, Henrik and R{\"{u}}ping, Stefan},
doi = {10.1007/s10618-009-0136-3},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - On subgroup discovery in numerical domains.pdf:pdf},
issn = {13845810},
journal = {Data Mining and Knowledge Discovery},
keywords = {Pattern mining,Performance,Pruning,Subgroup discovery},
number = {2},
pages = {210--226},
title = {{On subgroup discovery in numerical domains}},
volume = {19},
year = {2009}
}
@article{Ward2006,
author = {Ward, Greg and Ward, Greg and Baxter, Anthony and Baxter, Anthony},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2006 - Distributing Python Modules.pdf:pdf},
journal = {Distribution},
title = {{Distributing Python Modules}},
year = {2006}
}
@article{Pawlak2007,
abstract = {In this article, we discuss methods based on the combination of rough sets and Boolean reasoning with applications in pattern recognition, machine learning, data mining and conflict analysis.},
author = {Pawlak, Zdzis{\l}aw and Skowron, Andrzej},
doi = {10.1016/j.ins.2006.06.007},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2007 - Rough sets and Boolean reasoning.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {(In)discernibility,Approximate Boolean reasoning,Association rules,Boolean reasoning,Classifiers,Conflict analysis,Decision rules,Discretization,Reducts,Rough sets,Symbolic value grouping},
month = {jan},
number = {1},
pages = {41--73},
title = {{Rough sets and Boolean reasoning}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025506001502},
volume = {177},
year = {2007}
}
@article{Connolly2012,
abstract = {This paper examines the literature on computer games and serious games in regard to the potential positive impacts of gaming on users aged 14 years or above, especially with respect to learning, skill enhancement and engagement. Search terms identified 129 papers reporting empirical evidence about the impacts and outcomes of computer games and serious games with respect to learning and engagement and a multidimensional approach to categorizing games was developed. The findings revealed that playing computer games is linked to a range of perceptual, cognitive, behavioural, affective and motivational impacts and outcomes. The most frequently occurring outcomes and impacts were knowledge acquisition/content understanding and affective and motivational outcomes. The range of indicators and measures used in the included papers are discussed, together with methodological limitations and recommendations for further work in this area.},
author = {Connolly, Thomas M and Boyle, Elizabeth A and Macarthur, Ewan and Hainey, Thomas and Boyle, James M},
doi = {10.1016/j.compedu.2012.03.004},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Computers {\&} Education A systematic literature review of empirical evidence on computer games and serious games.pdf:pdf},
isbn = {0360-1315},
issn = {0360-1315},
journal = {Computers {\{}{\&}{\}} Education},
number = {2},
pages = {661--686},
publisher = {Elsevier Ltd},
title = {{Computers {\{}{\&}{\}} Education A systematic literature review of empirical evidence on computer games and serious games}},
url = {http://dx.doi.org/10.1016/j.compedu.2012.03.004},
volume = {59},
year = {2012}
}
@article{Zhao2012,
abstract = {Advances in computer technologies have enabled corporations to accumulate data at an unprecedented speed. Large-scale business data might contain billions of observations and thousands of features, which easily brings their scale to the level of terabytes. Most traditional feature selection algorithms are designed for a centralized computing architecture. Their usability significantly deteriorates when data size exceeds hundreds of gigabytes. High-performance distributed computing frameworks and protocols, such as the Message Passing Interface (MPI) and MapReduce, have been proposed to facilitate software development on grid infrastructures, enabling analysts to process large-scale problems efficiently. This paper presents a novel large-scale feature selection algorithm that is based on variance analysis. The algorithm selects features by evaluating their abilities to explain data variance. It supports both supervised and unsupervised feature selection and can be readily implemented in most distributed computing environments. The algorithm was developed as a SAS High-Performance Analytics procedure, which can read data in distributed form and perform parallel feature selection in both symmetric multiprocessing mode and massively parallel processing mode. Experimental results demonstrated the superior performance of the proposed method for large scale feature selection. {\textcopyright} 2012 Springer-Verlag.},
author = {Zhao, Zheng and Cox, James and Duling, David and Sarle, Warren},
doi = {10.1007/978-3-642-33460-3_21},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - Massively parallel feature selection an approach based on variance preservation.pdf:pdf},
isbn = {9783642334597},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Algorithms,Computer technology,Computing architect,Feature extraction,Learning systems,Multiproces,big-data,to-read},
mendeley-tags = {to-read},
number = {PART 1},
pages = {237--252},
publisher = {Springer Berlin Heidelberg},
title = {{Massively parallel feature selection: An approach based on variance preservation}},
url = {http://link.springer.com/10.1007/978-3-642-33460-3{\_}21},
volume = {7523 LNAI},
year = {2012}
}
@article{EntrepreneurialInsights2015,
author = {{Entrepreneurial Insights}},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - Big Data.pdf:pdf},
isbn = {9781617290343},
issn = {0422-2784},
title = {{Big Data}},
url = {http://www.entrepreneurial-insights.com/lexicon/big-data/},
year = {2015}
}
@book{Grolemund,
author = {Grolemund, G},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - Hands-On Programming with R.pdf:pdf},
isbn = {978-1-4493-5901-0},
title = {{Hands-On Programming with R}}
}
@article{Ferguson1999,
author = {Ferguson, Mike},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1999 - Evaluating and Selecting Data Mining Tools.pdf:pdf},
journal = {InfoDB},
number = {2},
pages = {1--10},
title = {{Evaluating and Selecting Data Mining Tools}},
volume = {11},
year = {1999}
}
@article{Blum2003,
abstract = {The emergence of metaheuristics for solving difficult combinatorial optimization problems is one of the most notable achievements of the last two decades in operations research. This paper provides an account of the most recent developments in the field and identifies some common issues and trends. Examples of applications are also reported for vehicle routing and scheduling problems.},
author = {Blum, C. and Roli, a.},
doi = {10.1007/s10479-005-3971-7},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2003 - Metaheuristics in combinatorial optimization overview and conceptual comparison.pdf:pdf},
isbn = {0254-5330},
issn = {02545330},
journal = {ACM Computing Surveys},
keywords = {Combinatorial optimization,Metaheuristics,Unifying framework,Vehicle routing},
number = {3},
pages = {189--213},
title = {{Metaheuristics in combinatorial optimization: overview and conceptual comparison}},
volume = {35},
year = {2003}
}
@article{Chandrashekar2014,
abstract = {Plenty of feature selection methods are available in literature due to the availability of data with hundreds of variables leading to data with very high dimension. Feature selection methods provides us a way of reducing computation time, improving prediction performance, and a better understanding of the data in machine learning or pattern recognition applications. In this paper we provide an overview of some of the methods present in literature. The objective is to provide a generic introduction to variable elimination which can be applied to a wide array of machine learning problems. We focus on Filter, Wrapper and Embedded methods. We also apply some of the feature selection techniques on standard datasets to demonstrate the applicability of feature selection techniques.},
annote = {{\textless}RESUME{\textgreater}

A brief intro to FS approaches is given and a brief comparison between the following is done:

We implement two filter methods: basic correlation and Mutal Informaiton wherein the features are ranked and the performance is obtained for the top ranked feature and subsequently adding the next ranked feature one by one to cover the whole feature set. 

In wrapper techniques we implement the (Sequential Floating Forward Selection (SFFS)) algorithm and the CHCGA (genetic) algorithm with the performance of the two classifiers as the objective function.},
author = {Chandrashekar, Girish and Sahin, Ferat},
doi = {10.1016/j.compeleceng.2013.11.024},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - A survey on feature selection methods.pdf:pdf},
issn = {00457906},
journal = {Computers {\&} Electrical Engineering},
month = {jan},
number = {1},
pages = {16--28},
title = {{A survey on feature selection methods}},
url = {http://www.sciencedirect.com/science/article/pii/S0045790613003066},
volume = {40},
year = {2014}
}
@article{Sanchez-Marono2007,
abstract = {Adequate selection of features may improve accuracy and efficiency of classifier methods. There are two main approaches for feature selection: wrapper methods, in which the features are selected using the classifier, and filter methods, in which the selection of features is independent of the classifier used. Although the wrapper approach may obtain better performances, it requires greater computational resources. For this reason, lately a new paradigm, hybrid approach, that combines both filter and wrapper methods has emerged. One of its problems is to select the filter method that gives the best relevance index for each case, and this is not an easy to solve question. Different approaches to relevance evaluation lead to a large number of indices for ranking and selection. In this paper, several filter methods are applied over artificial data sets with different number of relevant features, level of noise in the output, interaction between features and increasing number of samples. The results obtained for the four filters studied (ReliefF, Correlationbased Feature Selection, Fast Correlated Based Filter and INTERACT) are compared and discussed. The final aim of this study is to select a filter to construct a hybrid method for feature selection.},
author = {S{\'{a}}nchez-Maro{\~{n}}o, N and Alonso-Betanzos, A and Tombilla-Snarom{\'{a}}n, M},
doi = {10.1007/978-3-540-77226-2},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2007 - Filter methods for feature selection–a comparative study.pdf:pdf},
isbn = {978-3-540-77225-5},
journal = {Intelligent Data Engineering and Automated Learning - IDEAL 2007},
pages = {178--187},
title = {{Filter methods for feature selection–a comparative study}},
url = {http://link.springer.com/chapter/10.1007/978-3-540-77226-2{\_}19},
year = {2007}
}
@article{Rossum2013,
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - The PythonC API.pdf:pdf},
title = {{The Python/C API}},
year = {2013}
}
@article{Riquelme2003,
abstract = {This paper describes a new approach, HIerarchical DEcision Rules (HIDER), for learning generalizable rules in continuous and discrete domains based on evolutionary algorithms. The main contributions of our approach are the integration of both binary and real evolutionary coding; the use of specific operators; the relaxing coefficient to construct more flexible classifiers by indicating how general, with respect to the errors, decision rules must be; the coverage factor in the fitness function, which makes possible a quick expansion of the rule size; and the implicit hierarchy when rules are being obtained. HIDER is accuracy-aware since it can control the maximum allowed error for each decision rule. We have tested our system on real data from the UCI Repository. The results of a 10-fold cross-validation are compared to C4.5's and they show a significant improvement with respect to the number of rules and the error rate.},
author = {Riquelme, Jos{\'{e}} C. and Aguilar-Ruiz, Jes{\'{u}}s S. and Valle, Carmelo Del},
doi = {10.1016/S0020-0255(03)00175-0},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2003 - Supervised learning by means of accuracy-aware evolutionary algorithms.pdf:pdf},
issn = {00200255},
journal = {Information Sciences},
keywords = {Decision trees,Evolutionary algorithms,Supervised learning},
month = {nov},
number = {3-4},
pages = {173--188},
title = {{Supervised learning by means of accuracy-aware evolutionary algorithms}},
url = {http://www.sciencedirect.com/science/article/pii/S0020025503001750},
volume = {156},
year = {2003}
}
@inbook{Luque2011,
address = {Berlin, Heidelberg},
author = {Luque, Gabriel and Alba, Enrique},
booktitle = {Parallel Genetic Algorithms: Theory and Real World Applications},
chapter = {2},
doi = {10.1007/978-3-642-22084-5_2},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2011 - Parallel Models for.pdf:pdf},
isbn = {978-3-642-22084-5},
keywords = {10-1 cites},
mendeley-tags = {10-1 cites},
pages = {15--30},
publisher = {Springer Berlin Heidelberg},
title = {{Parallel Models for Genetic Algorithms}},
url = {http://dx.doi.org/10.1007/978-3-642-22084-5{\_}2},
year = {2011}
}
@article{Rossum2012f,
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Descriptor HowTo Guide.pdf:pdf},
journal = {Learning},
title = {{Descriptor HowTo Guide}},
year = {2012}
}
@article{Collier1999,
author = {Collier, Ken and Ph, D and Carey, Bernard and Marjaniemi, Curt},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1999 - A Methodology for Evaluating and Selecting Data Mining Software Keywords Data Mining , Tool Evaluation , Knowledge Discovery.pdf:pdf},
isbn = {0769500013},
journal = {IEEE},
keywords = {data mining,knowledge discovery,tool evaluation},
number = {c},
pages = {1--11},
title = {{A Methodology for Evaluating and Selecting Data Mining Software Keywords : Data Mining , Tool Evaluation , Knowledge Discovery}},
volume = {00},
year = {1999}
}
@article{Bolón-Canedo2012,
abstract = {With the advent of high dimensionality, adequate identification of relevant features of the data has become indispensable in real-world scenarios. In this context, the importance of feature selection is beyond doubt and different methods have been developed. However, with such a vast body of algorithms available, choosing the adequate feature selection method is not an easy-to-solve question and it is necessary to check their effectiveness on different situations. Nevertheless, the assessment of relevant features is difficult in real datasets and so an interesting option is to use artificial data. In this paper, several synthetic datasets are employed for this purpose, aiming at reviewing the performance of feature selection methods in the presence of a crescent number or irrelevant features, noise in the data, redundancy and interaction between attributes, as well as a small ratio between number of samples and number of features. Seven filters, two embedded methods, and two wrappers are applied over eleven synthetic datasets, tested by four classifiers, so as to be able to choose a robust method, paving the way for its application to real datasets.},
author = {Bol{\'{o}}n-Canedo, Ver{\'{o}}nica and S{\'{a}}nchez-Maro{\~{n}}o, Noelia and Alonso-Betanzos, Amparo},
doi = {10.1007/s10115-012-0487-8},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - A review of feature selection methods on synthetic data.1007{\_}s1011:1007{\_}s1011},
issn = {0219-3116},
journal = {Knowledge and Information Systems},
number = {3},
pages = {483--519},
title = {{A review of feature selection methods on synthetic data}},
url = {http://dx.doi.org/10.1007/s10115-012-0487-8},
volume = {34},
year = {2012}
}
@article{Wrobel1997,
abstract = {We consider the problem of finding statistically unusual subgroups in a multi-relation database, and extend previous work on single-relation subgroup discovery. We give a precise definition of the multi-relation subgroup discovery task, propose a specific form of declarative bias based on foreign links as a means of specifying the hypothesis space, and show how propositional evaluation functions can be adapted to the multi-relation setting. We then describe an algorithm for this problem setting that uses optimistic estimate and minimal support pruning, an optimal refinement operator and sampling to ensure efficiency and can easily be parallelized.},
author = {Wrobel, Stefan},
doi = {10.1007/3-540-63223-9},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1997 - An algorithm for multi-relational discovery of subgroups.pdf:pdf},
isbn = {0302-9743 (Print) 1611-3349 (Online)},
issn = {03029743},
journal = {Lecture Notes in Computer Science},
pages = {78--87},
title = {{An algorithm for multi-relational discovery of subgroups}},
url = {http://www.springerlink.com/content/c1u43270pk25106n/},
volume = {1263/1997},
year = {1997}
}
@article{Rossum2014c,
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - Python Setup and Usage.pdf:pdf},
title = {{Python Setup and Usage}},
year = {2014}
}
@article{Triguero2015,
abstract = {The application of data mining and machine learning techniques to biological and biomedicine data continues to be an ubiquitous research theme in current bioinformatics. The rapid advances in biotechnology are allowing us to obtain and store large quantities of data about cells, proteins, genes, etc., that should be processed. Moreover, in many of these problems such as contact map prediction, the problem tackled in this paper, it is difficult to collect representative positive examples. Learning under these circumstances, known as imbalanced big data classification, may not be straightforward for most of the standard machine learning methods. In this work we describe the methodology that won the ECBDL'14 big data challenge for a bioinformatics big data problem. This algorithm, named as ROSEFW-RF, is based on several MapReduce approaches to (1) balance the classes distribution through random oversampling, (2) detect the most relevant features via an evolutionary feature weighting process and a threshold to choose them, (3) build an appropriate Random Forest model from the pre-processed data and finally (4) classify the test data. Across the paper, we detail and analyze the decisions made during the competition showing an extensive experimental study that characterize the way of working of our methodology. From this analysis we can conclude that this approach is very suitable to tackle large-scale bioinformatics classifications problems.},
author = {Triguero, Isaac and del R{\'{i}}o, Sara and L{\'{o}}pez, Victoria and Bacardit, Jaume and Ben{\'{i}}tez, Jos{\'{e}} M and Herrera, Francisco},
doi = {10.1016/j.knosys.2015.05.027},
issn = {09507051},
journal = {Knowledge-Based Systems},
keywords = {Big data,Bioinformatics,Evolutionary feature selection,Hadoop,Imbalance classification,MapReduce},
month = {oct},
pages = {69--79},
title = {{ROSEFW-RF: The winner algorithm for the ECBDL'14 big data competition: An extremely imbalanced big data bioinformatics problem}},
url = {http://www.sciencedirect.com/science/article/pii/S0950705115002130},
volume = {87},
year = {2015}
}
@article{Bolón-Canedo2012,
abstract = {With the advent of high dimensionality, adequate identification of relevant features of the data has become indispensable in real-world scenarios. In this context, the importance of feature selection is beyond doubt and different methods have been developed. However, with such a vast body of algorithms available, choosing the adequate feature selection method is not an easy-to-solve question and it is necessary to check their effectiveness on different situations. Nevertheless, the assessment of relevant features is difficult in real datasets and so an interesting option is to use artificial data. In this paper, several synthetic datasets are employed for this purpose, aiming at reviewing the performance of feature selection methods in the presence of a crescent number or irrelevant features, noise in the data, redundancy and interaction between attributes, as well as a small ratio between number of samples and number of features. Seven filters, two embedded methods, and two wrappers are applied over eleven synthetic datasets, tested by four classifiers, so as to be able to choose a robust method, paving the way for its application to real datasets.},
author = {Bol{\'{o}}n-Canedo, Ver{\'{o}}nica and S{\'{a}}nchez-Maro{\~{n}}o, Noelia and Alonso-Betanzos, Amparo},
doi = {10.1007/s10115-012-0487-8},
issn = {0219-3116},
journal = {Knowledge and Information Systems},
number = {3},
pages = {483--519},
title = {{A review of feature selection methods on synthetic data}},
url = {http://dx.doi.org/10.1007/s10115-012-0487-8},
volume = {34},
year = {2012}
}
@article{Rajaraman2011,
abstract = {At the highest level of description, this book is about data mining. However, it focuses on data mining of very large amounts of data, that is, data so large it does not fit in main memory. Because of the emphasis on size, many of our examples are about the Web or data derived from the Web. Further, the book takes an algorithmic point of view: data mining is about applying algorithms to data, rather than using data to train a machine-learning engine of some sort.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rajaraman, Anand and Ullman, Jeffrey D},
doi = {10.1017/CBO9781139058452},
eprint = {arXiv:1011.1669v3},
isbn = {9781139058452},
issn = {01420615},
journal = {Lecture Notes for Stanford CS345A Web Mining},
pages = {328},
pmid = {1107015359},
title = {{Mining of Massive Datasets}},
url = {http://ebooks.cambridge.org/ref/id/CBO9781139058452},
volume = {67},
year = {2011}
}
@article{Berthold2009,
abstract = {The Konstanz Information Miner is a modular environment which enables easy visual assembly and interactive execution of a data pipeline. It is designed as a teaching, research and collaboration platform, which enables easy integration of new algorithms, data manipulation or visualization methods as new modules or nodes. In this paper we describe some of the design aspects of the underlying architecture and briefly sketch how new nodes can be incorporated.},
author = {Berthold, Michael R. and Cebron, Nicholas and Dill, Fabian and Gabriel, Thomas R. and K{\"{o}}tter, Tobias and Meinl, Thorsten and Ohl, Peter and Thiel, Kilian and Wiswedel, Bernd},
doi = {10.1145/1656274.1656280},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2009 - KNIME - The Konstanz Information Miner.pdf:pdf},
isbn = {978-3-540-78239-1},
issn = {19310145},
journal = {SIGKDD Explorations},
number = {1},
pages = {26--31},
title = {{KNIME - The Konstanz Information Miner}},
url = {http://centaur.reading.ac.uk/6139/},
volume = {11},
year = {2009}
}
@article{Liu2016,
author = {Liu, Yang and Xu, Lixiong and Li, Maozhen},
doi = {10.1007/s10766-016-0401-1},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2016 - The Parallelization of Back Propagation Neural Network in MapReduce and Spark.pdf:pdf},
issn = {0885-7458},
journal = {International Journal of Parallel Programming},
month = {feb},
pages = {1--20},
publisher = {Springer US},
title = {{The Parallelization of Back Propagation Neural Network in MapReduce and Spark}},
url = {http://link.springer.com/10.1007/s10766-016-0401-1},
year = {2016}
}
@article{Alcala-Fdez2015,
author = {Alcala-Fdez, J. and Alonso, J.},
doi = {10.1109/TFUZZ.2015.2426212},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2015 - A Survey of Fuzzy Systems Software Taxonomy, Current Research Trends and Prospects.pdf:pdf},
issn = {1063-6706},
journal = {IEEE Transactions on Fuzzy Systems},
number = {99},
pages = {1--17},
title = {{A Survey of Fuzzy Systems Software: Taxonomy, Current Research Trends and Prospects}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=7094263},
volume = {PP},
year = {2015}
}
@article{Rossum2012a,
author = {Rossum, Guido Van and Drake, Fred L},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Porting Extension Modules to 3 . 0.pdf:pdf},
journal = {Changes},
title = {{Porting Extension Modules to 3 . 0}},
year = {2012}
}
@book{Mahjoub2012,
author = {Mahjoub, A. Ridha and Markakis, Vangelis and Milis, Ioannis and Paschos, Vangelis Th.},
booktitle = {Handbook of combinatorics},
doi = {10.1007/978-3-642-32147-4},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Combinatorial optimization.pdf:pdf},
isbn = {9783642321467},
pages = {1--488},
title = {{Combinatorial optimization}},
url = {http://www.ulb.tu-darmstadt.de/tocs/59142804.pdf$\backslash$nhttps://www.zib.de/groetschel/pubnew/paper/groetschellovasz1995.pdf},
year = {2012}
}
@article{McAfee2012,
abstract = {Big data, the authors write, is far more powerful than the analytics of the past. Executives can measure and therefore manage more precisely than ever before. They can make better predictions and smarter decisions. They can target more-effective interventions in areas that so far have been dominated by gut and intuition rather than by data and rigor. The differences between big data and analytics are a matter of volume, velocity, and variety: More data now cross the internet every second than were stored in the entire internet 20 years ago. Nearly real-time information makes it possible for a company to be much more agile than its competitors. And that information can come from social networks, images, sensors, the web, or other unstructured sources. The managerial challenges, however, are very real. Senior decision makers have to learn to ask the right questions and embrace evidence-based decision making. Organizations must hire scientists who can find patterns in very large data sets and translate them into useful business information. IT departments have to work hard to integrate all the relevant internal and external sources of data. The authors offer two success stories to illustrate how companies are using big data: PASSUR Aerospace enables airlines to match their actual and estimated arrival times. Sears Holdings directly analyzes its incoming store data to make promotions much more precise and faster.},
author = {McAfee, Andrew and Brynjolfsson, Erik},
doi = {10.1007/s12599-013-0249-5},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Big data the management revolution.pdf:pdf},
isbn = {00178012},
issn = {0017-8012},
journal = {Harvard business review},
keywords = {Commerce,Decision Making, Organizational,Efficiency, Organizational,Humans,Information Management,Organizational Culture},
number = {10},
pages = {60--6, 68, 128},
pmid = {23074865},
title = {{Big data: the management revolution.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23074865},
volume = {90},
year = {2012}
}
@article{R.A.1991,
author = {R.A., Day},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/1991 - C{\{}{\'{o}}{\}}mo escribir y publicar trabajos cient{\{}{\'{i}}{\}}ficos745.pdf:pdf},
keywords = {7530},
number = {normas publicaci{\{}{\'{o}}{\}}n},
pages = {----},
title = {{C{\{}{\'{o}}{\}}mo escribir y publicar trabajos cient{\{}{\'{i}}{\}}ficos745}},
year = {1991}
}
@book{Collette2013,
abstract = {Gain hands-on experience with HDF5 for storing scientific data in Python. This practical guide quickly gets you up to speed on the details, best practices, and pitfalls of using HDF5 to archive and share numerical datasets ranging in size from gigabytes to terabytes. Through real-world examples and practical exercises, you'll explore topics such as scientific datasets, hierarchically organized groups, user-defined metadata, and interoperable files. Examples are applicable for users of both Python 2 and Python 3. If you're familiar with the basics of Python data analysis, this is an ideal introduction to HDF5.},
author = {Collette, Andrew},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - Python and HDF5.pdf:pdf},
isbn = {9781449367831},
pages = {152},
title = {{Python and HDF5}},
year = {2013}
}
@article{Kafai2012,
author = {Kafai, Mehran and Bhanu, Bir},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2012 - Dynamic Bayesian Networks for Vehicle Classification in Video.pdf:pdf},
journal = {IEEE Trans. Industrial Informatics},
number = {1},
pages = {100--109},
title = {{Dynamic Bayesian Networks for Vehicle Classification in Video}},
volume = {8},
year = {2012}
}
@article{Atzmueller2007,
author = {Atzmueller, Martin and Puppe, Frank},
doi = {10.1007/s10489-007-0057-z},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2007 - A case-based approach for characterization and analysis of subgroup patterns.pdf:pdf},
issn = {0924-669X},
journal = {Applied Intelligence},
number = {3},
pages = {210--221},
title = {{A case-based approach for characterization and analysis of subgroup patterns}},
url = {http://www.springerlink.com/index/10.1007/s10489-007-0057-z},
volume = {28},
year = {2007}
}
@book{RogeriodosSantosAlves;AlexSoaresdeSouza2014,
abstract = {Mycotoxins are small (MW approximately 700), toxic chemical products formed as secondary metabolites by a few fungal species that readily colonise crops and contaminate them with toxins in the field or after harvest. Ochratoxins and Aflatoxins are mycotoxins of major significance and hence there has been significant research on broad range of analytical and detection techniques that could be useful and practical. Due to the variety of structures of these toxins, it is impossible to use one standard technique for analysis and/or detection. Practical requirements for high-sensitivity analysis and the need for a specialist laboratory setting create challenges for routine analysis. Several existing analytical techniques, which offer flexible and broad-based methods of analysis and in some cases detection, have been discussed in this manuscript. There are a number of methods used, of which many are lab-based, but to our knowledge there seems to be no single technique that stands out above the rest, although analytical liquid chromatography, commonly linked with mass spectroscopy is likely to be popular. This review manuscript discusses (a) sample pre-treatment methods such as liquid-liquid extraction (LLE), supercritical fluid extraction (SFE), solid phase extraction (SPE), (b) separation methods such as (TLC), high performance liquid chromatography (HPLC), gas chromatography (GC), and capillary electrophoresis (CE) and (c) others such as ELISA. Further currents trends, advantages and disadvantages and future prospects of these methods have been discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {{et all Rog{\'{e}}rio dos Santos Alves; Alex Soares de Souza}},
booktitle = {Igarss 2014},
doi = {10.1007/s13398-014-0173-7.2},
eprint = {arXiv:1011.1669v3},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - No Title No Title.pdf:pdf},
isbn = {9780874216561},
issn = {13514180},
keywords = {high resolution images,research,risks management,sustainable reconstruction},
number = {1},
pages = {1--5},
pmid = {15991970},
title = {{No Title No Title}},
year = {2014}
}
@article{Verma2014,
author = {Verma, Author Vikas and Dhawan, Sunil},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2014 - Methodology for Selection of a Data Mining Tool.pdf:pdf},
pages = {189--192},
title = {{Methodology for Selection of a Data Mining Tool}},
year = {2014}
}
@book{Fallis2013b,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Fallis, A.G},
booktitle = {Journal of Chemical Information and Modeling},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:home/raul/.local/share/data/Mendeley Ltd./Mendeley Desktop/Downloaded/2013 - No Title No Title(5).pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{No Title No Title}},
volume = {53},
year = {2013}
}
